# HG changeset patch
# Parent e103034e3d910e495f62be6d83315941ce1dd158

diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -359,4 +359,56 @@ config DMATEST
 	  Simple DMA test client. Say N unless you're debugging a
 	  DMA Device driver.
 
+config LTQ_HWMCPY
+	bool "Lantiq Hardware Memcopy Engine"
+	default n
+	depends on LANTIQ && SOC_GRX500
+	---help---
+	  Lantiq Hardware Memory Copy Engine.
+
+config LTQ_UMT_SW_MODE
+		bool "Lantiq UMT SW mode"
+		depends on LTQ_HWMCPY && SOC_GRX500_A21
+		default n
+		---help---
+	  	In SW mode, a dedicated TC on VPE is used to update
+	  	the UMT counter in the destination port.
+
+choice
+	prompt "LTQ UMT Version Select"
+    default LTQ_UMT_EXPAND_MODE if SOC_GRX500_A21
+	default LTQ_UMT_LEGACY_MODE
+
+config LTQ_UMT_LEGACY_MODE
+	bool "Lantiq UMT in A11 legacy mode"
+	depends on LTQ_HWMCPY
+	---help---
+	  Lantiq UMT Hardware support A11(legacy) mode
+	  and A21(expand) mode. Enable this option, UMT
+	  HW will be working on legacy mode.
+	  In legacy mode, UMT hardware only support one
+	  UMT port.
+
+config LTQ_UMT_EXPAND_MODE
+	bool "Lantiq UMT in A21 expand mode"
+	depends on LTQ_HWMCPY && SOC_GRX500_A21
+	---help---
+	  In expand mode, UMT HW is able to support max
+	  four UMT port. CBM port and DMA channels that
+	  associated with the UMT port can be configurable.
+	  Expand mode use dedicated OCP master to send out
+	  the UMT message instead of using DMA as in the
+	  legacy mode.
+
+endchoice
+
+config LTQ_UMT_518_FW_SG
+	bool "Lantiq MIPS FW TC to support 518 Bonding SG"
+	default n
+	depends on LTQ_HWMCPY
+	---help---
+	  MIPS FW TC to support 518 Bonding scatter-gather
+	  memory to overhaul hardware limitaton.
+
+
 endif
diff --git a/drivers/dma/Makefile b/drivers/dma/Makefile
--- a/drivers/dma/Makefile
+++ b/drivers/dma/Makefile
@@ -38,3 +38,7 @@ obj-$(CONFIG_DMA_SA11X0) += sa11x0-dma.o
 obj-$(CONFIG_MMP_TDMA) += mmp_tdma.o
 obj-$(CONFIG_DMA_OMAP) += omap-dma.o
 obj-$(CONFIG_MMP_PDMA) += mmp_pdma.o
+obj-$(CONFIG_LTQ_UMT_LEGACY_MODE) += ltq_umt_legacy.o
+obj-$(CONFIG_LTQ_UMT_EXPAND_MODE) += ltq_umt_expand.o
+obj-$(CONFIG_LTQ_UMT_518_FW_SG) += mips_tc_sg.o
+obj-$(CONFIG_LTQ_HWMCPY) += ltq_hwmcpy.o
diff --git a/drivers/dma/ltq_hwmcpy.c b/drivers/dma/ltq_hwmcpy.c
new file mode 100755
--- /dev/null
+++ b/drivers/dma/ltq_hwmcpy.c
@@ -0,0 +1,1411 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ *
+ *  HISTORY
+ *  $Date		$Author	$Version               $Comment
+ *  01/02/2015	Zhu YiXin 1.0.0		 hwmemcpy driver
+ *  21/04/2015	Zhu YiXin 1.0.1   Add GCR workaround for UMT(A11 only)
+ *  15/07/2015	Zhu YiXin 1.1.0	optimized hwmcpy driver
+ */
+#define DEBUG
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <lantiq_dmax.h>
+#include <asm/mipsmtregs.h>
+#include <asm/gic.h>
+#include <asm/ltq_vmb.h>
+
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <lantiq_irq.h>
+
+#include <net/datapath_proc_api.h>
+#include <linux/ltq_hwmcpy.h>
+#include "ltq_hwmcpy_addr.h"
+#include "ltq_hwmcpy.h"
+
+#define MCPY_MIN_LEN		64
+#define YLD_PIN_ALLOC		1
+#define MCPY_DRV_VERSION	"1.1.0"
+
+u32 g_mcpy_dbg = MCPY_ERR | MCPY_INFO;
+void __iomem *g_mcpy_addr_base;
+static u32 g_mcpy_min_len = MCPY_MIN_LEN;
+
+static struct mcpy_ctrl ltq_mcpy_ctrl;
+static const char *g_trunksz[MCPY_TKSZ_MAX]
+	= {"512B", "1KB", "2KB", "4KB", "8KB", "16KB", "32KB", "64KB"};
+static const char * const hwmcpy_name[MCPY_PORTS_NUM] = {
+	"HWMCPY PORT 0",
+	"HWMCPY PORT 1",
+	"HWMCPY PORT 2",
+	"HWMCPY PORT 3",
+	"HWMCPY PORT 4",
+	"HWMCPY PORT 5",
+	"HWMCPY PORT 6",
+	"HWMCPY PORT 7",
+};
+static const struct mcpy_cfg mcpy_def_cfg[MCPY_PORTS_NUM] = {
+	/* Prio,  VPE id,  Enable */
+	{1,	0,	MCPY_PORT_ENABLED},
+	{1,	1,	MCPY_PORT_ENABLED},
+	{1,	2,	MCPY_PORT_ENABLED},
+	{1,	3,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+};
+
+#undef MCPY_DBG_DEF
+#define MCPY_DBG_DEF(name, value)	#name,
+char *g_mcpy_dbg_list[] = {
+	MCPY_DBG_LIST
+};
+#undef MCPY_DBG_DEF
+
+/* #define HWMCPY_PROFILE_CYCLE_CNT */
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+extern void *CycleCounter_Create(char *);
+extern void CycleCounter_Start(void *);
+extern void CycleCounter_End(void *);
+static DEFINE_PER_CPU(void *, hwmcpy_cycles);
+static DEFINE_PER_CPU(void *, hw_cycles);
+static DEFINE_PER_CPU(void *, sw_cycles);
+
+static void mcpy_profile_cycle_init(void);
+#endif
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+static void mcpy_profile_cycle_init(void)
+{
+    int i;
+    char name[32];
+
+    for_each_online_cpu(i) {
+        sprintf(name, "hwmcpy%02d", i);
+        per_cpu(hwmcpy_cycles, i) = CycleCounter_Create(name);
+	sprintf(name, "hw%02d", i);
+        per_cpu(hw_cycles, i) = CycleCounter_Create(name);
+	sprintf(name, "sw%02d", i);
+        per_cpu(sw_cycles, i) = CycleCounter_Create(name);
+    }
+}
+#else
+#define mcpy_profile_cycle_init()
+#endif
+
+struct device *mcpy_get_dev(void)
+{
+	return ltq_mcpy_ctrl.dev;
+}
+EXPORT_SYMBOL(mcpy_get_dev);
+
+static const char *mcpy_get_name_by_pid(int pid)
+{
+	return hwmcpy_name[pid];
+}
+
+static void mcpy_sw_reset(void)
+{
+	int timeout = 10000;
+	ltq_mcpy_w32_mask(0x10, 0x10 , MCPY_GCTRL);
+	/* HW auto clean the reset bit */
+	while ((ltq_mcpy_r32(MCPY_GCTRL) & 0x10) == 1 && timeout >= 0)
+		timeout--;	/*wait reset to finish */
+
+	if (timeout < 0 && (ltq_mcpy_r32(MCPY_GCTRL) & 0x10) == 1) {
+		mcpy_dbg(MCPY_ERR, "MCPY soft reset fail!!\n");
+		panic("MCPY soft reset fail!\n");
+	}
+}
+
+static inline void mcpy_set_port_irq_intvl(u32 pid, u32 intvl)
+{
+	ltq_mcpy_w32(intvl, PORT_TO_CNT(pid));
+}
+
+static inline void mcpy_port_irq_enable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT((pid << 1) + 1),
+		BIT((pid << 1) + 1), MCPY_INT_EN);
+}
+
+static inline void mcpy_port_irq_disable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT((pid << 1) + 1), 0, MCPY_INT_EN);
+}
+
+static inline void mcpy_port_yield_enable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT(pid << 1), BIT(pid << 1), MCPY_INT_EN);
+}
+
+static inline void mcpy_port_yield_disable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT(pid << 1), 0, MCPY_INT_EN);
+}
+
+static inline void mcpy_port_yield_ack(u32 pid)
+{
+	ltq_mcpy_w32(BIT(pid << 1), MCPY_INT_STAT);
+}
+
+static inline void mcpy_port_irq_ack(u32 pid)
+{
+	ltq_mcpy_w32(0x3 << (pid << 1), MCPY_INT_STAT);
+}
+
+static inline void mcpy_irq_ack_mcpy_error(void)
+{
+	u32 err = ltq_mcpy_r32(MCPY_INT_STAT);
+
+	mcpy_dbg(MCPY_ERR, "MCPY_INT_STAT: 0x%x, CMD Err: %s, Len Err: %s\n",
+		err, (err & MCPY_CMD_ERR) ? "Yes" : "No",
+		(err & MCPY_LEN_ERR) ? "Yes" : "No");
+	ltq_mcpy_w32((MCPY_CMD_ERR | MCPY_LEN_ERR), MCPY_INT_STAT);
+}
+
+static inline int mcpy_inquire_irq_status(u32 pid)
+{
+	return ltq_mcpy_r32(MCPY_INT_STAT) & (BIT(pid*2) | BIT(pid*2+1));
+}
+
+static void mcpy_intr_setup(struct mcpy_port *pport)
+{
+	if (pport->irq_mode == MCPY_YLD_MODE) { /* Enable Yield */
+		mcpy_port_irq_disable(pport->pid);
+		mcpy_port_yield_enable(pport->pid);
+	} else {
+/* PRIO low -> Enable Interrupt, Note: Enable interrupt must Enable Yield */
+		mcpy_port_yield_enable(pport->pid);
+		mcpy_port_irq_enable(pport->pid);
+	}
+}
+static int mcpy_dma_init(struct mcpy_port *pport)
+{
+	u32 dma_rxch, dma_txch;
+
+	pport->chan.rch = MCPY_DMA_RX_CID + pport->pid * 2;
+	pport->chan.tch = MCPY_DMA_TX_CID + pport->pid * 2;
+	pport->chan.rch_dnum = 2;
+	pport->chan.tch_dnum = 2;
+	pport->chan.rch_dbase =
+		pport->ctrl->phybase + MCPY_DBASE
+		+ pport->pid * MCPY_DBASE_OFFSET;
+	pport->chan.tch_dbase
+		= pport->chan.rch_dbase + pport->chan.rch_dnum * DMA_DESC_SIZE;
+	pport->chan.onoff     = DMA_CH_ON;
+	sprintf(pport->chan.rch_name, "MCPY Port%d RXCH", pport->pid);
+	sprintf(pport->chan.tch_name, "MCPY Port%d TXCH", pport->pid);
+
+	dma_rxch = _DMA_C(pport->ctrl->dma_ctrl_id,
+			pport->ctrl->dma_port_id,
+			pport->chan.rch);
+	mcpy_dbg(MCPY_INIT, "dma_ch: 0x%x, rch_name:%s\n",
+			dma_rxch, pport->chan.rch_name);
+	if (ltq_request_dma(dma_rxch, pport->chan.rch_name) < 0) {
+		mcpy_dbg(MCPY_ERR, "request dma chan [0x%x] fail\n", dma_rxch);
+		goto __MCPY_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_rxch,
+			pport->chan.rch_dbase, pport->chan.rch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_rxch);
+		goto __MCPY_PORT_FAIL;
+	}
+
+	dma_txch = _DMA_C(pport->ctrl->dma_ctrl_id,
+			pport->ctrl->dma_port_id,
+			pport->chan.tch);
+	mcpy_dbg(MCPY_INIT, "dma_ch: 0x%x, rch_name:%s\n",
+		    dma_txch, pport->chan.tch_name);
+	if (ltq_request_dma(dma_txch, pport->chan.tch_name) < 0) {
+		mcpy_dbg(MCPY_ERR, "request dma chan [0x%x] fail\n", dma_txch);
+		goto __MCPY_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_txch,
+		    pport->chan.tch_dbase, pport->chan.tch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_txch);
+		goto __MCPY_PORT_FAIL;
+	}
+
+	if (pport->chan.onoff == DMA_CH_ON) {
+		ltq_dma_chan_on(dma_rxch);
+		ltq_dma_chan_on(dma_txch);
+	} else {
+		ltq_dma_chan_off(dma_rxch);
+		ltq_dma_chan_off(dma_txch);
+	}
+
+	return 0;
+
+__MCPY_PORT_FAIL: /* Disable Flag in MCPY means malfunction */
+	return -ENODEV;
+}
+
+static inline int mcpy_done(u32 pid)
+{
+	u32 resp;
+
+	resp = MCPY_GET_RESPONSE(pid);
+	if (unlikely(!(resp & MRES_DONE))) {
+		mcpy_dbg(MCPY_ERR,
+			"port: %d done bit not set after get interrupt\n",
+			pid);
+		return -1;
+	}
+
+	if (unlikely((resp & MRES_ERROR) != 0)) {
+		mcpy_dbg(MCPY_ERR, "MCPY[%d]: MCPY Error, resp: 0x%x!!!\n",
+			pid, resp);
+		return -1;
+	}
+	return 0;
+}
+
+/* This function should not be called */
+static irqreturn_t mcpy_yld_dummy_handler(int irq, void *dev_id)
+{
+	struct mcpy_port *pport = (struct mcpy_port *)dev_id;
+
+	mcpy_dbg(MCPY_ERR,
+		"MCPY PID[%d]: Yield Dummy handler called!!!\n", pport->pid);
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t mcpy_irq_handler(int irq, void *dev_id)
+{
+	struct mcpy_port *pport;
+
+	pport = (struct mcpy_port *)dev_id;
+
+	/* mcpy_port_irq_disable(pport->pid); */
+	if (unlikely(mcpy_done(pport->pid) < 0))
+		mcpy_irq_ack_mcpy_error();
+
+	/*Clear status and Enable the interrupt */
+	mcpy_port_irq_ack(pport->pid);
+
+	/* Wake up the waiting process */
+	pport->cmd.status = MCPY_CMD_RELEASED;
+	barrier();
+	wake_up_interruptible(&pport->cmd.cmdq);
+
+	/* mcpy_port_irq_enable(pport->pid); */
+
+	return IRQ_HANDLED;
+}
+
+static inline void set_yield_mask(void *mask)
+{
+	u32 yqmask = read_c0_yqmask();
+
+	yqmask |= (*(u32 *)mask);
+	write_c0_yqmask(yqmask);
+	ehb();
+}
+
+void setup_percpu_yqmask(u32 mask, int cpu)
+{
+	preempt_disable();
+	if (cpu != smp_processor_id())
+		smp_call_function_single(cpu,
+			(smp_call_func_t)set_yield_mask,
+			(void *)&mask, 1);
+	else
+		set_yield_mask(&mask);
+	preempt_enable();
+}
+EXPORT_SYMBOL(setup_percpu_yqmask);
+
+void setup_tc_percpu(void *data, int cpu)
+{
+	preempt_disable();
+	if (cpu != smp_processor_id())
+		smp_call_function_single(cpu,
+			(smp_call_func_t)umt_init,
+			(void *)data, 1);
+	else
+		umt_init(data);
+	preempt_enable();
+}
+EXPORT_SYMBOL(setup_tc_percpu);
+
+static inline void mcpy_yield(u32 yield_pin)
+{
+	mips_mt_yield(1 << yield_pin);
+}
+
+static inline void mcpy_yield_handler(struct mcpy_port *pport)
+{
+	if (unlikely(mcpy_done(pport->pid) < 0))
+		mcpy_irq_ack_mcpy_error();
+
+	mcpy_port_yield_ack(pport->pid);
+	/* gic_intr_ack(pport->yield_no); */
+}
+
+static void mcpy_intr_init(void)
+{
+	ltq_mcpy_w32(0x3FFFF, MCPY_INTERNAL_INT_EN);
+	ltq_mcpy_w32(0x0,     MCPY_INTERNAL_INT_MASK);
+	ltq_mcpy_w32(0x0,     MCPY_INT_EN);
+	ltq_mcpy_w32(0x0,     MCPY_INT_MASK);
+}
+
+inline void mcpy_eva_cfg(void)
+{
+	 u32 cfg_reg;
+
+	 cfg_reg = read_c0_segctl0();
+	 ltq_mcpy_w32(cfg_reg & 0xFFFF, MCPY_MIPS_CFG(0));
+	 ltq_mcpy_w32((cfg_reg >> 16) & 0xFFFF, MCPY_MIPS_CFG(1));
+
+	 cfg_reg = read_c0_segctl1();
+	 ltq_mcpy_w32(cfg_reg & 0xFFFF, MCPY_MIPS_CFG(2));
+	 ltq_mcpy_w32((cfg_reg >> 16) & 0xFFFF, MCPY_MIPS_CFG(3));
+
+	 cfg_reg = read_c0_segctl2();
+	 ltq_mcpy_w32(cfg_reg & 0xFFFF, MCPY_MIPS_CFG(4));
+	 ltq_mcpy_w32((cfg_reg >> 16) & 0xFFFF, MCPY_MIPS_CFG(5));
+}
+
+static void mcpy_cmd_init(struct mcpy_cmd *pcmd, struct mcpy_port *pport)
+{
+	pcmd->port = pport;
+	mutex_init(&pcmd->mtx_lock);
+	init_waitqueue_head(&pcmd->cmdq);
+	pcmd->status = MCPY_CMD_RELEASED;
+}
+
+#if defined(YLD_PIN_ALLOC) && YLD_PIN_ALLOC
+static int mcpy_get_yield_pin(unsigned int vpe_id)
+{
+	unsigned int yld_pin_bitmap;
+	int i;
+
+	yld_pin_bitmap = vmb_yr_get(vpe_id, 1);
+	if (!(yld_pin_bitmap & 0xFFFF))
+		return -1;
+	for (i = 0; i < 16; i++) {
+		if (yld_pin_bitmap & (1 << i))
+			return i;
+	}
+
+	return -1;
+}
+#else
+static int mcpy_get_yield_pin(unsigned int vpe_id)
+{
+	static int yld_pin = 0xF;
+	return yld_pin--;
+}
+
+#endif
+
+static int mcpy_port_init(struct mcpy_port *pport, u32 pid)
+{
+	int yld_pin;
+	pport->pid = pid;
+	pport->name = mcpy_get_name_by_pid(pid);
+
+	if (pport->status & MCPY_PORT_DISABLED)
+		goto mcpy_port_err;
+
+	if (pport->irq_mode == MCPY_YLD_MODE) {
+		if (!cpu_online(pport->vpe_id)) {
+			mcpy_dbg(MCPY_INIT, "HWMCPY vpe: %d not online!\n",
+				pport->vpe_id);
+			goto mcpy_port_err;
+		}
+		yld_pin = mcpy_get_yield_pin(pport->vpe_id);
+		if (yld_pin == -1) { /* No Yield resource */
+			mcpy_dbg(MCPY_INIT, "HWMCPY Allocate Yield resource Failure !\n");
+			goto mcpy_port_err;
+		}
+		pport->yld_pin = yld_pin;
+	}
+
+	if (mcpy_dma_init(pport) < 0) {
+		mcpy_dbg(MCPY_ERR, "HWMCPY DMA init failure!\n");
+		goto mcpy_port_err;
+	}
+
+	mcpy_intr_setup(pport);
+	mcpy_set_port_irq_intvl(pid, pport->irq_intvl);
+	spin_lock_init(&pport->port_lock);
+	mcpy_cmd_init(&pport->cmd, pport);
+
+	if (pport->irq_mode == MCPY_YLD_MODE) {
+		mcpy_dbg(MCPY_INIT, "Register Yield: irq:%d\n",
+				pport->yld_no);
+		if (devm_request_irq(pport->ctrl->dev, pport->yld_no,
+			mcpy_yld_dummy_handler,
+			IRQF_DISABLED, pport->name, (void *)pport)) {
+			mcpy_dbg(MCPY_ERR, "%s: Request yield irq: %d fail\n",
+				pport->name, pport->irq_no);
+			goto mcpy_port_err;
+		}
+		setup_percpu_yqmask((1 << pport->yld_pin), pport->vpe_id);
+	}
+	if (pport->irq_no != 0) {
+		mcpy_dbg(MCPY_INIT,
+			"Register interrupt: irq:%d\n", pport->irq_no);
+		if (devm_request_irq(pport->ctrl->dev, pport->irq_no,
+			mcpy_irq_handler,
+			IRQF_DISABLED, pport->name, (void *)pport)) {
+			mcpy_dbg(MCPY_ERR, "%s: Request irq: %d fail\n",
+				pport->name, pport->irq_no);
+			goto mcpy_port_err;
+		}
+	}
+	if (pport->irq_mode == MCPY_YLD_MODE) {
+		disable_irq(pport->yld_no);
+	/* NOTE: gic_yield_setup API must be called after register the irq
+	handler as it will clear the percpu mask in gic while request_irq
+	will set the per cpu mask via set irq affinity function.
+	For Yield interrupt, it must be cleared so that gic will not handle
+	Yield interrupt as it will set RMASK in gic*/
+		if (gic_yield_setup(pport->vpe_id,
+			pport->yld_pin, pport->yld_no)) {
+			mcpy_dbg(MCPY_INIT, "Yield Setup Fail: vpe:%d, pin:%d, yld_no:%d\n",
+				pport->vpe_id, pport->yld_pin, pport->yld_no);
+			goto mcpy_port_err;
+		}
+	}
+
+	return 0;
+
+mcpy_port_err:
+	pport->status = MCPY_PORT_DISABLED;
+	return -ENODEV;
+}
+
+static int mcpy_port_cfg_init(struct platform_device *pdev,
+			struct mcpy_port *port, int pid)
+{
+	struct device_node *np = pdev->dev.of_node;
+	char res_name[32];
+	u32 cfg_res[5];
+	struct mcpy_port_ctrl *port_ctrl = port->pctrl;
+
+	sprintf(res_name, "lantiq,mcpy-ch%d", pid);
+	if (of_property_read_u32_array(np, res_name,
+			cfg_res, ARRAY_SIZE(cfg_res)) < 0) {
+		port->prio = mcpy_def_cfg[pid].prio;
+		port->vpe_id = mcpy_def_cfg[pid].vpe_id;
+		port->status = mcpy_def_cfg[pid].en;
+		port->trunk_size = MCPY_DEF_TRKSZ;
+		port->irq_intvl = MCPY_DEF_IRQ_INTVAL;
+	} else {
+		port->prio = cfg_res[0] == MCPY_PRIO_LOW ?
+				MCPY_PRIO_LOW : MCPY_PRIO_HIGH;
+		port->vpe_id = cfg_res[1] < num_possible_cpus() ?
+				(int)cfg_res[1] : -1;
+		port->trunk_size = cfg_res[2] >= MCPY_TKSZ_MAX ?
+				MCPY_DEF_TRKSZ : cfg_res[2];
+		port->irq_intvl = cfg_res[3] == 0 ?
+				MCPY_DEF_IRQ_INTVAL : cfg_res[3];
+		port->status = cfg_res[4] == 1 ?
+				MCPY_PORT_ENABLED : MCPY_PORT_DISABLED;
+	}
+
+	port->pid = pid;
+	sprintf(res_name, "yld%d", pid);
+	port->yld_no = platform_get_irq_byname(pdev, res_name);
+	if (port->yld_no <= 0) {
+		mcpy_dbg(MCPY_ERR,
+			"Cannot get mcpy ch %d yield irq!!\n", pid);
+		return -ENODEV;
+	}
+	sprintf(res_name, "irq%d", pid);
+	port->irq_no = platform_get_irq_byname(pdev, res_name);
+	if (port->irq_no <= 0) {
+		mcpy_dbg(MCPY_ERR,
+			"Cannot get mcpy ch %d irq !!\n", pid);
+		return -ENODEV;
+	}
+	port->irq_mode = port->prio;
+
+	if (port->status & MCPY_PORT_DISABLED)
+		return -ENODEV;
+	if (port->vpe_id >= 0 && port->vpe_id < num_possible_cpus()) {
+		if (port->prio)
+			port_ctrl->hi_per_vpe[port->vpe_id] = pid;
+		else
+			port_ctrl->lo_per_vpe[port->vpe_id] = pid;
+		port->status |= MCPY_PORT_RESERVED;
+	} else {
+		if (port->prio == MCPY_PRIO_LOW)
+			port_ctrl->prio_lo_map |= BIT(pid);
+	}
+
+	return 0;
+}
+
+static void mcpy_init(struct platform_device *pdev, struct mcpy_ctrl *pctrl)
+{
+	int i;
+	struct mcpy_port *pport;
+	struct mcpy_port_ctrl *port_ctrl;
+	u32 port_map = 0;
+
+	pctrl->dma_ctrl_id = DMA3;
+	pctrl->dma_port_id = DMA3_PORT;
+
+	mcpy_sw_reset();
+	mcpy_intr_init();
+
+	port_ctrl = &pctrl->port_ctrl;
+	port_ctrl->ctrl = pctrl;
+	spin_lock_init(&port_ctrl->mcpy_lock);
+
+	port_ctrl->hi_per_vpe = kmalloc(num_possible_cpus() * sizeof(u32),
+					GFP_KERNEL);
+	port_ctrl->lo_per_vpe = kmalloc(num_possible_cpus() * sizeof(u32),
+					GFP_KERNEL);
+	if (!port_ctrl->hi_per_vpe || !port_ctrl->lo_per_vpe) {
+		mcpy_dbg(MCPY_ERR, "alloc hi/lo array fail!\n");
+		return;
+	}
+	memset(port_ctrl->hi_per_vpe, -1, num_possible_cpus() * sizeof(u32));
+	memset(port_ctrl->lo_per_vpe, -1, num_possible_cpus() * sizeof(u32));
+	port_ctrl->lo_idx = 0;
+
+	for (i = 0; i < MCPY_PORTS_NUM; i++) {
+		pport = &port_ctrl->ports[i];
+		pport->ctrl = pctrl;
+		pport->pctrl = port_ctrl;
+		if (mcpy_port_cfg_init(pdev, pport, i) < 0)
+			continue;
+
+		if (mcpy_port_init(pport, i) < 0) {
+			port_ctrl->prio_lo_map &= ~(BIT(i));
+			continue;
+		}
+		port_map |= BIT(i);
+	}
+	ltq_mcpy_w32_mask((0xFF << 8), (port_map << 8 | 1), MCPY_GCTRL);
+
+	/* Copy GCR register to MCPY register */
+	mcpy_eva_cfg();
+}
+
+static inline struct mcpy_port *mcpy_get_port_by_id(u32 pid)
+{
+	return &ltq_mcpy_ctrl.port_ctrl.ports[pid];
+}
+
+static inline struct mcpy_port *mcpy_get_resv_port(u32 pid, u32 prio)
+{
+	struct mcpy_port *pport;
+
+	if (unlikely(pid >= MCPY_PORTS_NUM))
+		return NULL;
+
+	pport = mcpy_get_port_by_id(pid);
+	if (unlikely(pport->prio != prio ||
+		!(pport->status & MCPY_PORT_RESERVED)))
+		return NULL;
+
+	return pport;
+}
+/*
+ *  Find an available hwmcpy port
+ *  1. if Prio is high, then search dedicated port for that VPE.
+ *  High prio MCPY channel doesn't support dynamic allocate
+ *  due to the Yield feature.
+ *  2. if prio is low, then search dedicated port for that VPE, else search
+ *     on the prio_lo_map for the free hwmcpy port.
+ */
+static struct mcpy_port *mcpy_get_port_by_prio(u32 prio, u32 cpuid)
+{
+	int i;
+	struct mcpy_port *pport = NULL;
+	struct mcpy_port_ctrl *pctrl;
+
+	pctrl = &ltq_mcpy_ctrl.port_ctrl;
+	if (prio == MCPY_PRIO_HIGH) {
+		if (likely(pctrl->hi_per_vpe[cpuid] < MCPY_PORTS_NUM))
+			return &pctrl->ports[pctrl->hi_per_vpe[cpuid]];
+		else
+			return NULL;
+	} else { /* LOW prio */
+		if (pctrl->lo_per_vpe[cpuid] < MCPY_PORTS_NUM)
+			pport = &pctrl->ports[pctrl->hi_per_vpe[cpuid]];
+		else if (pctrl->prio_lo_map) {
+			spin_lock_bh(&pctrl->mcpy_lock);
+			for (i = pctrl->lo_idx; i < MCPY_PORTS_NUM; i++) {
+				if (pctrl->prio_lo_map & BIT(i)) {
+					pport = &pctrl->ports[i];
+					pctrl->lo_idx
+						= (i + 1) % MCPY_PORTS_NUM;
+					break;
+				}
+			}
+			if (!pport) {
+				for (i = 0; i < pctrl->lo_idx; i++) {
+					if (pctrl->prio_lo_map & BIT(i)) {
+						pport = &pctrl->ports[i];
+						pctrl->lo_idx = i + 1;
+						break;
+					}
+				}
+			}
+			spin_unlock_bh(&pctrl->mcpy_lock);
+		}
+		return pport;
+	}
+
+	return NULL;
+}
+
+/*
+static void mcpy_dma_map(struct device *dev, void *addr,
+				size_t size, u32 flags)
+{
+	dma_addr_t dma_addr;
+	enum dma_data_direction dir = DMA_NONE;
+
+	if ((flags & (DMA_TO_DEVICE | DMA_FROM_DEVICE))
+		== (DMA_TO_DEVICE | DMA_FROM_DEVICE)) {
+		dir = DMA_BIDIRECTIONAL;
+	} else if ((flags & (DMA_TO_DEVICE | DMA_FROM_DEVICE)) == 0) {
+		dir = DMA_NONE;
+	} else {
+		dir = flags & (DMA_TO_DEVICE | DMA_FROM_DEVICE);
+	}
+
+	if (dir != DMA_NONE) {
+		dma_addr = dma_map_single(dev, (void *)addr, size, dir);
+		dma_unmap_single(dev, dma_addr, size, dir);
+	}
+}
+*/
+
+int mcpy_set_cpy_cmd(struct mcpy_cmd *pcmd, u32 dst,
+		u32 src, u32 len, enum mcpy_type mode)
+{
+	u32 dioc, sioc, ie, gather, last, ipc;
+	u32 cmd0, cmd1, cmd2, cmd3;
+	u32 pid;
+	const u32 sphy = 1; /* fixed use physical address */
+	const u32 dphy = 1;
+
+	src = __pa(src);
+	dst = __pa(dst);
+	mcpy_dbg(MCPY_DBG,
+		   "dst: 0x%x, src:0x%x, len:%d, mode: %d\n",
+		   dst, src, len, mode);
+
+	pid = pcmd->port->pid;
+	gather = pcmd->flags & MCPY_CMD_GATHER ? 1 : 0;
+	last   = pcmd->flags & MCPY_CMD_LAST   ? 1 : 0;
+	ipc    = pcmd->flags & MCPY_CMD_IPC    ? 1 : 0;
+
+	if (gather == 0 || last == 1)
+		ie = 1;
+	else
+		ie = 0;
+
+	dioc = sioc = 0;
+	switch (mode) {
+	case MCPY_PHY_TO_PHY:
+		dioc = sioc = 0;
+		break;
+	case MCPY_PHY_TO_IOCU:
+		dioc = 1;
+		sioc = 0;
+		break;
+	case MCPY_IOCU_TO_PHY:
+		sioc = 1;
+		dioc = 0;
+		break;
+	case MCPY_IOCU_TO_IOCU:
+		dioc = sioc = 1;
+		break;
+	default:
+		mcpy_dbg(MCPY_ERR, "MCPY type err!!\n");
+		return -EINVAL;
+	}
+
+	cmd0 = ((sphy & 0x1) << 31) | ((dphy & 0x1) << 30)
+		   | ((ie & 0x1) << 29) | ((pid & 0x7) << 26)
+		   | ((gather & 0x1) << 25) | ((last & 0x1) << 24)
+		   | ((ipc & 0x1) << 23)
+		   | ((pcmd->trunk_size & 0x7) << 20) | (len & 0xFFFFF);
+	cmd1 = src;
+	cmd2 = dst;
+	cmd3 = ((dioc & 0x1) << 30) | ((sioc & 0x1) << 29)
+		   | (pcmd->context & 0x1FFFFFFF);
+
+
+	mcpy_dbg(MCPY_DBG,
+		   "Pid:%d, CMD0:0x%x, CMD1:0x%x, CMD2: 0x%x, CMD3: 0x%x\n",
+		   pid, cmd0, cmd1, cmd2, cmd3);
+
+	/* Enable irq before issue the command */
+	if (in_interrupt() && ie)
+		enable_irq(pcmd->port->yld_no);
+
+	while (!(ltq_mcpy_r32(MCPY_CMD(3, pid)) & MCPY_OWN_BIT))
+		;
+
+	ltq_mcpy_w32(cmd0, MCPY_CMD(0, pid));
+	ltq_mcpy_w32(cmd1, MCPY_CMD(1, pid));
+	ltq_mcpy_w32(cmd2, MCPY_CMD(2, pid));
+	wmb();
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_Start(per_cpu(hw_cycles, smp_processor_id())); 
+#endif
+
+	ltq_mcpy_w32(cmd3, MCPY_CMD(3, pid));
+
+	return 0;
+}
+
+static inline void mcpy_reset_cmd(struct mcpy_cmd *pcmd)
+{
+	pcmd->flags = 0;
+	pcmd->status = MCPY_CMD_RELEASED;
+}
+
+static inline void mcpy_port_release(struct mcpy_port *pport)
+{
+	mcpy_reset_cmd(&pport->cmd);
+}
+
+/*
+ * HW memcpy API: reserve one HW memcopy port
+ * Only free Low priority MCPY can be reserved
+ * return: pid<0-7>-Success/-1 -Failure
+ */
+int ltq_mcpy_reserve(void)
+{
+	struct mcpy_port_ctrl *pctrl;
+	int pid = -1;
+	int i;
+
+	pctrl = &ltq_mcpy_ctrl.port_ctrl;
+	spin_lock_bh(&pctrl->mcpy_lock);
+
+	if (pctrl->prio_lo_map) {
+		for (i = 0; i < MCPY_PORTS_NUM; i++) {
+			if (pctrl->prio_lo_map & BIT(i)) {
+				pid = i;
+				pctrl->prio_lo_map &= ~BIT(i);
+				pctrl->ports[i].status |= MCPY_PORT_RESERVED;
+			}
+		}
+	}
+
+	spin_unlock_bh(&pctrl->mcpy_lock);
+
+	return pid;
+}
+EXPORT_SYMBOL(ltq_mcpy_reserve);
+
+/*
+ * HW memcpy API: release one HW memcopy port
+ * @pid: reserved HW memcopy port id
+ */
+void ltq_mcpy_release(u32 pid)
+{
+	struct mcpy_port_ctrl *pctrl;
+
+	if (pid >= MCPY_PORTS_NUM)
+		return;
+
+	pctrl = &ltq_mcpy_ctrl.port_ctrl;
+	spin_lock_bh(&pctrl->mcpy_lock);
+	pctrl->prio_lo_map |= BIT(pid);
+	pctrl->ports[pid].status &= ~MCPY_PORT_RESERVED;
+	spin_unlock_bh(&pctrl->mcpy_lock);
+
+	return;
+}
+EXPORT_SYMBOL(ltq_mcpy_release);
+
+/*
+ * HW memcopy API
+ *   @dst:      Destionation address
+ *   @src:      Source address
+ *   @len:      Number of bytes to be copied
+ *   @pid:      HWMCPY Port ID if reserved flag is provided
+ *                 otherwise it will be ignored
+ *   @mode:   PHY_to_PHY, PHY_to_L2Cache,
+ *                 L2Cache_to_PHY, L2Cache_to_L2Cache
+ *                 (MCPY_PHY_TO_PHY, MCPY_PHY_TO_IOCU,
+ *                 MCPY_IOCU_TO_PHY,MCPY_IOCU_TO_IOCU)
+ *   @flags:    IPC/trunksize/port reserved
+ *   Note:       Non-Sleep process must use Yield Enabled Port and vice versa.
+ */
+void *ltq_hwmemcpy(void *dst, const void *src, u32 len,
+				u32 pid, enum mcpy_type mode, u32 flags)
+{
+	struct mcpy_port *pport;
+	enum mcpy_prio prio = MCPY_PRIO_HIGH;
+	u32 cpuid = smp_processor_id();
+	int sleep_en = !in_interrupt();
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_Start(per_cpu(hwmcpy_cycles, smp_processor_id())); 
+#endif
+	if ((u32)src < (u32)dst && ((u32)src + len) > (u32)dst)
+		goto __SW_MEMMOVE_PATH;
+
+	if (len < g_mcpy_min_len)
+		goto __SW_MEMCPY_PATH;
+
+	if (unlikely(mode >= MCPY_SW_CPY))
+		goto __SW_MEMCPY_PATH;
+	/* TODO: Check TLB address */
+
+	if (unlikely(sleep_en))
+		prio = MCPY_PRIO_LOW;
+
+	if (unlikely((flags & HWMCPY_F_RESERVED) != 0)) {
+		pport = mcpy_get_resv_port(pid, prio);
+		if (unlikely(!pport))
+			goto __SW_MEMCPY_PATH;
+	} else {
+		pport = mcpy_get_port_by_prio((u32)prio, cpuid);
+		if (!pport) {
+			mcpy_dbg(MCPY_ERR,
+				"Cannot find a valid port: prio: %d\n",	prio);
+			goto __SW_MEMCPY_PATH;
+		}
+	}
+
+	if (sleep_en)
+		mutex_lock(&pport->cmd.mtx_lock);
+
+	/*generate cmd and wait */
+	pport->cmd.status = MCPY_CMD_PROCESSING;
+	if (flags & HWMCPY_F_IPC)
+		pport->cmd.flags |= MCPY_CMD_IPC;
+
+	if (flags & HWMCPY_F_CHKSZ_SET)
+		pport->cmd.trunk_size = (flags & 0x1C) >> 2;
+	else
+		pport->cmd.trunk_size = pport->trunk_size;
+
+	mcpy_set_cpy_cmd(&pport->cmd, (u32)dst, (u32)src, len, mode);
+
+	/*Sleep or Yield */
+	if (sleep_en) {
+		wait_event_interruptible(pport->cmd.cmdq,
+			pport->cmd.status == MCPY_CMD_RELEASED);
+		mcpy_dbg(MCPY_DBG, "pid: %d wakeup\n", pport->pid);
+	} else {
+		do {
+			mcpy_dbg(MCPY_DBG, "HWMEMCPY Using Yield\n");
+			mcpy_yield(pport->yld_pin);
+		} while (!mcpy_inquire_irq_status(pport->pid));
+	}
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_End(per_cpu(hw_cycles, smp_processor_id())); 
+#endif
+
+	/*Wake up and clean function */
+	if (sleep_en) {
+		if (unlikely(pport->cmd.status != MCPY_CMD_RELEASED)) {
+			mcpy_dbg(MCPY_ERR, "Prio Low: hw memcopy interrupted!!\n");
+			/*TODO:  Should recover */
+			mcpy_port_release(pport);
+			mutex_unlock(&pport->cmd.mtx_lock);
+			return NULL;
+		}
+		mutex_unlock(&pport->cmd.mtx_lock);
+	} else { /* Yield Wake up*/
+		mcpy_yield_handler(pport);
+		/* Disable Yield irq if not in use */
+		disable_irq(pport->yld_no);
+	}
+
+	pport->mib_bytes += len;
+	pport->mib_use_times += 1;
+	mcpy_port_release(pport);
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_End(per_cpu(hwmcpy_cycles, smp_processor_id()));
+#endif    
+	return dst;
+
+__SW_MEMCPY_PATH:
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_Start(per_cpu(sw_cycles, smp_processor_id())); 
+#endif
+	memcpy(dst, src, len);
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_End(per_cpu(sw_cycles, smp_processor_id())); 
+#endif
+	return dst;
+__SW_MEMMOVE_PATH:
+	return memmove(dst, src, len);
+}
+EXPORT_SYMBOL(ltq_hwmemcpy);
+
+/*
+ * HW Memcpy API
+ * Copy small pieces scattered memory content into a whole continues memory
+ * Note: User only required call this function once to do the gathering.
+ *
+ *   @dst:            Destionation address
+ *   @src:            Source fragment list
+ *   @frag_num:    Number of fragments to be copied
+ *   @pid:            HWMCPY Port ID if reserved flag is provided
+ *                        otherwise it will be ignored
+ *   @mode:   PHY_to_PHY, PHY_to_L2Cache,
+ *                 L2Cache_to_PHY,  L2Cache_to_L2Cache
+ *                 (MCPY_PHY_TO_PHY, MCPY_PHY_TO_IOCU,
+ *                 MCPY_IOCU_TO_PHY,MCPY_IOCU_TO_IOCU)
+ *   @flags:    IPC/trunksize/port reserved
+ *   return:     0 if success
+ *                 -1 if failure
+ *   @limitation:  This API is not able to handle memory overlap copy.
+ *
+ */
+
+int ltq_hwmcpy_sg(void *dst, const struct mcpy_frag *src, u32 frag_num,
+				u32 pid, enum mcpy_type mode, u32 flags)
+{
+	struct mcpy_port *pport;
+	enum mcpy_prio prio = MCPY_PRIO_HIGH;
+	u32 cpuid = smp_processor_id();
+	int sleep_en = !in_interrupt();
+	int i, total_len = 0;
+
+	/* TODO: Check TLB address */
+	if (unlikely(sleep_en))
+		prio = MCPY_PRIO_LOW;
+
+	if (flags & HWMCPY_F_RESERVED) {
+		pport = mcpy_get_resv_port(pid, prio);
+		if (unlikely(!pport))
+			return -1;
+	} else {
+		pport = mcpy_get_port_by_prio(prio, cpuid);
+		if (!pport) {
+			mcpy_dbg(MCPY_ERR,
+				"Cannot find a valid port: prio: %d\n", prio);
+			return -1;
+		}
+	}
+
+	if (sleep_en)
+		mutex_lock(&pport->cmd.mtx_lock);
+
+	pport->cmd.status = MCPY_CMD_PROCESSING;
+	for (i = 0; i < frag_num; i++) {
+		/*generate cmd and wait */
+		/* ??? required for every segment or only for last one? */
+		if (flags & HWMCPY_F_IPC)
+			pport->cmd.flags |= MCPY_CMD_IPC;
+
+		if ((flags & HWMCPY_F_CHKSZ_SET) != 0)
+			pport->cmd.trunk_size = (flags & 0x1C) >> 2;
+		else
+			pport->cmd.trunk_size = pport->trunk_size;
+
+		pport->cmd.flags |= MCPY_CMD_GATHER;
+
+		if (unlikely(i + 1 >= frag_num))
+			pport->cmd.flags |= MCPY_CMD_LAST;
+
+		mcpy_set_cpy_cmd(&pport->cmd, (u32)dst,
+			(u32)(src[i].ptr + src[i].offset),
+			src[i].size, mode);
+		total_len += src[i].size;
+	}
+
+	/*Sleep or Yield */
+	if (sleep_en)
+		wait_event_interruptible(pport->cmd.cmdq,
+			pport->cmd.status == MCPY_CMD_RELEASED);
+
+	else {
+		do {
+			mcpy_dbg(MCPY_DBG, "HWMEMCPY Using Yield\n");
+			mcpy_yield(pport->yld_pin);
+		} while (!mcpy_inquire_irq_status(pport->pid));
+	}
+
+	/*Wake up and clean function */
+	if (sleep_en) {
+		if (pport->cmd.status != MCPY_CMD_RELEASED) {
+			mcpy_dbg(MCPY_ERR, "Prio Low: hw memcopy gathering interrupted!!\n");
+			mcpy_port_release(pport);
+			mutex_unlock(&pport->cmd.mtx_lock);
+			return -1;
+		}
+		mutex_unlock(&pport->cmd.mtx_lock);
+	} else { /* Yield Wake up*/
+		mcpy_yield_handler(pport);
+		disable_irq(pport->yld_no);
+	}
+
+	pport->mib_bytes += total_len;
+	pport->mib_use_times += 1;
+	mcpy_port_release(pport);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_hwmcpy_sg);
+
+/**
+ *  Proc Functions
+ */
+static int mcpy_pctrl_read_proc(struct seq_file *s, void *v)
+{
+	struct mcpy_ctrl *pctrl = s->private;
+	struct mcpy_port_ctrl *port_ctrl = &pctrl->port_ctrl;
+	int i;
+
+	seq_puts(s, "\n MCPY Port Control\n");
+	seq_puts(s, "-----------------------------------------\n");
+	seq_printf(s, "MCPY base addr:0x%x, physical base addr: 0x%x\n",
+		(u32)pctrl->membase, pctrl->phybase);
+	seq_printf(s, "Low prio free mcpy port bitmap: 0x%x\n",
+		port_ctrl->prio_lo_map);
+	for (i = 0; i < num_possible_cpus(); i++) {
+		seq_printf(s, "VPE id: %d, Reserved: ", i);
+		if (port_ctrl->hi_per_vpe[i] <= MCPY_PORTS_NUM) {
+			seq_printf(s, "High Prio Port: %d ",
+				port_ctrl->hi_per_vpe[i]);
+		} else if (port_ctrl->lo_per_vpe[i] <= MCPY_PORTS_NUM) {
+			seq_printf(s, "Low Prio Port: %d ",
+				port_ctrl->lo_per_vpe[i]);
+		} else
+			seq_puts(s, "No mcpy ports");
+
+		seq_puts(s, "\n");
+	}
+	seq_printf(s, "MCPY Min Len threshold: %d\n", g_mcpy_min_len);
+
+	return 0;
+}
+
+
+static int mcpy_pctrl_read_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mcpy_pctrl_read_proc, PDE_DATA(inode));
+}
+
+static const struct file_operations mcpy_pctrl_proc_fops = {
+	.open           = mcpy_pctrl_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = single_release,
+};
+
+
+static int mcpy_dbg_read_proc(struct seq_file *s, void *v)
+{
+	int i;
+
+	seq_puts(s, "MCPY DBG Enable: ");
+	for (i = 0; i < DBG_MAX; i++) {
+		if (g_mcpy_dbg & BIT(i))
+			seq_printf(s, "%s ", g_mcpy_dbg_list[i]);
+	}
+	seq_puts(s, "\n");
+
+	return 0;
+}
+
+
+static int mcpy_dbg_read_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mcpy_dbg_read_proc, PDE_DATA(inode));
+}
+
+static ssize_t mcpy_dbg_write(struct file *file, const char __user *buf,
+			size_t count, loff_t *data)
+{
+	char str[32];
+	int len, rlen, i, j;
+	int num, enable = 0;
+	char *param_list[20];
+	len = count < sizeof(str) ? count : sizeof(str) - 1;
+	rlen = len - copy_from_user(str, buf, len);
+	str[rlen] = 0;
+
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if ((dp_strcmpi(param_list[0], "enable") == 0)
+		|| (dp_strcmpi(param_list[0], "en") == 0)) {
+		enable = 1;
+	} else if (dp_strcmpi(param_list[0], "disable") == 0
+		|| (dp_strcmpi(param_list[0], "dis") == 0)) {
+		enable = 0;
+	} else {
+		goto proc_dbg_help;
+	}
+	if (num <= 1) {
+		g_mcpy_dbg = 0;
+		return count;
+	}
+	for (i = 1; i < num; i++) {
+		for (j = 0; j < ARRAY_SIZE(g_mcpy_dbg_list); j++) {
+			if (dp_strcmpi(param_list[i],
+					g_mcpy_dbg_list[j]) == 0) {
+				if (enable)
+					g_mcpy_dbg |= BIT(j);
+				else
+					g_mcpy_dbg &= ~(BIT(j));
+				break;
+			}
+		}
+	}
+
+	return count;
+
+proc_dbg_help:
+	mcpy_dbg(MCPY_INFO, "echo <enable/disable> err/event/init/info/dbg > dbg\n");
+
+	return count;
+}
+
+static const struct file_operations mcpy_dbg_proc_fops = {
+	.open           = mcpy_dbg_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.write		= mcpy_dbg_write,
+	.release        = single_release,
+};
+
+static void *mcpy_port_seq_start(struct seq_file *s, loff_t *pos)
+{
+	struct mcpy_ctrl *pctrl = s->private;
+	struct mcpy_port *pport;
+
+	if (*pos >= MCPY_PORTS_NUM)
+		return NULL;
+
+	pport = &pctrl->port_ctrl.ports[*pos];
+
+	return pport;
+}
+
+static void *mcpy_port_seq_next(struct seq_file *s, void *v, loff_t *pos)
+{
+	struct mcpy_ctrl *pctrl = s->private;
+	struct mcpy_port *pport;
+
+	if (++*pos >= MCPY_PORTS_NUM)
+		return NULL;
+	pport = &pctrl->port_ctrl.ports[*pos];
+	return pport;
+}
+
+static void mcpy_port_seq_stop(struct seq_file *s, void *v)
+{
+
+}
+
+static int mcpy_port_seq_show(struct seq_file *s, void *v)
+{
+	struct mcpy_port *pport = (struct mcpy_port *)v;
+
+	seq_printf(s, "----------HWMCPY Port[%d] info----------\n",
+		pport->pid);
+	seq_printf(s, "irq no: %d, yield no: %d, irq interval: %d\n",
+		pport->irq_no, pport->yld_no, pport->irq_intvl);
+	seq_printf(s, "priority: %s\n",
+		(pport->prio == MCPY_PRIO_HIGH) ?
+			"High Priority" : "Low Priority");
+	if (pport->irq_mode == MCPY_YLD_MODE)
+		seq_printf(s, "IRQ mode: Yield, Yield pin:%d, map to CPU:%d\n",
+			pport->yld_pin, pport->vpe_id);
+	else
+		seq_puts(s, "IRQ mode: Interrupt\n");
+	seq_printf(s, "port status: %s %s\n",
+		(pport->status & MCPY_PORT_DISABLED) ? "Disabled" : "Enabled",
+		(pport->status & MCPY_PORT_RESERVED) ? "Reserved" : "");
+	seq_printf(s, "dma rxch_id: %d, rch_base: 0x%x, rch_des_num: %d\n",
+		pport->chan.rch, pport->chan.rch_dbase,
+		pport->chan.rch_dnum);
+	seq_printf(s, "dma txch_id: %d, tch_base: 0x%x, tch_des_num: %d\n",
+		pport->chan.tch, pport->chan.tch_dbase,
+		pport->chan.tch_dnum);
+	seq_printf(s, "dma chan on/off: %s\n",
+		pport->chan.onoff == DMA_CH_ON ? "ON" : "OFF");
+	seq_printf(s, "trunk size: %s\n", g_trunksz[pport->trunk_size]);
+	seq_printf(s, "mib: use times: %llu, copied bytes: %llu\n",
+		pport->mib_use_times, pport->mib_bytes);
+
+	return 0;
+}
+
+static const struct seq_operations mcpy_port_seq_ops = {
+	.start = mcpy_port_seq_start,
+	.next = mcpy_port_seq_next,
+	.stop = mcpy_port_seq_stop,
+	.show = mcpy_port_seq_show,
+};
+
+static int mcpy_port_seq_open(struct inode *inode, struct file *file)
+{
+	int ret = seq_open(file, &mcpy_port_seq_ops);
+
+	if (ret == 0) {
+		struct seq_file *m = file->private_data;
+		m->private = PDE_DATA(inode);
+	}
+	return ret;
+}
+
+static const struct file_operations mcpy_port_proc_fops = {
+	.open = mcpy_port_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+static int mcpy_proc_init(struct mcpy_ctrl *pctrl)
+{
+	char proc_name[64] = {0};
+	struct proc_dir_entry *entry;
+
+	strcpy(proc_name, "driver/hwmcpy");
+	pctrl->proc = proc_mkdir(proc_name, NULL);
+	if (!pctrl->proc)
+		return -ENOMEM;
+
+	entry = proc_create_data("mcpy_ctrl", 0, pctrl->proc,
+			&mcpy_pctrl_proc_fops, pctrl);
+	if (!entry)
+		goto err1;
+	entry = proc_create_data("port_info", 0, pctrl->proc,
+			&mcpy_port_proc_fops, pctrl);
+	if (!entry)
+		goto err2;
+	/* entry = proc_create_data("umt_info", 0, pctrl->proc,
+			&mcpy_umt_proc_fops, &pctrl->umt); */
+
+	entry = proc_create_data("dbg", 0, pctrl->proc,
+			&mcpy_dbg_proc_fops, &pctrl);
+	if (!entry)
+		goto err3;
+
+	return 0;
+
+err3:
+	remove_proc_entry("port_info", pctrl->proc);
+err2:
+	remove_proc_entry("mcpy_ctrl", pctrl->proc);
+err1:
+	remove_proc_entry(proc_name, NULL);
+	return -ENOMEM;
+}
+
+struct mcpy_umt *mcpy_get_umt(void)
+{
+	return &ltq_mcpy_ctrl.umt;
+}
+
+static int mcpy_xrx500_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	struct mcpy_ctrl *pctrl;
+	void __iomem *mcpy_addr_base;
+	struct device_node *np = pdev->dev.of_node;
+
+	pctrl = &ltq_mcpy_ctrl;
+	memset(pctrl, 0, sizeof(ltq_mcpy_ctrl));
+
+	/* load the memory ranges */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		panic("Failed to get HWMCPY resources\n");
+
+	pctrl->phybase = res->start;
+	mcpy_addr_base = devm_ioremap_resource(&pdev->dev, res);
+
+	if (IS_ERR(mcpy_addr_base))
+		panic("Failed to remap HWMCPY resources\n");
+
+	pctrl->membase = mcpy_addr_base;
+	g_mcpy_addr_base = mcpy_addr_base;
+	if (of_property_read_u32(np, "lantiq,mcpy-minlen",
+					&g_mcpy_min_len) < 0) {
+		g_mcpy_min_len = MCPY_MIN_LEN;
+	}
+
+	pctrl->dev = &pdev->dev;
+
+	mcpy_init(pdev, pctrl);
+	/* Link platform with driver data for retrieving */
+	platform_set_drvdata(pdev, pctrl);
+	mcpy_proc_init(pctrl);
+	#ifdef CONFIG_LTQ_UMT_518_FW_SG
+	setup_tc_percpu((void *)pctrl, 0);
+	#else
+	umt_init(pctrl);
+	#endif
+
+	mcpy_dbg(MCPY_INFO, "HW MCPY driver: Version: %s, Init Done !!",
+		MCPY_DRV_VERSION);
+
+	mcpy_profile_cycle_init();
+
+	return 0;
+
+}
+
+static int  mcpy_xrx500_release(struct platform_device *pdev)
+{
+	return 0;
+}
+static const struct of_device_id mcpy_xrx500_match[] = {
+	{ .compatible = "lantiq,mcpy-xrx500" },
+	{},
+};
+
+static struct platform_driver mcpy_xrx500_driver = {
+	.probe = mcpy_xrx500_probe,
+	.remove = mcpy_xrx500_release,
+	.driver = {
+		.name = "mcpy-xrx500",
+		.owner = THIS_MODULE,
+		.of_match_table = mcpy_xrx500_match,
+	},
+};
+
+int __init mcpy_xrx500_init(void)
+{
+	return platform_driver_register(&mcpy_xrx500_driver);
+}
+
+device_initcall(mcpy_xrx500_init);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Yixin.Zhu@lantiq.com");
+MODULE_DESCRIPTION("LTQ Hardware Memcpy Driver");
+MODULE_SUPPORTED_DEVICE("LTQ CPE Devices GRX35X, GRX5XX");
diff --git a/drivers/dma/ltq_hwmcpy.h b/drivers/dma/ltq_hwmcpy.h
new file mode 100755
--- /dev/null
+++ b/drivers/dma/ltq_hwmcpy.h
@@ -0,0 +1,234 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ */
+
+#ifndef __HWMCPY_H__
+#define __HWMCPY_H__
+
+#define MCPY_PORTS_NUM		8
+#define UMT_PORTS_NUM		4
+#define MCPY_CMD_DEPTH		16
+#define MCPY_OWN_BIT		BIT(31)
+#define MIN_UMT_PRD		20
+
+enum mcpy_prio {
+	MCPY_PRIO_LOW = 0,	/* Can Sleep */
+	MCPY_PRIO_HIGH,		/* Cannot Sleep */
+};
+
+enum {
+	MCPY_CMD_GATHER = 0x1,
+	MCPY_CMD_LAST   = 0x2,
+	MCPY_CMD_IPC	= 0x4,
+	MCPY_CMD_SRC_FLUSH = 0x8,
+	MCPY_CMD_SRC_INV   = 0x10,
+	MCPY_CMD_DST_FLUSH = 0x20,
+	MCPY_CMD_DST_INV   = 0x40,
+};
+
+enum mcpy_trunk_size {
+	MCPY_TKSZ_512B	= 0,
+	MCPY_TKSZ_1KB,
+	MCPY_TKSZ_2KB,
+	MCPY_TKSZ_4KB,
+	MCPY_TKSZ_8KB,
+	MCPY_TKSZ_16KB,
+	MCPY_TKSZ_32KB,
+	MCPY_TKSZ_64KB,
+	MCPY_TKSZ_MAX
+};
+
+enum mcpy_irq_md {
+	MCPY_IRQ_MODE = 0,
+	MCPY_YLD_MODE = 1,
+};
+
+enum mcpy_cmd_stat {
+	MCPY_CMD_PROCESSING	= 1,
+	MCPY_CMD_RELEASED	= 0,
+};
+
+enum dma_chan_on_off {
+	DMA_CH_OFF = 0,	/*!< DMA channel is OFF */
+	DMA_CH_ON = 1,	/*!< DMA channel is ON */
+};
+
+#define MCPY_DEF_TRKSZ		MCPY_TKSZ_512B
+#define MCPY_DEF_IRQ_INTVAL	50
+#define UMT_DEF_DMACID		7
+
+struct mcpy_port;
+struct mcpy_ctrl;
+
+struct mcpy_cmd {
+	struct mcpy_port *port;   /* back pointer */
+	u32 context;
+	u32 trunk_size;
+	enum mcpy_cmd_stat status;
+	enum mcpy_type type;
+	u32 flags;
+	wait_queue_head_t cmdq;
+	/*struct semaphore sem; */
+	struct mutex mtx_lock;
+	struct semaphore sem_lock;
+};
+
+struct dma_ch {
+	u32 rch;
+	u32 tch;
+	u32 rch_dbase;
+	u32 tch_dbase;
+	u32 rch_dnum;
+	u32 tch_dnum;
+	enum dma_chan_on_off onoff;
+	char rch_name[32];
+	char tch_name[32];
+};
+
+enum mcpy_port_status {
+	MCPY_PORT_DISABLED = 1,
+	MCPY_PORT_ENABLED = 2,
+	MCPY_PORT_RESERVED = 4,
+};
+
+#ifdef CONFIG_LTQ_UMT_LEGACY_MODE
+struct mcpy_umt {
+	struct mcpy_ctrl *ctrl;  /* back pointer */
+	enum umt_mode umt_mode;
+	enum umt_msg_mode msg_mode;
+	enum umt_status status;
+	u32 umt_period;
+	u32 umt_dst;
+	struct dma_ch chan;
+	spinlock_t umt_lock;
+	struct device *dev;
+};
+#else
+
+struct umt_port {
+	struct mcpy_umt *pctrl;
+	u32 umt_pid;
+	u32 ep_id;
+	enum umt_mode umt_mode;
+	enum umt_msg_mode msg_mode;
+	enum umt_status status;
+	u32 umt_period;
+	u32 umt_dst;
+	u32 cbm_pid; /* CBM WLAN ID (0 - 3) */
+	u32 dma_cid; /* DMA Chan ID */
+	enum umt_status suspend;
+	spinlock_t umt_port_lock;
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+	u32 dq_idx;
+	u32 umt_ep_dst;
+	u32 umtid_map_cbmid;
+#endif
+};
+
+struct mcpy_umt {
+	struct mcpy_ctrl *ctrl;  /* back pointer */
+	struct umt_port ports[UMT_PORTS_NUM];
+	u32 dma_ctrlid;
+	enum umt_status status;
+	struct proc_dir_entry *proc;
+	spinlock_t umt_lock;
+};
+
+#endif
+
+struct mcpy_port {
+	struct mcpy_ctrl *ctrl;  /* back pointer */
+	struct mcpy_port_ctrl *pctrl; /* back pointer */
+	const char *name;
+	u32 prio;
+	int vpe_id;
+	u32 pid;
+	int irq_no;
+	int yld_no;
+	u32 yld_pin;
+	struct mcpy_cmd cmd;
+	struct dma_ch chan;
+	enum mcpy_port_status status;
+	spinlock_t port_lock;
+	enum mcpy_trunk_size trunk_size;
+	enum mcpy_irq_md irq_mode;
+	u32 irq_intvl;
+	/* atomic_t users; remove for performace inprovement*/
+	u64 mib_bytes;
+	u64 mib_use_times;
+};
+
+struct mcpy_port_ctrl {
+	struct mcpy_ctrl *ctrl; /*back pointer */
+	u32 prio_lo_map; /* available port bitmap for low priority */
+	int lo_idx;
+	u32 *hi_per_vpe; /*Dedicated mcpy per VPE */
+	u32 *lo_per_vpe; /*Dedicated mcpy per VPE */
+	struct mcpy_port ports[MCPY_PORTS_NUM];
+	spinlock_t mcpy_lock;
+};
+
+struct mcpy_ctrl {
+	void __iomem *membase;
+	u32 phybase;
+	u32 dma_ctrl_id;
+	u32 dma_port_id;
+	struct mcpy_port_ctrl port_ctrl;
+	struct mcpy_umt  umt;
+	struct device *dev;
+	struct proc_dir_entry *proc;
+};
+
+struct mcpy_cfg {
+	u32 prio;
+	int vpe_id;
+	u32 en;
+};
+
+/* Debug Level */
+#undef MCPY_DBG_DEF
+#define MCPY_DBG_DEF(name, value)  MCPY_##name = BIT(value),
+#define MCPY_DBG_LIST			\
+	MCPY_DBG_DEF(ERR, 0)		\
+	MCPY_DBG_DEF(EVENT, 1)		\
+	MCPY_DBG_DEF(INIT, 2)		\
+	MCPY_DBG_DEF(INFO, 3)		\
+	MCPY_DBG_DEF(DBG,  4)		\
+	MCPY_DBG_DEF(MAX, 10)		\
+
+enum {
+	MCPY_DBG_LIST
+};
+#undef MCPY_DBG_DEF
+#define MCPY_DBG_DEF(name, value) DBG_##name,
+enum {
+	MCPY_DBG_LIST
+};
+#undef MCPY_DBG_DEF
+
+extern u32 g_mcpy_dbg;
+#define mcpy_dbg(dbg_level, fmt, arg...) \
+	do { \
+		if (unlikely(g_mcpy_dbg & dbg_level)) { \
+			if (dbg_level & MCPY_ERR) { \
+				pr_err_ratelimited(fmt, ##arg); \
+			} else if ((dbg_level & MCPY_INFO) || \
+					(dbg_level & MCPY_INIT)) { \
+				pr_info_ratelimited(fmt, ##arg); \
+			} else { \
+				pr_debug_ratelimited(fmt, ##arg); \
+			} \
+		} \
+	} \
+	while (0)
+
+int umt_init(struct mcpy_ctrl *);
+struct mcpy_umt *mcpy_get_umt(void);
+void setup_percpu_yqmask(u32 mask, int cpu);
+
+#endif  /* __HWMCPY_H__ */
+
diff --git a/drivers/dma/ltq_hwmcpy_addr.h b/drivers/dma/ltq_hwmcpy_addr.h
new file mode 100755
--- /dev/null
+++ b/drivers/dma/ltq_hwmcpy_addr.h
@@ -0,0 +1,92 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ */
+
+#ifndef __HWMCPY_ADDR_H__
+#define __HWMCPY_ADDR_H__
+
+#define MCPY_GCTRL				0x200
+#define MCPY_INTERNAL_INT_EN			0x8C
+#define MCPY_INTERNAL_INT_MASK			0x84
+#define MCPY_INT_EN				0x88
+#define MCPY_INT_MASK				0x80
+#define MCPY_INT_STAT				0x90
+#define MCPY_PORT_TO_CNT_0			0x300
+#define MCPY_MRES_REG0_0			0x100
+#define MCPY_MRES_REG1_0			0x104
+#define MCPY_MIPS_CFG_0				0x240
+#define MCPY_UMT_SW_MODE			0x94
+#define MCPY_UMT_PERD				0xDC
+#define MCPY_UMT_MSG(x)				(0x220 + (x) * 4)
+#define MCPY_UMT_DEST				0x230
+#define MCPY_UMT_XBASE				0x400
+#define MCPY_UMT_XOFFSET			0x100
+#define MCPY_UMT_XMSG(x)			(0x0 + (x) * 4)
+#define MCPY_UMT_XPERIOD			0x20
+#define MCPY_UMT_XDEST				0x30
+#define MCPY_UMT_XSW_MODE			0x34
+#define MCPY_UMT_TRG_MUX			0xE0
+#define MCPY_UMT_CNT_CTRL			0xE4
+
+#define MCPY_UMT_X_ADDR(x, off) (MCPY_UMT_XBASE + \
+				((x) - 1) * MCPY_UMT_XOFFSET + (off))
+
+#define MRES_ERROR				BIT(30)
+#define MRES_DONE				BIT(31)
+#define MCPY_CMD_ERR				BIT(16)
+#define MCPY_LEN_ERR				BIT(17)
+
+extern void __iomem *g_mcpy_addr_base;
+
+#define ltq_mcpy_r32(x)		ltq_r32(g_mcpy_addr_base + (x))
+#define ltq_mcpy_w32(x, y)	ltq_w32((x), g_mcpy_addr_base + (y))
+#define ltq_mcpy_w32_mask(x, y, z)	\
+			ltq_w32_mask((x), (y), g_mcpy_addr_base + (z))
+
+#define PORT_TO_CNT(pid)	(MCPY_PORT_TO_CNT_0 + (pid) * 0x10)
+#define PORT_MRES(pid)		(MCPY_MRES_REG1_0 + (pid) * 0x10)
+#define MCPY_SET_RESPONSE(pid)	ltq_mcpy_w32(0, PORT_MRES(pid))
+#define MCPY_GET_RESPONSE(pid)	ltq_mcpy_r32(PORT_MRES(pid))
+#define MCPY_MIPS_CFG(x)	(MCPY_MIPS_CFG_0 + (x) * 0x10)
+
+#define MCPY_CMD(cid, pid)	((cid) * 0x4 + (pid) * 0x10)
+
+/* GCR Address */
+#define GCR_BASE		0xB2300000
+#define GCR_CUS_BASE            0xB23F0000
+#define CCA_IC_MREQ0            0x18
+#define GCR_CCA_IC_MREQ(id)     (GCR_CUS_BASE + CCA_IC_MREQ0 + (id) * 4)
+#define GCR_REG0_BASE		0x0090
+#define GCR_REG1_BASE		0x00A0
+#define GCR_REG2_BASE		0x00B0
+#define GCR_REG3_BASE		0x00C0
+#define GCR_REG4_BASE		0x0190
+#define GCR_REG5_BASE		0x01A0
+#define GCR_REG0_MASK		0x0098
+#define GCR_REG1_MASK		0x00A8
+#define GCR_REG2_MASK		0x00B8
+#define GCR_REG3_MASK		0x00C8
+#define GCR_REG4_MASK		0x0198
+#define GCR_REG5_MASK		0x01A8
+#define GCR_CTRL_REG		(GCR_BASE + 0x10)
+#define GCR_CFG_ENABLE		(GCR_BASE + 0x60)
+#define GCR_UMT_REG		(GCR_BASE + GCR_REG3_BASE)
+#define GCR_UMT_MASK		(GCR_BASE + GCR_REG3_MASK)
+
+#define MCPY_DMA_RX_CID		12
+#define MCPY_DMA_TX_CID		(MCPY_DMA_RX_CID + 1)
+#define UMT_DMA_RX_CID		28
+#define UMT_DMA_TX_CID		(UMT_DMA_RX_CID + 1)
+#define MCPY_DBASE		0x10600
+#define MCPY_DBASE_OFFSET	0x100
+#define UMT_DBASE		0x11800
+#define DMA_DESC_SIZE		16 /* 4 DWs */
+
+
+#define DMA3_MASTERID           29
+
+#endif  /*__HWMCPY_ADDR_H__ */
diff --git a/drivers/dma/ltq_umt_expand.c b/drivers/dma/ltq_umt_expand.c
new file mode 100644
--- /dev/null
+++ b/drivers/dma/ltq_umt_expand.c
@@ -0,0 +1,1375 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ * UMT Driver for GRX350 A21
+ */
+#define DEBUG
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <asm/ltq_vmb.h>
+
+#include <lantiq_dmax.h>
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <lantiq_irq.h>
+
+#include <net/datapath_proc_api.h>
+#include "ltq_hwmcpy_addr.h"
+#include <linux/ltq_hwmcpy.h>
+#include "ltq_hwmcpy.h"
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+#include "mips_tc_sg.h"
+#endif
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+#include <linux/kthread.h>
+#include <linux/delay.h>
+
+#include "net/lantiq_cbm_api.h"
+
+static u8 umt_tc_thread_stack[4096]__attribute__((aligned(16)));
+static u8 umt_tc_thread_gp[4096]__attribute__((aligned(4096)));
+static struct task_struct *ksw_umt_tsk;
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+static u8 mips_tc_thread_stack[4096]__attribute__((aligned(16)));
+static u8 mips_tc_thread_gp[4096]__attribute__((aligned(4096)));
+#endif
+#define GPTC_1A_GIC_IRQ 172
+#define GPTC_3A 4
+#define GPT_IRNCR 0xFC
+#define GPTC1_MODULE_BASE 0x16300000u
+#define GPTC1_BASE 		(GPTC1_MODULE_BASE | KSEG1)
+#define GPTC1_IRNCR     (GPTC1_BASE + GPT_IRNCR)
+static u32 jiffies1;
+static u32 jiffies2;
+#define UMT_GIC_BASE_ADDR		0xb2320000  // KSEG0 address of the GIC
+#define GIC_SH_RMASK31_0 0x0300
+#define GIC_SH_SMASK31_00 0x0380
+#define GIC_SH_MASK31_00 0x0400
+#define GIC_SH_PEND31_00 0x0480
+#define GIC_SH_WEDGE 0x0280
+
+#define GPTC_1A_PMASK(irq_no)		  (UMT_GIC_BASE_ADDR+GIC_SH_PEND31_00+((irq_no>>5)*0x4))
+#define GPTC_1A_MASK_BIT(irq_no)      (irq_no - ((irq_no>>5)*32))
+#define GIC_SH_WEDGE_REG    (GIC_SH_WEDGE | UMT_GIC_BASE_ADDR)
+
+#define LTQ_UMT_SW_INTERVAL_DEFAULT 40    // the default interval is 40ms
+static u32 gptc_pmask, gptc_maskbit, g_umt_interval = LTQ_UMT_SW_INTERVAL_DEFAULT; 
+#define CHECK_BIT(var,pos) ((var) & (1<<(pos)))
+
+struct thrd_param {
+	u32 dummy; //for reserve
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+	u32 mips_tc_shared_ctxt_mem;
+#endif
+};
+
+static struct thrd_param g_umt_thread_info;
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+static struct thrd_param g_mips_thread_info;
+#endif
+static volatile u32 g_tot_dq_cnt[UMT_PORTS_NUM];
+#endif
+
+static u32 g_dma_ctrl = DMA1TX;
+
+static inline void umt_set_mode(u32 umt_id, enum umt_mode umt_mode)
+{
+	u32 val, off;
+
+	if (!umt_id)
+		ltq_mcpy_w32_mask(0x2, ((u32)umt_mode) << 1, MCPY_GCTRL);
+	else {
+		off = 16 + (umt_id - 1) * 3;
+		val = ltq_mcpy_r32(MCPY_GCTRL) & ~(BIT(off));
+		ltq_mcpy_w32(val | (((u32)umt_mode) << off), MCPY_GCTRL);
+	}
+}
+
+static inline void umt_set_msgmode(u32 umt_id, enum umt_msg_mode msg_mode)
+{
+	if (!umt_id)
+		ltq_mcpy_w32((u32)msg_mode, MCPY_UMT_SW_MODE);
+	else
+		ltq_mcpy_w32((u32)msg_mode,
+			MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XSW_MODE));
+}
+
+/* input in term of microseconds */
+static inline u32 umt_us_to_cnt(int usec)
+{
+	struct clk *ngi_clk = clk_get_xbar();
+
+	return usec * (clk_get_rate(ngi_clk) / 1000000);
+}
+
+static inline void umt_set_period(u32 umt_id, u32 umt_period)
+{
+	umt_period = umt_us_to_cnt(umt_period);
+
+	if (!umt_id)
+		ltq_mcpy_w32(umt_period, MCPY_UMT_PERD);
+	else
+		ltq_mcpy_w32(umt_period,
+			MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XPERIOD));
+}
+
+static inline void umt_set_dst(u32 umt_id, u32 umt_dst)
+{
+	if (!umt_id)
+		ltq_mcpy_w32(umt_dst, MCPY_UMT_DEST);
+	else
+		ltq_mcpy_w32(umt_dst,
+			MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XDEST));
+}
+
+static inline void umt_set_mux(u32 umt_id, u32 cbm_pid, u32 dma_cid)
+{
+	u32 mux_sel;
+
+	cbm_pid = cbm_pid & 0xF;
+	dma_cid = dma_cid & 0xF;
+	mux_sel = ltq_mcpy_r32(MCPY_UMT_TRG_MUX) &
+			(~((0xF000F) << (umt_id * 4)));
+	mux_sel |= (dma_cid << (umt_id * 4)) |
+			(cbm_pid << (16 + (umt_id * 4)));
+	ltq_mcpy_w32(mux_sel, MCPY_UMT_TRG_MUX);
+}
+
+static inline void umt_set_endian(int dw_swp, int byte_swp)
+{
+	u32 val;
+
+	val = ltq_mcpy_r32(MCPY_GCTRL);
+	if (byte_swp)
+		val |= BIT(28);
+	else
+		val &= ~(BIT(28));
+
+	if (dw_swp)
+		val |= BIT(29);
+	else
+		val &= ~(BIT(29));
+
+	ltq_mcpy_w32(val, MCPY_GCTRL);
+}
+
+static inline void umt_en_expand_mode(void)
+{
+	u32 val;
+
+	val = ltq_mcpy_r32(MCPY_GCTRL) | BIT(31);
+	ltq_mcpy_w32(val, MCPY_GCTRL);
+
+	if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+		umt_set_endian(1, 0);
+	else
+		umt_set_endian(1, 1);
+}
+
+static inline void umt_enable(u32 umt_id, enum umt_status status)
+{
+	u32 val, off;
+
+	if (!umt_id)
+		ltq_mcpy_w32_mask(0x4, ((u32)status) << 2, MCPY_GCTRL);
+	else {
+		off = 17 + (umt_id - 1) * 3;
+		val = (ltq_mcpy_r32(MCPY_GCTRL) & ~BIT(off))
+				| (((u32)status) << off);
+		ltq_mcpy_w32(val, MCPY_GCTRL);
+	}
+}
+
+static inline void umt_suspend(u32 umt_id, enum umt_status status)
+{
+	u32 val;
+
+	if (status)
+		val = ltq_mcpy_r32(MCPY_UMT_CNT_CTRL) | BIT(umt_id);
+	else
+		val = ltq_mcpy_r32(MCPY_UMT_CNT_CTRL) & (~(BIT(umt_id)));
+
+	ltq_mcpy_w32(val, MCPY_UMT_CNT_CTRL);
+}
+
+/*This function will disable umt */
+static inline void umt_reset_umt(u32 umt_id)
+{
+	u32 mode;
+	umt_enable(umt_id, UMT_DISABLE);
+
+	mode = ltq_mcpy_r32(MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XSW_MODE));
+
+	if (mode == UMT_SELFCNT_MODE) {
+		umt_set_mode(umt_id, UMT_USER_MODE);
+		umt_set_mode(umt_id, UMT_SELFCNT_MODE);
+	} else {
+		umt_set_mode(umt_id, UMT_SELFCNT_MODE);
+		umt_set_mode(umt_id, UMT_USER_MODE);
+	}
+
+	return;
+}
+
+/**
+ * intput:
+ * @umt_id: UMT port id, (0 - 3)
+ * @ep_id:  Aligned with datapath lib ep_id
+ * @period: measured in microseconds.
+ * ret:  Fail < 0 / Success: 0
+ */
+int ltq_umt_set_period(u32 umt_id, u32 ep_id, u32 period)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (period < MIN_UMT_PRD || umt_id >= UMT_PORTS_NUM)
+		goto period_err;
+
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT is not initialized!\n");
+		return -ENODEV;
+	}
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id) {
+		spin_unlock_bh(&port->umt_port_lock);
+		goto period_err;
+	}
+
+	if (port->umt_period != period) {
+		port->umt_period = period;
+		umt_set_period(umt_id, port->umt_period);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+
+period_err:
+	mcpy_dbg(MCPY_ERR, "umt_id: %d, ep_id: %d, period: %d\n",
+		umt_id, ep_id, period);
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_set_period);
+
+/**
+ * API to configure the UMT port.
+ * input:
+ * @umt_id: (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ * @umt_mode:  0-self-counting mode, 1-user mode.
+ * @msg_mode:  0-No MSG, 1-MSG0 Only, 2-MSG1 Only, 3-MSG0 & MSG1.
+ * @dst:  Destination PHY address.
+ * @period(ms): only applicable when set to self-counting mode.
+ *              self-counting interval time. if 0, use the original setting.
+ * @enable: 1-Enable/0-Disable
+ * @ret:  Fail < 0 , SUCCESS:0
+ */
+int ltq_umt_set_mode(u32 umt_id, u32 ep_id, u32 umt_mode, u32 msg_mode,
+			u32 phy_dst, u32 period, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT is not initialized!!\n");
+		return -ENODEV;
+	}
+	if ((umt_mode >= (u32)UMT_MODE_MAX)
+			|| (msg_mode >= (u32)UMT_MSG_MAX)
+			|| (enable >= (u32)UMT_STATUS_MAX)
+			|| (phy_dst == 0)
+			|| (period == 0)
+			|| (umt_id >= UMT_PORTS_NUM)) {
+		mcpy_dbg(MCPY_ERR, "umt_id: %d, umt_mode: %d, msg_mode: %d, enable: %d, phy_dst: %d\n",
+			umt_id, umt_mode, msg_mode, enable, phy_dst);
+		return -EINVAL;
+	}
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, port ep_id: %d\n",
+			ep_id, port->ep_id);
+		spin_unlock_bh(&port->umt_port_lock);
+		return -EINVAL;
+	}
+
+	umt_reset_umt(umt_id);
+
+	port->umt_mode = (enum umt_mode)umt_mode;
+	port->msg_mode = (enum umt_msg_mode)msg_mode;
+	port->umt_dst	= phy_dst;
+	port->umt_period = period;
+	port->status = (enum umt_status)enable;
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+	if (IS_ENABLED(CONFIG_LTQ_UMT_SW_MODE)) {
+		port->umt_ep_dst = phy_dst | KSEG3;
+	}
+#endif
+	umt_set_mode(umt_id, port->umt_mode);
+	umt_set_msgmode(umt_id, port->msg_mode);
+	umt_set_dst(umt_id, port->umt_dst);
+	umt_set_period(umt_id, port->umt_period);
+	umt_enable(umt_id, port->status);
+	/* setup the CBM/DMA mapping */
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_set_mode);
+
+/**
+ * API to enable/disable umt port
+ * input:
+ * @umt_id (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ * @enable: Enable: 1 / Disable: 0
+ * ret:  Fail < 0, Success: 0
+ */
+int ltq_umt_enable(u32 umt_id, u32 ep_id, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (umt_id >= UMT_PORTS_NUM)
+		return -EINVAL;
+	if (enable >= (u32)UMT_STATUS_MAX
+			|| pumt->status != UMT_ENABLE)
+		return -ENODEV;
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id || port->umt_dst == 0 || port->ep_id == 0) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, umt port ep_id: %d, umt_dst: 0x%x\n",
+			ep_id, port->ep_id, port->umt_dst);
+		goto en_err;
+	}
+
+	if (port->status != enable) {
+		port->status = (enum umt_status)enable;
+		umt_enable(umt_id, port->status);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+
+en_err:
+	spin_unlock_bh(&port->umt_port_lock);
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_enable);
+
+/**
+ * API to suspend/resume umt US/DS counter
+ * input:
+ * @umt_id (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ * @enable: suspend: 1 / resume: 0
+ * ret:  Fail < 0, Success: 0
+ */
+int ltq_umt_suspend(u32 umt_id, u32 ep_id, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (umt_id >= UMT_PORTS_NUM)
+		return -EINVAL;
+	if (enable >= (u32)UMT_STATUS_MAX
+			|| pumt->status != UMT_ENABLE)
+		return -ENODEV;
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id || port->umt_dst == 0 || port->ep_id == 0) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, umt port ep_id: %d, umt_dst: 0x%x\n",
+			ep_id, port->ep_id, port->umt_dst);
+		goto en_err;
+	}
+
+	if (port->suspend != enable) {
+		port->suspend = (enum umt_status)enable;
+		umt_enable(umt_id, port->status);
+		umt_suspend(umt_id, port->suspend);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+
+en_err:
+	spin_unlock_bh(&port->umt_port_lock);
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_suspend);
+
+/**
+ * API to request and allocate UMT port
+ * input:
+ * @ep_id: aligned with datapath lib EP.
+ * @cbm_pid: CBM Port ID(0-3), 0 - CBM port 4, 1 - CBM port 24,
+ * 2 - CBM port 25, 3 - CBM port 26
+ * output:
+ * @dma_ctrlid: DMA controller ID. aligned with DMA driver DMA controller ID
+ * @dma_cid: DMA channel ID.
+ * @umt_id: (0 - 3)
+ * ret: Fail: < 0,  Success: 0
+ */
+int ltq_umt_request(u32 ep_id, u32 cbm_pid,
+		u32 *dma_ctrlid, u32 *dma_cid, u32 *umt_id)
+{
+	int i, pid;
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (!dma_ctrlid || !dma_cid || !umt_id) {
+		mcpy_dbg(MCPY_ERR, "Output pointer is NULL!\n");
+		goto param_err;
+	}
+
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT not initialized!\n");
+		goto param_err;
+	}
+	if (!ep_id) {
+		mcpy_dbg(MCPY_ERR, "%s: ep_id cannot be zero!\n", __func__);
+		goto param_err;
+	}
+
+	if (cbm_pid >= UMT_PORTS_NUM) {
+		mcpy_dbg(MCPY_ERR, "%s: cbm pid must be in ranage(0 - %d)\n",
+			__func__, UMT_PORTS_NUM);
+		goto param_err;
+	}
+
+	pid = -1;
+	spin_lock_bh(&pumt->umt_lock);
+	for (i = 0; i < UMT_PORTS_NUM; i++) {
+		port = &pumt->ports[i];
+		spin_lock_bh(&port->umt_port_lock);
+		if (port->ep_id == ep_id && port->cbm_pid == cbm_pid) {
+			pid = i;
+			spin_unlock_bh(&port->umt_port_lock);
+			break;
+		} else if (port->ep_id == 0 && pid == -1)
+			pid = i;
+		spin_unlock_bh(&port->umt_port_lock);
+	}
+	spin_unlock_bh(&pumt->umt_lock);
+
+	if (pid < 0) {
+		mcpy_dbg(MCPY_ERR, "No free UMT port!\n");
+		return -ENODEV;
+	}
+
+	port = &pumt->ports[pid];
+	spin_lock_bh(&port->umt_port_lock);
+	port->ep_id = ep_id;
+	port->cbm_pid = cbm_pid;
+	umt_set_mux(port->umt_pid, port->cbm_pid, port->dma_cid);
+	*dma_ctrlid = pumt->dma_ctrlid;
+	*dma_cid = port->dma_cid;
+	*umt_id = port->umt_pid;
+	spin_unlock_bh(&port->umt_port_lock);
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+       {
+               uint32_t flag = 0, cbm_pid_l = 0;
+               int ret = 0;
+               cbm_dq_port_res_t dqport;
+
+               ret = cbm_get_wlan_umt_pid( ep_id,  &cbm_pid_l);
+               if (ret != 0) {
+                        mcpy_dbg(MCPY_ERR, "Failed to get cbm port using ep id !\n");
+                        goto param_err;
+                }
+
+               memset(&dqport, 0, sizeof (dqport));
+               ret = cbm_dequeue_port_resources_get(ep_id, &dqport, flag);
+
+               if (ret != 0) {
+                        mcpy_dbg(MCPY_ERR, "Failed to get dp port using ep id !\n");
+                        goto param_err;
+                }
+                
+                if (dqport.deq_info->port_no == 4 ||
+                        dqport.deq_info->port_no == 24 ||
+                        dqport.deq_info->port_no == 25 ||
+                        dqport.deq_info->port_no == 26)
+                {
+                        spin_lock_bh(&port->umt_port_lock);
+                        if (cbm_pid == cbm_pid_l)
+                        {
+                                port->umtid_map_cbmid = dqport.deq_info->port_no;
+                        } else {
+                                port->umtid_map_cbmid = 0;
+                        }
+                        spin_unlock_bh(&port->umt_port_lock);
+                }
+                else
+                {
+                     if (dqport.deq_info) 
+			kfree(dqport.deq_info);
+                     mcpy_dbg(MCPY_ERR, "port no %d not valid !\n", dqport.deq_info->port_no);
+                     goto param_err;
+                }
+
+               if (dqport.deq_info) 
+			kfree(dqport.deq_info);
+       }
+#endif
+
+	return 0;
+
+param_err:
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_request);
+
+/**
+ * API to release umt port
+ * input:
+ * @umt_id (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ *
+ * ret:  Fail < 0, Success: 0
+ */
+int ltq_umt_release(u32 umt_id, u32 ep_id)
+{
+	struct mcpy_umt *pumt;
+	struct umt_port *port;
+
+	if (umt_id >= UMT_PORTS_NUM)
+		return -ENODEV;
+
+	pumt = mcpy_get_umt();
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT is not initialized!\n");
+		return -ENODEV;
+	}
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, UMT port ep_id: %d\n",
+			ep_id, port->ep_id);
+		spin_unlock_bh(&port->umt_port_lock);
+
+		return -ENODEV;
+	} else {
+		port->ep_id = 0;
+		port->cbm_pid = 0;
+		port->umt_dst = 0;
+		port->umt_period = 0;
+		port->status = UMT_DISABLE;
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+		port->umt_ep_dst = 0;
+		port->umtid_map_cbmid = 0;
+#endif
+		umt_enable(port->umt_pid, UMT_DISABLE);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_release);
+
+
+
+static void umt_port_init(struct mcpy_umt *pumt,
+		struct device_node *node, int pid)
+{
+	char res_cid[32];
+	int cid;
+	struct umt_port *port;
+
+	port = &pumt->ports[pid];
+	sprintf(res_cid, "lantiq,umt%d-dmacid", pid);
+	if (of_property_read_u32(node, res_cid, &cid) < 0)
+		cid = UMT_DEF_DMACID + pid;
+
+	port->pctrl = pumt;
+	port->umt_pid = pid;
+	port->dma_cid = cid;
+	port->ep_id = 0;
+	port->status = UMT_DISABLE;
+	spin_lock_init(&port->umt_port_lock);
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+	port->dq_idx = 0;
+	port->umt_ep_dst = 0;
+	port->umtid_map_cbmid = 0;
+#endif
+}
+
+static void *umt_port_seq_start(struct seq_file *s, loff_t *pos)
+{
+	struct mcpy_umt *pumt = s->private;
+	struct umt_port *port;
+
+	if (*pos >= UMT_PORTS_NUM)
+		return NULL;
+
+	port = &pumt->ports[*pos];
+
+	return port;
+}
+
+static void *umt_port_seq_next(struct seq_file *s, void *v, loff_t *pos)
+{
+	struct mcpy_umt *pumt = s->private;
+	struct umt_port *port;
+
+	if (++*pos >= UMT_PORTS_NUM)
+		return NULL;
+	port = &pumt->ports[*pos];
+	return port;
+}
+
+static void umt_port_seq_stop(struct seq_file *s, void *v)
+{
+
+}
+
+static int umt_port_seq_show(struct seq_file *s, void *v)
+{
+	struct umt_port *port = v;
+	int pid = port->umt_pid;
+	u32 val;
+
+	seq_printf(s, "\nUMT port %d configuration\n", pid);
+	seq_puts(s, "-----------------------------------------\n");
+	seq_printf(s, "UMT port ep_id: %d\n", port->ep_id);
+	seq_printf(s, "UMT Mode: \t%s\n",
+		port->umt_mode == UMT_SELFCNT_MODE ?
+		"UMT SelfCounting Mode" : "UMT User Mode");
+	switch (port->msg_mode) {
+	case UMT_NO_MSG:
+		seq_puts(s, "UMT MSG Mode: \tUMT NO MSG\n");
+		break;
+	case UMT_MSG0_ONLY:
+		seq_puts(s, "UMT MSG Mode: \tUMT MSG0 Only\n");
+		break;
+	case UMT_MSG1_ONLY:
+		seq_puts(s, "UMT MSG Mode: \tUMT MSG1 Only\n");
+		break;
+	case UMT_MSG0_MSG1:
+		seq_puts(s, "UMT MSG Mode: \tUMT_MSG0_And_MSG1\n");
+		break;
+	default:
+		seq_printf(s, "UMT MSG Mode Error! Msg_mode: %d\n",
+			port->msg_mode);
+	}
+	seq_printf(s, "UMT DST: \t0x%x\n", port->umt_dst);
+	if (port->umt_mode == UMT_SELFCNT_MODE)
+		seq_printf(s, "UMT Period: \t%d(us)\n", port->umt_period);
+	seq_printf(s, "UMT Status: \t%s\n",
+			port->status == UMT_ENABLE ? "Enable" :
+			port->status == UMT_DISABLE ? "Disable" : "Init Fail");
+	seq_printf(s, "UMT DMA CID: \t%d\n", port->dma_cid);
+	seq_printf(s, "UMT CBM PID: \t%d\n", port->cbm_pid);
+
+	seq_printf(s, "++++Register dump of umt port: %d++++\n", pid);
+	if (pid == 0) {
+		seq_printf(s, "UMT Status: \t%s\n",
+			(ltq_mcpy_r32(MCPY_GCTRL) & BIT(2)) != 0 ?
+			"Enable" : "Disable");
+		seq_printf(s, "UMT Mode: \t%s\n",
+			(ltq_mcpy_r32(MCPY_GCTRL) & BIT(1)) != 0 ?
+			"UMT User MSG mode" : "UMT SelfCounting mode");
+		seq_printf(s, "UMT MSG Mode: \t%d\n",
+			ltq_mcpy_r32(MCPY_UMT_SW_MODE));
+		seq_printf(s, "UMT Dst: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_DEST));
+		seq_printf(s, "UMT Period: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_PERD));
+		seq_printf(s, "UMT MSG0: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_MSG(0)));
+		seq_printf(s, "UMT MSG1: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_MSG(1)));
+	} else {
+		seq_printf(s, "UMT Status: \t%s\n",
+			(ltq_mcpy_r32(MCPY_GCTRL) &
+				BIT(17 + 3 * (pid - 1))) != 0 ?
+			"Enable" : "Disable");
+		seq_printf(s, "UMT Mode: \t%s\n",
+			(ltq_mcpy_r32(MCPY_GCTRL) &
+				BIT(16 + 3 * (pid - 1))) != 0 ?
+			"UMT User MSG mode" : "UMT SelfCounting mode");
+		seq_printf(s, "UMT MSG Mode: \t%d\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XSW_MODE)));
+		seq_printf(s, "UMT Dst: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XDEST)));
+		seq_printf(s, "UMT Period: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XPERIOD)));
+		seq_printf(s, "UMT MSG0: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XMSG(0))));
+		seq_printf(s, "UMT MSG1: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XMSG(1))));
+	}
+
+	val = ltq_mcpy_r32(MCPY_UMT_TRG_MUX);
+	seq_printf(s, "DMA CID: \t%d\n",
+		(val & ((0xF) << (pid * 4))) >> (pid * 4));
+	seq_printf(s, "CBM PID: \t%d\n",
+		(val & ((0xF) << (16 + pid * 4))) >> (16 + pid * 4));
+
+	return 0;
+}
+
+
+static const struct seq_operations umt_port_seq_ops = {
+	.start = umt_port_seq_start,
+	.next = umt_port_seq_next,
+	.stop = umt_port_seq_stop,
+	.show = umt_port_seq_show,
+};
+
+static int umt_cfg_read_proc_open(struct inode *inode, struct file *file)
+{
+	int ret = seq_open(file, &umt_port_seq_ops);
+
+	if (ret == 0) {
+		struct seq_file *m = file->private_data;
+		m->private = PDE_DATA(inode);
+	}
+	return ret;
+}
+
+static const struct file_operations mcpy_umt_proc_fops = {
+	.open           = umt_cfg_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = seq_release,
+};
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+#define MICROSEC_TO_SEC(x) (1000000/x)
+
+static int umt_tc_info_read_proc(struct seq_file *s, void *v)
+{
+	u32 days, sec, min, hr;
+	struct umt_port *port = NULL;
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	int i = 0;
+
+	sec = jiffies1 / MICROSEC_TO_SEC(50);
+
+	if (sec >= 60) {
+		min = sec / 60;
+		sec = sec - (min * 60);
+	} else
+		min = 0;
+
+	if (min >= 60) {
+		hr = min / 60;
+		min = min - (hr * 60);
+	} else {
+		hr = 0;
+	}
+
+	if (hr >= 24) {
+		days = hr / 24;
+		hr = hr - (days * 24);
+	} else {
+		days = 0;
+	}
+
+	seq_printf(s, "Jiffies   : %08d\n", jiffies1);
+	seq_printf(s, "Jiffies_2 : %08d\n", jiffies2);
+	seq_printf(s, "Uptime(d:h:m:s): %02d:%02d:%02d:%02d\n", days, hr, min, sec);
+
+	for (i = 0 ; i < UMT_PORTS_NUM ; i ++) {
+		port = &pumt->ports[i];
+		seq_printf(s, "Packets to be dequeued: %d  cbm id %d \n", g_tot_dq_cnt[i], port->umtid_map_cbmid);
+		seq_printf(s, "thread info: umt dst: 0x%x, interval: 0x%x, en: %d, dq_idx: %u, ep_dst:0x%x\n",
+			port->umt_dst,g_umt_interval, port->status, port->dq_idx, port->umt_ep_dst);
+	}
+	seq_printf(s, "GPTC1_IRNCR : %08x\n", REG32(GPTC1_IRNCR));
+	seq_printf(s, "GIC_GPTC_MASK : %08x\n", REG32(gptc_pmask));
+	return 0;
+}
+
+static int umt_tc_info_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, umt_tc_info_read_proc, PDE_DATA(inode));
+}
+
+static const struct file_operations umt_tc_info_proc_fops = {
+	.owner      = THIS_MODULE,
+	.open = umt_tc_info_proc_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+/* UMT interval related proc */
+static int umt_interval_read_proc(struct seq_file *s, void *v)
+{
+	seq_printf(s, "UMT interval: %d\n", g_umt_interval);
+	return 0;
+}
+
+static ssize_t proc_write_umt_interval(struct file *file, const char __user *buf, size_t count, loff_t *data)
+{
+    int len, tmp=0;
+    char str[64];
+    char *p;
+	struct clk *gptc_clk;
+	u32 rate;
+
+    len = min(count, (size_t)(sizeof(str) - 1));
+    len -= copy_from_user(str, buf, len);
+    while ( len && str[len - 1] <= ' ' )
+        len--;
+    str[len] = 0;
+    for ( p = str; *p && *p <= ' '; p++, len-- );
+    if ( !*p )
+        return count;
+
+    strict_strtol(p, 10, (long*)&tmp);
+	g_umt_interval = tmp;
+
+	/* restart the GPTU with new values */
+	gptc_clk = clk_get_sys("16300000.gptu", "timer3a");
+	if (IS_ERR(gptc_clk)) {
+		pr_err("failed to get clock timer3a\r\n");
+		goto out;
+	}
+	clk_disable(gptc_clk);
+
+	rate = (1000 * 1000)/g_umt_interval;
+	pr_info("setting the umt interval to: %d\n", (unsigned int)g_umt_interval);
+	if (g_umt_interval != LTQ_UMT_SW_INTERVAL_DEFAULT)
+		pr_info("warning... recommend value is : %d\n", LTQ_UMT_SW_INTERVAL_DEFAULT);
+	pr_info("setting the gptu rate to: %d\n", (unsigned int)rate);
+	clk_set_rate(gptc_clk, rate);
+
+	if (clk_enable(gptc_clk)) {
+		pr_err("%s enable failed\r\n", __func__);
+		clk_set_rate(gptc_clk, 0);
+	} else {
+		pr_info("%s Timer 3A is restarted for UMT !\n", __func__);
+	}
+out:
+	return len;
+}
+
+static int umt_interval_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, umt_interval_read_proc, PDE_DATA(inode));
+}
+
+static const struct file_operations umt_interval_proc_fops = {
+    	.owner      = THIS_MODULE,
+	.open = umt_interval_proc_open,
+	.read = seq_read,
+   	.write      = proc_write_umt_interval,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+#endif
+
+static int umt_proc_init(struct mcpy_umt *pumt)
+{
+	struct proc_dir_entry *entry;
+
+	pumt->proc = proc_mkdir("umt", pumt->ctrl->proc);
+	if (!pumt->proc)
+		return -ENOMEM;
+
+	entry = proc_create_data("umt_info", 0, pumt->proc,
+			&mcpy_umt_proc_fops, pumt);
+	if (!entry)
+		goto err1;
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+	entry = proc_create_data("umt_tc_info", 0, pumt->proc,
+			&umt_tc_info_proc_fops, pumt);
+	if (!entry)
+		goto err2;
+	entry = proc_create_data("umt_interval", 0, pumt->proc,
+			&umt_interval_proc_fops, pumt);
+	if (!entry)
+		goto err3;
+#endif
+
+	return 0;
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+err3:
+	remove_proc_entry("umt_tc_info", pumt->ctrl->proc);
+err2:
+	remove_proc_entry("umt_info", pumt->ctrl->proc);
+#endif
+err1:
+	remove_proc_entry("umt", pumt->ctrl->proc);
+	mcpy_dbg(MCPY_ERR, "UMT proc create fail!\n");
+	return -1;
+}
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+typedef unsigned long               reg32_t;
+
+#define _m32c0_mfc0(reg, sel) \
+__extension__ ({ \
+  register unsigned long __r; \
+  __asm__ __volatile ("mfc0 %0,$%1,%2" \
+		      : "=d" (__r) \
+      		      : "JK" (reg), "JK" (sel)); \
+  __r; \
+})
+
+#define _m32c0_mtc0(reg, sel, val) \
+do { \
+    __asm__ __volatile ("%(mtc0 %z0,$%1,%2; ehb%)" \
+			: \
+			: "dJ" ((reg32_t)(val)), "JK" (reg), "JK" (sel) \
+			: "memory"); \
+} while (0)
+
+#define _m32c0_mftc0(rt,sel)                                                   \
+({                                                                      \
+         unsigned long  __res;                                          \
+                                                                        \
+        __asm__ __volatile__(                                           \
+        "       .set    push                                    \n"     \
+        "       .set    mips32r2                                \n"     \
+        "       .set    noat                                    \n"     \
+        "       # mftc0 $1, $" #rt ", " #sel "                  \n"     \
+        "       .word   0x41000800 | (" #rt " << 16) | " #sel " \n"     \
+        "       move    %0, $1                                  \n"     \
+        "       .set    pop                                     \n"     \
+        : "=r" (__res));                                                \
+                                                                        \
+        __res;                                                          \
+})
+
+#define _m32c0_mttc0(rd, sel, v)                                                       \
+({                                                                      \
+        __asm__ __volatile__(                                           \
+        "       .set    push                                    \n"     \
+        "       .set    mips32r2                                \n"     \
+        "       .set    noat                                    \n"     \
+        "       move    $1, %0                                  \n"     \
+        "       # mttc0 %0," #rd ", " #sel "                    \n"     \
+        "       .word   0x41810000 | (" #rd " << 11) | " #sel " \n"     \
+        "       .set    pop                                     \n"     \
+        :                                                               \
+        : "r" (v));                                                     \
+})
+
+/* move to gpr */
+#define _m32c0_mttgpr(rd,v) \
+do {									\
+	__asm__ __volatile__(						\
+	"	.set	push					\n"	\
+	"	.set	mips32r2				\n"	\
+	"	.set	noat					\n"	\
+	"	move	$1, %0					\n"	\
+	"	# mttgpr $1, " #rd "				\n"	\
+	"	.word	0x41810020 | (" #rd " << 11)		\n"	\
+	"	.set	pop					\n"	\
+	: : "r" (v));							\
+} while (0)
+
+#define mips32_setmvpcontrol(x)	    _m32c0_mtc0(0,1,x)
+#define mips32_getmvpcontrol()	     _m32c0_mfc0(0,1)
+#define mips32_mt_settcstatus(val)	mttc0(2, 1, val)
+#define mips32_getvpecontrol()	        _m32c0_mfc0(1,1)
+#define mips32_setvpecontrol(x)	        _m32c0_mtc0(1,1,x)
+
+#define VPECONTROL_TARGTC1	0x000000ff	
+#define  VPECONTROL_TARGTC_SHIFT	 0	
+
+/*
+ * targeted VPE register macros
+ */
+#define mips32_mt_settarget(tc) \
+	mips32_setvpecontrol ( \
+		(mips32_getvpecontrol () & ~(VPECONTROL_TARGTC)) \
+		| ((tc) << VPECONTROL_TARGTC_SHIFT))
+
+#define mips32_mt_settchalt(val)	_m32c0_mttc0(2, 4, val)
+#define mips32_mt_gettchalt()	_m32c0_mftc0(2, 4)
+#define mips32_mt_settcbind(val)	_m32c0_mttc0(2, 2, val)
+#define mips32_mt_gettcbind()		_m32c0_mftc0(2, 2)
+#define mips32_mt_setsp(val)		_m32c0_mttgpr(29, val)
+#define mips32_mt_settcrestart(val)	_m32c0_mttc0(2, 3, val)
+#define mips32_mt_gettcrestart()	_m32c0_mftc0(2, 3)
+#define mips32_mt_gettcstatus()     _m32c0_mftc0(2, 1)
+#define mips32_mt_getvpecontrol()   _m32c0_mftc0(1, 1)
+#define mips32_mt_setvpecontrol(val)    _m32c0_mttc0(1, 1, val)
+
+void run_tc(u32 tc_num,
+	    u32 vpe_num,
+	    u32 start_addr,
+	    u32 stack_top,
+	    u32 arg0,
+	    u32 arg1)
+{
+    mips32_setmvpcontrol((mips32_getmvpcontrol() & ~MVPCONTROL_EVP) | MVPCONTROL_VPC); // VPC=1 EVP=0.
+	asm("ehb");
+	pr_info("getmvpctrl = %x\n", (unsigned int)mips32_getmvpcontrol());
+
+    // set the target to tc_num
+    mips32_mt_settarget(tc_num); // Have all 'mips32_mt_*' functions target tc1.
+	asm("ehb");
+	pr_info("getvpectrl for target TC = %x\n", (unsigned int)mips32_getvpecontrol());
+
+    // set DA bit so yields will work.
+    mips32_mt_settcstatus(mips32_mt_gettcstatus() | TCSTATUS_DA);
+	asm("ehb");
+	pr_info("gettcstatus = %x\n", (unsigned int)mips32_mt_gettcstatus());
+
+    // Make sure tc is halted. (This done in h/w as part of reset.)
+    mips32_mt_settchalt(TCHALT_H); // H=1.
+	asm("ehb");
+	pr_info("tc is halted = %x\n", (unsigned int)mips32_mt_gettchalt());
+
+    // Make sure tc is bound to vpe. (This is done in h/w as part of reset.)
+    mips32_mt_settcbind ((vpe_num << TCBIND_CURVPE_SHIFT) | (tc_num << TCBIND_CURTC_SHIFT));
+	asm("ehb");
+	pr_info("gettcbind = %x\n", (unsigned int)mips32_mt_gettcbind());
+
+    // Set the stack pointer so we can call a c function
+    mips32_mt_setsp(stack_top);
+	asm("ehb");
+	pr_info("stack top = %x\n", stack_top);
+
+    //_m32c0_mttgpr(25, start_addr);
+	//asm("ehb");
+
+    // Set the global pointer
+     _m32c0_mttgpr(28, &umt_tc_thread_gp);
+	asm("ehb");
+
+    if(arg0)
+    {
+        // Pass the Arg0
+        _m32c0_mttgpr(4, arg0);
+	asm("ehb");
+    }
+
+    if(arg1)
+    {
+        // Pass the Arg1
+        _m32c0_mttgpr(5, arg1);
+		asm("ehb");
+    }
+
+    // Point tc to code
+    mips32_mt_settcrestart(start_addr);
+	asm("ehb");
+	pr_info("start address of TC = %x\n", (unsigned int)mips32_mt_gettcrestart());
+
+    mips32_mt_settcstatus(mips32_mt_gettcstatus() | (TCSTATUS_A | TCSTATUS_DA));
+	asm("ehb");
+	pr_info("current TC status = %x\n", (unsigned int)mips32_mt_gettcstatus());
+
+        // Unhalt tc.
+    mips32_mt_settchalt(0); // H=0 (allow vpe_num/tc_num to execute).
+	asm("ehb");
+	pr_info("tc is unhalted = %x\n", (unsigned int)mips32_mt_gettchalt());
+
+    // Enable threading.
+    mips32_mt_setvpecontrol(mips32_mt_getvpecontrol() | VPECONTROL_TE); // TE=1.
+
+    // Turn off the vpe configuration flag and enable (other) vpe.
+    mips32_setmvpcontrol((mips32_getmvpcontrol() & ~MVPCONTROL_VPC) | MVPCONTROL_EVP); // VPC=0 EVP=1.
+	asm("ehb");
+	pr_info("current mvp control = %x\n", (unsigned int)mips32_getmvpcontrol());
+
+    return;
+}
+
+static inline void set_yield_mask(void *mask)
+{
+	u32 yqmask = read_c0_yqmask();
+
+	yqmask |= (*(u32 *)mask);
+	write_c0_yqmask(yqmask);
+	ehb();
+}
+
+
+void umt_callback_fn(struct thrd_param *param)
+{
+	u32 dq_cnt, dq_ptr;
+	int i = 0, ret = 0;
+	struct umt_port *port = NULL;
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	u32 umt_ep_dst_local = 0;
+
+	if (pumt->status != UMT_ENABLE)
+		return ;
+
+	for (i = 0; i < UMT_PORTS_NUM; i++) {
+		port = &pumt->ports[i];
+		umt_ep_dst_local = port->umt_ep_dst;
+		if (port->status == UMT_ENABLE && umt_ep_dst_local && port->umtid_map_cbmid)
+		{
+			ret = cbm_dequeue_dma_port_stats_get(port->umtid_map_cbmid, &dq_ptr, 0);
+			if (ret != 0)
+				continue;
+			if (dq_ptr >= port->dq_idx)
+				dq_cnt = dq_ptr - port->dq_idx;
+			else
+				dq_cnt = 0xFFFFFFFF - port->dq_idx + dq_ptr + 1;
+			
+			if (dq_cnt)
+			{
+				if (dq_cnt>32)
+					dq_cnt = 32;
+				writel(dq_cnt, (void * __force)(umt_ep_dst_local + 0x4));
+				g_tot_dq_cnt[i]+=dq_cnt;
+				port->dq_idx = port->dq_idx + dq_cnt;
+			}
+		}
+	}
+}
+
+static inline u8 umt_process_gptc(u32 gptc_num, struct thrd_param *param)
+{
+	int irq = (GPTC_1A_GIC_IRQ-8);
+
+	if (CHECK_BIT(REG32(gptc_pmask), gptc_maskbit)) { 
+		jiffies1++;
+		GIC_CLR_INTR_MASK(irq) ;
+		gic_clear_edge(irq);
+		wmb();
+		/* Do the UMT stuff */
+		umt_callback_fn(param);
+		//Clear the Interrupt Status Register
+		REG32(GPTC1_IRNCR) = ((u32)0x1 << gptc_num);
+		GIC_SET_INTR_MASK(irq);
+	} else {
+		jiffies2++;
+	}
+	return 1;
+}
+#ifdef RUN_MIPS_TC
+asmlinkage
+static void mips_tc_thread (u32 arg0, u32 arg1)
+{
+	u32 mask = (1 << 8);
+	struct thrd_param *param = (struct thrd_param *) arg0;
+	struct mips_tc_sg_mem *shared_mem;
+	shared_mem = (struct mips_tc_sg_mem *)param->mips_tc_shared_ctxt_mem;
+	/* init the yq mask */
+	set_yield_mask(&mask);
+
+	while (1) {
+		mips_mt_yield(1<<8);
+		mips_tc_sg(shared_mem);
+	}
+
+}
+#endif /* RUN_MIPS_TC */
+
+asmlinkage
+static void umt_tc_thread (u32 arg0, u32 arg1)
+{
+	u32 mask = (1 << 8);
+	struct thrd_param *param = (struct thrd_param *) arg0;
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+	struct mips_tc_sg_mem *shared_mem;
+#endif
+	//struct thrd_param *param = &g_umt_thread_info;
+
+	/* init the yq mask */
+	set_yield_mask(&mask);
+
+	jiffies1 = 0;
+	jiffies2 = 0;
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+	shared_mem = (struct mips_tc_sg_mem *)param->mips_tc_shared_ctxt_mem;
+#endif
+
+	while (1) {
+		mips_mt_yield(1<<8);
+		umt_process_gptc(GPTC_3A, param);
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+#ifndef RUN_MIPS_TC
+		mips_tc_sg(shared_mem);
+	if (CHECK_BIT(REG32(GPTC1_IRNICR), GPTC_3A))
+			{
+			REG32(GPTC1_IRNCR) = ((u32)0x1 << GPTC_3A);
+			}
+#endif /* RUN_MIPS_TC */
+#endif
+	}	
+
+}
+static irqreturn_t umt_tc_dummy_isr(int irq, void *dev_id)
+{
+	return IRQ_HANDLED;
+}
+
+int umt_reset_port_dq_idx(u32 cbm_id)
+{
+	struct mcpy_umt *pumt;
+	struct umt_port *port;
+	u32 umt_id = 0;
+
+        pumt = mcpy_get_umt();
+
+        if (pumt->status != UMT_ENABLE) {
+                mcpy_dbg(MCPY_ERR, "UMT is not initialized!\n");
+                return -ENODEV;
+        }
+
+        for (umt_id = 0; umt_id < UMT_PORTS_NUM; umt_id++) {
+                port = &pumt->ports[umt_id];
+                if (port->umtid_map_cbmid == cbm_id)
+                        port->dq_idx = 0;
+        }
+
+	return 0;
+}
+
+static int ksw_umt_thread(void *arg)
+{
+#define SW_UMT_WAIT_DEFAULT 1000
+	static u32 jiffies1_pre = 0;
+	unsigned int sw_umt_msecs = SW_UMT_WAIT_DEFAULT; //1 seconds
+
+	do {
+		if ((g_umt_interval / 100) > SW_UMT_WAIT_DEFAULT)
+			sw_umt_msecs = (g_umt_interval / 100);
+		else
+			sw_umt_msecs = SW_UMT_WAIT_DEFAULT;
+
+		msleep(sw_umt_msecs);
+		if (jiffies1_pre == jiffies1) {
+			pr_info("sw umt init gptc again!\n");
+			REG32(GPTC1_IRNCR) = ((u32)0x1 << GPTC_3A);
+		}
+		jiffies1_pre = jiffies1;
+	} while (1);
+
+	return 0;
+}
+
+#endif
+
+/* TODO: Register UMT error interrupt Handler */
+int umt_init(struct mcpy_ctrl *pctrl)
+{
+	struct device_node *node = pctrl->dev->of_node;
+	struct mcpy_umt *pumt;
+	int i;
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+	int tc_num;
+	struct clk *gptc_clk;
+	u32 rate;
+#endif
+	mcpy_dbg(MCPY_INFO, "UMT initialize called on processor: %d !\n", smp_processor_id());
+
+	pumt = &pctrl->umt;
+	pumt->ctrl = pctrl;
+	pumt->dma_ctrlid = g_dma_ctrl;
+	spin_lock_init(&pumt->umt_lock);
+	umt_en_expand_mode();
+
+	for (i = 0; i < UMT_PORTS_NUM; i++)
+		umt_port_init(pumt, node, i);
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+#ifdef RUN_MIPS_TC
+	mips_tc_init(&g_mips_thread_info.mips_tc_shared_ctxt_mem);
+#else
+	mips_tc_init(&g_umt_thread_info.mips_tc_shared_ctxt_mem);
+#endif
+#endif
+	umt_proc_init(pumt);
+	pumt->status = UMT_ENABLE;
+
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+	gptc_maskbit = GPTC_1A_MASK_BIT((GPTC_1A_GIC_IRQ-8));
+	gptc_pmask = GPTC_1A_PMASK((GPTC_1A_GIC_IRQ-8));
+
+	/* fill the info needed by thread */
+	g_umt_thread_info.dummy = 0;
+
+	pr_info("<%s>pmask addr = %x and bit = %d\n", __FUNCTION__,gptc_pmask, gptc_maskbit);
+	/* start the thread on TC */
+	tc_num = vmb_tc_alloc(smp_processor_id());
+	pr_info("<%s>allocated TC num %d\n", __FUNCTION__,tc_num);
+	run_tc(tc_num, smp_processor_id(), (u32) &umt_tc_thread, (u32)&umt_tc_thread_stack + 4080, (u32)&g_umt_thread_info, 0);
+#ifdef RUN_MIPS_TC
+	tc_num = vmb_tc_alloc(smp_processor_id());
+	pr_info("<%s>allocated TC num %d\n", __FUNCTION__,tc_num);
+	run_tc(tc_num, smp_processor_id(), (u32) &mips_tc_thread, (u32)&mips_tc_thread_stack + 4080, (u32)&g_mips_thread_info, 0);
+#endif /* RUN_MIPS_TC */
+	/* setup the interrupt and yeild */
+	if (request_irq(GPTC_1A_GIC_IRQ, umt_tc_dummy_isr, IRQF_DISABLED, "umt_gptc_irq", NULL)) {
+		pr_err("%s: failed to request gptu irq - ", __func__);
+		return -ENOENT;
+	}
+
+	if (gic_yield_setup(smp_processor_id(), 8, GPTC_1A_GIC_IRQ)) {
+		pr_info("Yield Setup Fail:\n");
+		return -ENOENT;
+	} else {
+		pr_info("Yield setup passed for gptc_irq = %d\n", GPTC_1A_GIC_IRQ);
+	}
+
+	/* start the GPTC timer */
+	gptc_clk = clk_get_sys("16300000.gptu", "timer3a");
+	if (IS_ERR(gptc_clk)) {
+		pr_err("failed to get clock timer3a\r\n");
+		return -ENOENT;
+	}
+	rate = (1000 * 1000)/g_umt_interval;
+	pr_info("setting the gptu rate to: %d\n", (unsigned int)rate);
+	clk_set_rate(gptc_clk, rate);
+	if (clk_enable(gptc_clk)) {
+		pr_err("%s enable failed\r\n", __func__);
+		clk_set_rate(gptc_clk, 0);
+	} else {
+		pr_info("%s Timer 3A is running for UMT !\n", __func__);
+	}
+	
+	ksw_umt_tsk = kthread_create(ksw_umt_thread, NULL, "ksw_umt_thread");
+	if (IS_ERR(ksw_umt_tsk)) {
+		ksw_umt_tsk = NULL;
+		pr_err("create sw umt thread failed!\n");
+	} else {
+		wake_up_process(ksw_umt_tsk);
+	}
+#endif
+
+	mcpy_dbg(MCPY_INFO, "UMT initialize success on processor: %d !\n", smp_processor_id());
+
+	return 0;
+}
+
+
+
diff --git a/drivers/dma/ltq_umt_legacy.c b/drivers/dma/ltq_umt_legacy.c
new file mode 100644
--- /dev/null
+++ b/drivers/dma/ltq_umt_legacy.c
@@ -0,0 +1,341 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ * UMT Driver for GRX350 A11
+ */
+#define DEBUG
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <lantiq_dmax.h>
+	 
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <lantiq_irq.h>
+	 
+#include <net/datapath_proc_api.h>
+#include "ltq_hwmcpy_addr.h"
+#include <linux/ltq_hwmcpy.h>
+#include "ltq_hwmcpy.h"
+
+static int umt_dma_init(struct dma_ch *pchan, struct mcpy_umt *pumt)
+{
+	u32 dma_rxch, dma_txch;
+
+	pchan->rch = UMT_DMA_RX_CID;
+	pchan->tch = UMT_DMA_TX_CID;
+	pchan->rch_dnum = 1;
+	pchan->tch_dnum = 1;
+	pchan->rch_dbase =
+		pumt->ctrl->phybase + UMT_DBASE;
+		pchan->tch_dbase = pchan->rch_dbase + 0x10;
+	pchan->onoff = DMA_CH_ON;
+	sprintf(pchan->rch_name, "UMT RXCH");
+	sprintf(pchan->tch_name, "UMT TXCH");
+
+	dma_rxch = _DMA_C(pumt->ctrl->dma_ctrl_id,
+		pumt->ctrl->dma_port_id,
+		pchan->rch);
+	mcpy_dbg(MCPY_DBG, "dma_ch: 0x%x, rch_name:%s\n",
+		dma_rxch, pchan->rch_name);
+	if (ltq_request_dma(dma_rxch, pchan->rch_name) < 0) {
+		mcpy_dbg(MCPY_ERR,
+			"umt request dma chan [0x%x] fail\n", dma_rxch);
+		goto __UMT_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_rxch,
+			pchan->rch_dbase, pchan->rch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_rxch);
+		goto __UMT_PORT_FAIL;
+	}
+
+	dma_txch = _DMA_C(pumt->ctrl->dma_ctrl_id,
+			pumt->ctrl->dma_port_id,
+			pchan->tch);
+	mcpy_dbg(MCPY_DBG, "dma_ch: 0x%x, rch_name:%s\n",
+		dma_txch, pchan->tch_name);
+	if (ltq_request_dma(dma_txch, pchan->tch_name) < 0) {
+		mcpy_dbg(MCPY_ERR, "request dma chan [0x%x] fail\n", dma_txch);
+		goto __UMT_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_txch,
+			pchan->tch_dbase, pchan->tch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_txch);
+		goto __UMT_PORT_FAIL;
+	}
+
+	if (pchan->onoff == DMA_CH_ON) {
+		ltq_dma_chan_on(dma_rxch);
+		ltq_dma_chan_on(dma_txch);
+	} else {
+		ltq_dma_chan_off(dma_rxch);
+		ltq_dma_chan_off(dma_txch);
+	}
+
+	return 0;
+
+__UMT_PORT_FAIL:
+	pumt->status = UMT_BROKEN;
+	return -1;
+}
+
+static inline void umt_set_mode(enum umt_mode umt_mode)
+{
+	ltq_mcpy_w32_mask(0x2, ((u32)umt_mode) << 1, MCPY_GCTRL);
+}
+
+static inline void umt_set_msgmode(enum umt_msg_mode msg_mode)
+{
+	ltq_mcpy_w32(msg_mode, MCPY_UMT_SW_MODE);
+}
+
+static inline u32 umt_msec_to_cnt(int msec)
+{
+	return (msec * 0x17AC / 20);
+}
+
+static inline void umt_set_period(u32 umt_period)
+{
+	umt_period = umt_msec_to_cnt(umt_period);
+	ltq_mcpy_w32(umt_period, MCPY_UMT_PERD);
+}
+
+static inline void umt_set_dst(u32 umt_dst)
+{
+	ltq_mcpy_w32(umt_dst, MCPY_UMT_DEST);
+}
+
+static inline void umt_enable(enum umt_status status)
+{
+	ltq_mcpy_w32_mask(0x4, ((u32)status) << 2, MCPY_GCTRL);
+}
+
+/*This function will disable umt */
+static inline void umt_reset_umt(enum umt_mode mode)
+{
+	umt_enable(UMT_DISABLE);
+	if (mode == UMT_SELFCNT_MODE) {
+		umt_set_mode(UMT_USER_MODE);
+		umt_set_mode(UMT_SELFCNT_MODE);
+	} else {
+		umt_set_mode(UMT_SELFCNT_MODE);
+		umt_set_mode(UMT_USER_MODE);
+	}
+	return;
+}
+
+int ltq_umt_set_period(u32 period)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	if (period == 0)
+		return -1;
+
+	if (pumt->umt_period == period)
+		return 0;
+	else {
+		spin_lock_bh(&pumt->umt_lock);
+		pumt->umt_period = period;
+		umt_set_period(pumt->umt_period);
+		spin_unlock_bh(&pumt->umt_lock);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_set_period);
+
+/*
+* @umt_mode:	0-self-counting mode, 1-user mode.
+* @msg_mode:	0-No MSG, 1-MSG0 Only, 2-MSG1 Only, 3-MSG0 & MSG1.
+* @dst:  Destination PHY address.
+* @period: only applicable when set to self-counting mode.
+*		 self-counting interval time. if 0, use the original setting.
+* @enable: 1-Enable/0-Disable
+* @ret: -1 Fail/0-SUCCESS
+*/
+int ltq_umt_set_mode(u32 umt_mode, u32 msg_mode,
+			u32 phy_dst, u32 period, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+
+	if (pumt->status == UMT_BROKEN)
+		return -ENODEV;
+	if (umt_mode >= (u32)UMT_MODE_MAX
+			|| msg_mode >= (u32)UMT_MSG_MAX
+			|| enable   >= (u32)UMT_STATUS_MAX
+			|| phy_dst  == 0)
+		return -EINVAL;
+
+	spin_lock_bh(&pumt->umt_lock);
+	umt_reset_umt(pumt->umt_mode);
+
+	pumt->umt_mode = (enum umt_mode)umt_mode;
+	pumt->msg_mode = (enum umt_msg_mode)msg_mode;
+	pumt->umt_dst	= phy_dst;
+	if (period)
+		pumt->umt_period = period;
+	pumt->status = (enum umt_status)enable;
+
+	umt_set_mode(pumt->umt_mode);
+	umt_set_msgmode(pumt->msg_mode);
+	umt_set_dst(pumt->umt_dst);
+	if (period)
+		umt_set_period(pumt->umt_period);
+	umt_enable(pumt->status);
+	spin_unlock_bh(&pumt->umt_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_set_mode);
+
+/*
+* Enable/Disable UMT
+* @enable: 1-Enable/0-Disable
+* ret: -1 Fail/0-Success
+*/
+int ltq_umt_enable(u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	if (enable >= (u32)UMT_STATUS_MAX
+			|| pumt->status == UMT_BROKEN)
+		return -ENODEV;
+
+	if (pumt->status != enable) {
+		spin_lock_bh(&pumt->umt_lock);
+		pumt->status = (enum umt_status)enable;
+		umt_enable(pumt->status);
+		spin_unlock_bh(&pumt->umt_lock);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_enable);
+
+static void umt_access_wkrd(void)
+{
+	unsigned int __iomem *addr;
+	u32 val;
+
+	addr = (unsigned int __iomem *)GCR_CFG_ENABLE;
+	ltq_w32(0x123F0001, addr);
+	addr = (unsigned int __iomem *)GCR_CCA_IC_MREQ(DMA3_MASTERID);
+	ltq_wk_w32(2, addr);
+	addr = (unsigned int __iomem *)GCR_CTRL_REG;
+	val = ltq_r32(addr);
+	val |= 2;
+	ltq_w32(val, addr);
+	mcpy_dbg(MCPY_INFO, "UMT workaround applied!!\n");
+}
+
+static void umt_chip_init(struct mcpy_umt *pumt)
+{
+	umt_access_wkrd();
+	umt_set_mode(pumt->umt_mode);
+	umt_set_msgmode(pumt->msg_mode);
+	if (pumt->umt_period)
+		umt_set_period(pumt->umt_period);
+	if (pumt->umt_dst)
+		umt_set_dst(pumt->umt_dst);
+	umt_enable((u32)pumt->status);
+}
+
+/* TODO: Register UMT error interrupt Handler */
+void umt_init(struct mcpy_ctrl *pctrl)
+{
+	struct device_node *node = pctrl->dev->of_node;
+	struct mcpy_umt *pumt;
+
+	pumt = &pctrl->umt;
+	pumt->ctrl = pctrl;
+	pumt->umt_mode = UMT_SELFCNT_MODE;
+	pumt->status = UMT_DISABLE;
+	pumt->msg_mode = UMT_MSG0_MSG1;
+	pumt->ctrl = pctrl;
+
+	if (of_property_read_u32(node,
+		"lantiq,umt-period", &pumt->umt_period) < 0)
+	pumt->umt_period = 0;	 
+	spin_lock_init(&pumt->umt_lock);
+	pumt->dev = pctrl->dev;
+
+	if (umt_dma_init(&pumt->chan, pumt) < 0)
+		return;
+	umt_chip_init(pumt);
+	return;
+}
+
+static int umt_cfg_read_proc(struct seq_file *s, void *v)
+{
+	struct mcpy_umt *pumt = s->private;
+
+	seq_puts(s, "\nUMT configuration\n");
+	seq_puts(s, "-----------------------------------------\n");
+	seq_printf(s, "UMT Mode                 %s\n",
+		pumt->umt_mode == UMT_SELFCNT_MODE ?
+		"UMT SelfCounting Mode" : "UMT User Mode");
+	switch (pumt->msg_mode) {
+	case UMT_NO_MSG:
+		seq_puts(s, "UMT MSG Mode             UMT NO MSG\n");
+		break;
+	case UMT_MSG0_ONLY:
+		seq_puts(s, "UMT MSG Mode             UMT MSG0 Only\n");
+		break;
+	case UMT_MSG1_ONLY:
+		seq_puts(s, "UMT MSG Mode             UMT MSG1 Only\n");
+		break;
+	case UMT_MSG0_MSG1:
+		seq_puts(s, "UMT MSG Mode             UMT_MSG0_And_MSG1\n");
+		break;
+	default:
+		seq_printf(s, "UMT MSG Mode Error! Msg_mode: %d\n",
+			pumt->msg_mode);
+	}
+	seq_printf(s, "UMT DST                  0x%x\n", pumt->umt_dst);
+	if (pumt->umt_mode == UMT_SELFCNT_MODE)
+		seq_printf(s, "UMT Period               0x%x\n",
+			pumt->umt_period == 0 ? 0x17AC : pumt->umt_period);
+	seq_printf(s, "UMT Status               %s\n",
+			pumt->status == UMT_ENABLE ? "Enable" :
+			pumt->status == UMT_DISABLE ? "Disable" : "Init Fail");
+	seq_printf(s, "dma rxch_id: %d, rch_base: 0x%x, rch_des_num: %d\n",
+		pumt->chan.rch, pumt->chan.rch_dbase,
+		pumt->chan.rch_dnum);
+	seq_printf(s, "dma txch_id: %d, tch_base: 0x%x, tch_des_num: %d\n",
+		pumt->chan.tch, pumt->chan.tch_dbase,
+		pumt->chan.tch_dnum);
+	seq_printf(s, "dma chan on/off: %s\n",
+		pumt->chan.onoff == DMA_CH_ON ? "ON" : "OFF");
+
+	return 0;
+}
+
+static int umt_cfg_read_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, umt_cfg_read_proc, PDE_DATA(inode));
+}
+
+static const struct file_operations mcpy_umt_proc_fops = {
+	.open           = umt_cfg_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = single_release,
+};
+
+
diff --git a/drivers/dma/mips_tc_sg.c b/drivers/dma/mips_tc_sg.c
new file mode 100644
--- /dev/null
+++ b/drivers/dma/mips_tc_sg.c
@@ -0,0 +1,1161 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2017 
+ * MIPS TC FW Scatter-Gathering
+ */
+#define DEBUG
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <asm/ltq_vmb.h>
+
+#include <lantiq_dmax.h>
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <lantiq_irq.h>
+
+#include <net/datapath_proc_api.h>
+#include "ltq_hwmcpy_addr.h"
+#include <linux/ltq_hwmcpy.h>
+#include "ltq_hwmcpy.h"
+
+#include <linux/kthread.h>
+#include <linux/sched.h>
+#include <linux/err.h>
+
+#include "ltq_hwmcpy_addr.h"
+#include "mips_tc_sg.h"
+#include "net/lantiq_cbm_api.h"
+
+#define MIPS_TC_PRIV_DATA_LEN (1024 * 64)
+#define MIPS_TC_MAJOR_VER 0
+#define MIPS_TC_MID_VER 0
+#define MIPS_TC_MINOR_VER 1
+
+static  void mips_tc_memcpy_dispatch();
+static  void mips_tc_memcpy_pp();
+
+//static inline void mips_tc_test_init();
+static  void mips_tc_test_results(struct mips_tc_tx_descriptor *,struct mips_tc_rx_descriptor *);
+static  void mips_tc_conf_init();
+static  void mips_tc_memcpy_dma_rx(struct mips_tc_tx_des_soc soc_rx_out_src_des, u32 *soc_rx_out_dst_des_ptr, struct mips_tc_rx_descriptor *sg_dma_rx_des_ptr, struct mips_tc_metadata *metadata_ptr);
+static  void mips_tc_memcpy_dma_tx(struct mips_tc_tx_des_soc soc_rx_out_src_des, struct mips_tc_tx_descriptor *sg_dma_tx_des_ptr);
+static inline void mips_tc_soc_rx_out_src_rel(struct mips_tc_tx_des_soc *soc_rx_out_src_des_ptr);
+static inline void mips_tc_dma_tx_rel();
+static inline void mips_tc_dma_rx_rel();
+static  void mips_tc_soc_rx_out_dst_enq(struct mips_tc_rx_des_soc *soc_rx_out_dst_des_ptr);
+static  void mips_tc_rx_in_umt();
+unsigned int mips_tc_cal_des_dist(unsigned int idx1, unsigned int idx2, unsigned int des_num);
+
+
+
+#define MAX_PKT_PROCESS 50
+#define DMA0_DES_NUM_MASK	0x7FF
+
+
+#define DMA_2ND_READ	0
+#define DMA_2ND_WRITE	1
+
+struct mips_tc_q_cfg_ctxt *soc_rx_out_src_ctxt, *soc_rx_out_dst_ctxt, *sg_dma_tx_ctxt, *sg_dma_rx_ctxt;
+struct mips_tc_fw_memcpy_ctxt_t *fw_memcpy_ctxt;
+struct mips_tc_fw_pp_ctxt_t *fw_pp_ctxt;
+struct mips_tc_info *info;
+u32 *rxin_hd_acc_addr;
+u32 tc_mode_bonding;
+u32 loop_cnt;
+struct mips_tc_metadata *metadata;
+int prnt_flag = 0;
+u32 *dma1_tx_data_ptr_base;
+//struct mips_tc_metadata pseudo_metadata[2048];
+
+struct mips_tc_sg_mem *shared_working_vir_mem;
+struct mips_tc_sg_mem *shared_vir_uncached_mem;
+struct mips_tc_sg_mem *shared_vir_cached_mem;
+dma_addr_t shared_phy_mem;
+
+int global_cnt = 0;
+
+
+/*
+ * Linux kernel thread to print out message from DSL FW TC
+ * */
+void kernel_debug_thread(void *data)
+{
+	int i = 0;
+	struct mips_tc_msg_param *msg;
+	struct mips_tc_ctrl_dbg *ctrl;
+	ctrl = &shared_working_vir_mem->ctrl;
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		if(kthread_should_stop())
+			break;
+		//printk("%s: idx[%d] debug flags [%d]\n", __func__,
+			//	ctrl->msg_cur_idx, ctrl->debug_flags);
+		if (ctrl->dbg_flags) {
+			/* check for message in shared memory */
+			for (i = 0; i < MIPS_TC_DEBUG_MSG_ARRAY_LEN; i++) {
+				msg = &ctrl->msgs[(ctrl->msg_cur_idx + i)
+						% MIPS_TC_DEBUG_MSG_ARRAY_LEN];
+				//printk("%s: msg_cur_idx [%d]%d-%s\n", __func__, ctrl->msg_cur_idx,
+					//msg->flags, msg->message);
+				if (msg->flags) {
+					pr_info("%s", msg->message);
+					msg->flags = 0;
+					ctrl->msg_cur_idx = (ctrl->msg_cur_idx + i)
+						% MIPS_TC_DEBUG_MSG_ARRAY_LEN;
+				} else
+					break;
+			}
+		}
+		msleep(100);
+	}
+}
+
+/*
+ * Print debug function from TC, not linux kernel
+ * */
+int tc_print(struct mips_tc_sg_mem * shared_vir_mem,
+		int level, const char* fmt, ...)
+{
+	/* check for message in shared memory */
+	int i;
+	struct mips_tc_msg_param *msg;
+	struct mips_tc_ctrl_dbg *ctrl;
+	char str[MIPS_TC_DEBUG_MSG_LEN];
+	va_list vl;
+	va_start(vl, fmt);
+	vsnprintf( str, sizeof( str), fmt, vl);
+	va_end(vl);
+	ctrl = &shared_vir_mem->ctrl;
+	*(str + MIPS_TC_DEBUG_MSG_LEN - 1) = 0;
+	for (i = 0; i < MIPS_TC_DEBUG_MSG_ARRAY_LEN; i++) {
+		msg = &ctrl->msgs[(ctrl->msg_cur_idx + i)
+			% MIPS_TC_DEBUG_MSG_ARRAY_LEN];
+		if (msg->flags == 0) {
+			msg->flags = level;
+			memcpy((void *)msg->message, (void *) str, strlen(str));
+			break;
+		}
+	}
+	return (MIPS_TC_DEBUG_MSG_ARRAY_LEN - i);
+}
+int mips_tc_get_shared_mem(struct mips_tc_sg_mem **shared_mem)
+{
+	if (!shared_working_vir_mem) {
+		*shared_mem = NULL;
+		return -1;
+	}
+	*shared_mem = shared_working_vir_mem;
+	return 0;
+}
+EXPORT_SYMBOL(mips_tc_get_shared_mem);
+extern struct device *mcpy_get_dev(void);
+static struct task_struct *dbg_task;
+#ifdef USE_WLAN_SRAM
+struct mips_tc_sg_mem g_static_mem;
+#define BBCPU_SRAM_ADDR 0xb4000000
+#define BBCPU_SRAM_SIZE 0x800000
+#define SHARED_MEM_CTXT BBCPU_SRAM_ADDR
+#define PRIV_MEM_DATA (BBCPU_SRAM_ADDR + 0x2000)
+#endif
+void mips_tc_init(u32 *shared_ctxt_mem)
+{
+	struct device *dev;
+	struct mips_tc_ctrl_dbg *ctrl;
+	u32 data_mem_vir;
+	dma_addr_t data_mem_phy;
+	dev = mcpy_get_dev();
+	if (!dev) {
+		pr_err("%s:%d: HWCPY is Invalid\n", __func__, __LINE__);
+		return;
+	}
+	#ifdef USE_WLAN_SRAM
+	shared_vir_uncached_mem = (struct mips_tc_sg_mem *)SHARED_MEM_CTXT;
+	shared_vir_cached_mem = (struct mips_tc_sg_mem *)SHARED_MEM_CTXT;
+	shared_working_vir_mem = shared_vir_cached_mem;
+	memset(shared_working_vir_mem, 0, sizeof(struct mips_tc_sg_mem));
+	#else
+	shared_vir_uncached_mem = dma_zalloc_coherent(dev,
+		sizeof(struct mips_tc_sg_mem),
+		&shared_phy_mem,
+		GFP_ATOMIC | GFP_DMA
+	);
+	shared_vir_cached_mem = (struct mips_tc_sg_mem *)
+		__va(shared_phy_mem);
+	shared_working_vir_mem = shared_vir_cached_mem;
+	#endif
+	if (!shared_working_vir_mem) {
+		pr_err("%s:%d: Invalid shared memory\n", __func__, __LINE__);
+		return -1;
+	}
+#ifdef USE_WLAN_SRAM
+	shared_vir_uncached_mem->ctrl.priv_data_vir =
+		PRIV_MEM_DATA;
+	memset((void *)shared_vir_uncached_mem->ctrl.priv_data_vir,
+		0, 2048 * 4);
+#else
+	shared_vir_uncached_mem->ctrl.priv_data_len = MIPS_TC_PRIV_DATA_LEN;
+	shared_vir_uncached_mem->ctrl.priv_data_vir = dma_zalloc_coherent(dev,
+		shared_vir_uncached_mem->ctrl.priv_data_len,
+		&shared_vir_uncached_mem->ctrl.priv_data_phy,
+		GFP_ATOMIC | GFP_DMA
+	);
+#endif
+	printk("=MIPS TC Shared memory: %p %p %p | Data: %08x\n",
+			shared_vir_uncached_mem,
+			shared_vir_cached_mem,
+			shared_working_vir_mem,
+			shared_vir_uncached_mem->ctrl.priv_data_vir);
+	*shared_ctxt_mem = (u32)shared_working_vir_mem;
+	/* firmware information */
+	ctrl = &shared_vir_cached_mem->ctrl;
+	ctrl->ver_major = MIPS_TC_MAJOR_VER;
+	ctrl->ver_mid = MIPS_TC_MID_VER ;
+	ctrl->ver_minor = MIPS_TC_MINOR_VER;
+	if(IS_ERR(dbg_task)){
+		printk("Unable to start kernel thread.\n");
+		dbg_task = NULL;
+		return;
+	}
+	wake_up_process(dbg_task);
+}
+
+
+static  void mips_tc_conf_init(struct mips_tc_sg_mem *shared_vir_mem)
+{
+	/* Point to cached shared memory */
+	soc_rx_out_src_ctxt = (struct mips_tc_q_cfg_ctxt *)(&(shared_vir_mem->conf.soc_rx_out_src_ctxt));
+	soc_rx_out_dst_ctxt = (struct mips_tc_q_cfg_ctxt *)(&(shared_vir_mem->conf.soc_rx_out_dst_ctxt));
+	sg_dma_tx_ctxt		= (struct mips_tc_q_cfg_ctxt *)(&(shared_vir_mem->conf.soc_sg_dma_tx_ctxt));
+	sg_dma_rx_ctxt		= (struct mips_tc_q_cfg_ctxt *)(&(shared_vir_mem->conf.soc_sg_dma_rx_ctxt));
+	fw_memcpy_ctxt		= (struct mips_tc_fw_memcpy_ctxt_t *)(&(shared_vir_mem->conf.fw_memcpy_ctxt));
+	fw_pp_ctxt			= (struct fw_pp_ctxt *)(&(shared_vir_mem->conf.fw_pp_ctxt));
+	info				= &shared_vir_mem->mib;
+	rxin_hd_acc_addr 	= (u32 *)(&(shared_vir_mem->conf.aca_hw_rxin_hd_acc_addr));
+	tc_mode_bonding 	= shared_vir_mem->conf.tc_mode_bonding;
+	loop_cnt 	= shared_vir_mem->ctrl.ctrl_max_process_pkt;
+	if (loop_cnt == 0) {
+		loop_cnt = MAX_PKT_PROCESS;
+		shared_vir_mem->ctrl.ctrl_max_process_pkt = loop_cnt;
+	}
+
+	//cached address
+	metadata = (struct mips_tc_metadata *)__va(shared_vir_mem->ctrl.priv_data_phy);
+	dma1_tx_data_ptr_base 	= shared_vir_mem->conf.cache_rxout_ptr;
+
+	//fw_pp_ctxt->dma_tx_ptr = (sg_dma_tx_ctxt->des_base_addr & 0x0FFFFFFF);
+	//fw_pp_ctxt->dma_rx_ptr = (sg_dma_rx_ctxt->des_base_addr & 0x0FFFFFFF);
+	
+}
+
+unsigned int mips_tc_cal_des_dist(unsigned int idx1, unsigned int idx2, unsigned int des_num)
+{
+	uint32_t result;
+
+	if (idx1 >= idx2)
+		result = (idx1 - idx2);
+	else
+		result = (des_num + idx1 - idx2);
+
+	return result;
+} 
+
+static int retries = 10000;
+static int index = 0;
+
+int mips_tc_sg(struct mips_tc_sg_mem *shared_vir_mem)
+{
+#if 0
+	retries--;
+	if (retries == 0) {
+		tc_print(shared_vir_mem, 1, "Hello from DSL %d\n", index++);
+		retries = 10000;
+	
+	}
+#endif
+	if (shared_vir_mem->ctrl.mips_tc_control == MIPS_TC_C_STOP)
+	{
+		//tc_print(shared_vir_mem, 1, "\n Hello from DSL, mips_tc_c_stop %s Not Run\n", __FUNCTION__);
+		shared_vir_mem->mib.state = MIPS_TC_S_IDLE;
+		return 0;
+	}
+	if (shared_vir_mem->mib.state == MIPS_TC_S_IDLE) {
+		mips_tc_conf_init(shared_vir_mem);
+		shared_vir_mem->mib.state = MIPS_TC_S_RUNNING;
+	}
+
+	if(!(sg_dma_tx_ctxt->des_base_addr)||!(sg_dma_rx_ctxt->des_base_addr)||!(soc_rx_out_src_ctxt->des_base_addr)||!(soc_rx_out_dst_ctxt->des_base_addr))
+	{
+		retries--;
+		if (retries == 0) {
+		tc_print(shared_vir_mem, 1, "Hello from DSL Not Run%d\n", index++);
+		tc_print(shared_vir_mem, 1, "\n sg dma base= %x, sg_dma_tx_ctxt = %x\n", sg_dma_tx_ctxt->des_base_addr, sg_dma_tx_ctxt);
+		retries = 1000000;
+		}
+		
+		return 0;
+	}
+	else
+	{ 
+		if(!prnt_flag)
+		{	
+			//mips_tc_test_init(shared_vir_mem);
+		
+			tc_print(shared_vir_mem, 1, "\n Hello from DSL %s \n", __FUNCTION__);
+			tc_print(shared_vir_mem, 1, "<%s> SG DMA TX Base = %x and SG DMA RX Base = %x\n", __FUNCTION__,sg_dma_tx_ctxt->des_base_addr, sg_dma_rx_ctxt->des_base_addr);
+			prnt_flag = 1;
+		}
+	}
+	
+	#if 0
+	if(global_cnt % 2 == 0)
+	{
+		mips_tc_memcpy_dispatch();
+	}
+
+	else
+	{
+		mips_tc_memcpy_pp();
+	}
+	
+	global_cnt++;
+	#endif
+	mips_tc_memcpy_dispatch();
+	mips_tc_memcpy_pp();
+	
+	
+	return 0;
+}
+
+//Module Level Debug Code Start
+struct mips_tc_q_cfg_ctxt pseudo_soc_rx_out_src_ctxt, pseudo_soc_rx_out_dst_ctxt, pseudo_sg_dma_tx_ctxt, pseudo_sg_dma_rx_ctxt;
+struct mips_tc_fw_memcpy_ctxt_t pseudo_fw_memcpy_ctxt;
+struct mips_tc_fw_pp_ctxt_t pseudo_fw_pp_ctxt;
+struct mips_tc_tx_des_soc pseudo_soc_rx_out_src_des[255];
+struct mips_tc_tx_descriptor pseudo_sg_dma_tx_des[255];
+struct mips_tc_rx_des_soc pseudo_soc_rx_out_dst_des[255];
+struct mips_tc_rx_descriptor pseudo_sg_dma_rx_des[255];
+
+
+static  void mips_tc_test_init(
+	struct mips_tc_sg_mem *shared_vir_mem)
+{	
+	//struct mips_tc_tx_descriptor *tx_des_ptr;
+	//struct mips_tc_rx_descriptor *rx_des_ptr;
+	
+	soc_rx_out_src_ctxt = &pseudo_soc_rx_out_src_ctxt;//0x7a547800;
+	soc_rx_out_dst_ctxt = &pseudo_soc_rx_out_dst_ctxt;
+	sg_dma_tx_ctxt 		= &pseudo_sg_dma_tx_ctxt;
+	sg_dma_rx_ctxt 		= &pseudo_sg_dma_rx_ctxt;
+	
+	fw_memcpy_ctxt		= &pseudo_fw_memcpy_ctxt;
+	fw_pp_ctxt			= &pseudo_fw_pp_ctxt;
+	
+	soc_rx_out_src_ctxt->des_in_own_val = 1;
+	//soc_rx_out_src_ctxt->des_num = 255;
+	soc_rx_out_src_ctxt->des_num = 3;
+	soc_rx_out_src_ctxt->des_base_addr = &pseudo_soc_rx_out_src_des[0];
+	soc_rx_out_src_ctxt->rd_pkt_cnt = 0;
+	soc_rx_out_src_ctxt->wr_pkt_cnt = 0;
+	soc_rx_out_src_ctxt->rd_frag_cnt = 0;
+	soc_rx_out_src_ctxt->wr_frag_cnt = 0;
+	soc_rx_out_src_ctxt->rd_byte_cnt = 0;
+	soc_rx_out_src_ctxt->wr_byte_cnt = 0;
+	
+	soc_rx_out_dst_ctxt->des_in_own_val = 1;
+	//soc_rx_out_dst_ctxt->des_num = 255;
+	soc_rx_out_dst_ctxt->des_num = 3;
+	soc_rx_out_dst_ctxt->des_base_addr = &pseudo_soc_rx_out_dst_des[0];
+	soc_rx_out_dst_ctxt->rd_pkt_cnt = 0;
+	soc_rx_out_dst_ctxt->wr_pkt_cnt = 0;
+	soc_rx_out_dst_ctxt->rd_frag_cnt = 0;
+	soc_rx_out_dst_ctxt->wr_frag_cnt = 0;
+	soc_rx_out_dst_ctxt->rd_byte_cnt = 0;
+	soc_rx_out_dst_ctxt->wr_byte_cnt = 0;
+	
+	
+	sg_dma_tx_ctxt->des_in_own_val = 1;
+	sg_dma_tx_ctxt->des_num= 255;
+	//sg_dma_tx_ctxt->des_num= 2;
+	sg_dma_tx_ctxt->des_base_addr = &pseudo_sg_dma_tx_des[0];
+	sg_dma_tx_ctxt->rd_pkt_cnt = 0;
+	sg_dma_tx_ctxt->wr_pkt_cnt = 0;
+	sg_dma_tx_ctxt->rd_frag_cnt = 0;
+	sg_dma_tx_ctxt->wr_frag_cnt = 0;
+	sg_dma_tx_ctxt->rd_byte_cnt = 0;
+	sg_dma_tx_ctxt->wr_byte_cnt = 0;
+	
+	sg_dma_rx_ctxt->des_in_own_val = 1;
+	sg_dma_rx_ctxt->des_num = 255;
+	//sg_dma_rx_ctxt->des_num = 2;
+	sg_dma_rx_ctxt->des_base_addr = &pseudo_sg_dma_rx_des[0];
+	sg_dma_rx_ctxt->rd_pkt_cnt = 0;
+	sg_dma_rx_ctxt->wr_pkt_cnt = 0;
+	sg_dma_rx_ctxt->rd_frag_cnt = 0;
+	sg_dma_rx_ctxt->wr_frag_cnt = 0;
+	sg_dma_rx_ctxt->rd_byte_cnt = 0;
+	sg_dma_rx_ctxt->wr_byte_cnt = 0;
+	
+	pseudo_soc_rx_out_src_des[0].dword2_3.own = 1;
+	pseudo_soc_rx_out_src_des[0].dword2_3.sop = 1;
+	pseudo_soc_rx_out_src_des[0].dword2_3.eop = 0;
+	pseudo_soc_rx_out_src_des[0].dword2_3.data_ptr = 0x80000000;
+	pseudo_soc_rx_out_src_des[0].dword2_3.data_len = 12;
+	pseudo_soc_rx_out_src_des[0].dword2_3.byte_off = 7;
+	
+	pseudo_soc_rx_out_src_des[1].dword2_3.own = 1;
+	pseudo_soc_rx_out_src_des[1].dword2_3.sop = 0;
+	pseudo_soc_rx_out_src_des[1].dword2_3.eop = 0;
+	pseudo_soc_rx_out_src_des[1].dword2_3.data_ptr = 0x81000000;
+	pseudo_soc_rx_out_src_des[1].dword2_3.data_len = 12;
+	
+	pseudo_soc_rx_out_src_des[2].dword2_3.own = 1;
+	pseudo_soc_rx_out_src_des[2].dword2_3.sop = 0;
+	pseudo_soc_rx_out_src_des[2].dword2_3.eop = 1;
+	pseudo_soc_rx_out_src_des[2].dword2_3.data_ptr = 0x82000000;
+	pseudo_soc_rx_out_src_des[2].dword2_3.data_len = 12;
+	
+	pseudo_soc_rx_out_dst_des[0].dword2_3.data_ptr = 0x90000000;
+	pseudo_soc_rx_out_dst_des[0].dword2_3.own = 0;
+	
+	pseudo_soc_rx_out_dst_des[1].dword2_3.data_ptr = 0xa0000000;
+	pseudo_soc_rx_out_dst_des[1].dword2_3.own = 0;
+	
+	pseudo_soc_rx_out_dst_des[2].dword2_3.data_ptr = 0xb0000000;
+	pseudo_soc_rx_out_dst_des[2].dword2_3.own = 0;
+	
+	tc_print(shared_vir_mem, 1, "<%s> src_des_base = %x, dst_des_base = %x; sg_tx_des_base = %x; sg_rx_des_base = %x\n", __FUNCTION__,&pseudo_soc_rx_out_src_des[0], &pseudo_soc_rx_out_dst_des[0], &pseudo_sg_dma_tx_des[0], &pseudo_sg_dma_rx_des[0]);
+	tc_print(shared_vir_mem, 1, "<%s> src_ctxt = %x, dst_ctxt = %x; sg_tx_ctxt = %x; sg_rx_ctxt = %x; \n", __FUNCTION__,&pseudo_soc_rx_out_src_ctxt, &pseudo_soc_rx_out_dst_ctxt, &pseudo_sg_dma_tx_ctxt, &pseudo_sg_dma_rx_ctxt);
+	tc_print(shared_vir_mem, 1, "<%s> memcpy_ctxt = %x, pp_ctxt = %x\n", __FUNCTION__,&pseudo_fw_memcpy_ctxt, &pseudo_fw_pp_ctxt);
+	
+
+	
+}
+
+static  void mips_tc_test_results(struct mips_tc_tx_descriptor *sg_dma_tx_des_ptr,struct mips_tc_rx_descriptor *sg_dma_rx_des_ptr )
+{
+	
+	int *sg_dma_tx_des_dw0, *sg_dma_tx_des_dw1, *sg_dma_rx_des_dw0, *sg_dma_rx_des_dw1 ;
+	sg_dma_tx_des_dw0 = (int *)(sg_dma_tx_des_ptr);
+	sg_dma_tx_des_dw1 = sg_dma_tx_des_dw0 + 4;
+	
+	sg_dma_rx_des_dw0 = (int *)(sg_dma_rx_des_ptr);
+	sg_dma_rx_des_dw1 = sg_dma_rx_des_dw0 + 4;
+	
+	//pr_info("<%s> TX des dw0 = %x, TX des dw1 = %x; RX des dw0 = %x; RX des dw1 = %x\n", __FUNCTION__,*sg_dma_tx_des_dw0, *sg_dma_tx_des_dw1, *sg_dma_rx_des_dw0, *sg_dma_rx_des_dw1);
+	//pr_info("<%s> fw_memcpy_ctxt:sop = %x, eop = %x; byte_off = %x; src_data_ptr = %x, dst_data_ptr = %x\n\n", __FUNCTION__, fw_memcpy_ctxt->sop, fw_memcpy_ctxt->eop, fw_memcpy_ctxt->byte_off, fw_memcpy_ctxt->src_data_ptr, fw_memcpy_ctxt->dst_data_ptr);
+
+}
+
+//Module Level Debug Code End
+
+static  void mips_tc_memcpy_dma_rx(struct mips_tc_tx_des_soc soc_rx_out_src_des, u32 *soc_rx_out_dst_des_ptr, struct mips_tc_rx_descriptor *sg_dma_rx_des_ptr, struct mips_tc_metadata *metadata_ptr)
+{
+	struct mips_tc_rx_descriptor sg_dma_rx_des;
+	u32 tmp_dst_data_ptr;
+	
+	fw_pp_ctxt->status = 2;
+	
+	//Save sop and eop information to context
+	fw_memcpy_ctxt->sop = soc_rx_out_src_des.dword2_3.sop;
+	fw_memcpy_ctxt->eop = soc_rx_out_src_des.dword2_3.eop;
+	fw_memcpy_ctxt->src_data_ptr = soc_rx_out_src_des.dword2_3.data_ptr;
+	
+	sg_dma_rx_des.sop_frag = soc_rx_out_src_des.dword2_3.sop;
+	sg_dma_rx_des.eop_frag = soc_rx_out_src_des.dword2_3.eop;
+	
+	//Workaround Memcpy Channel do not support metadata and data_len zero issue
+	metadata_ptr->sop_frag = soc_rx_out_src_des.dword2_3.sop;
+	metadata_ptr->eop_frag = soc_rx_out_src_des.dword2_3.eop;
+	metadata_ptr->data_len = soc_rx_out_src_des.dword2_3.data_len;
+	metadata_ptr->src_data_ptr = soc_rx_out_src_des.dword2_3.data_ptr;
+	
+	
+	sg_dma_rx_des.sop = 1;
+	sg_dma_rx_des.eop = 1;
+	
+	//Workaround memcpy DMA split issue
+	//sg_dma_rx_des.data_len = soc_rx_out_src_des.dword2_3.data_len;
+	sg_dma_rx_des.data_len = 0x800;
+				
+	if(fw_memcpy_ctxt->sop == 1)
+	{
+		//sg_dma_rx_des.data_ptr = soc_rx_out_dst_des.dword2_3.data_ptr;
+		sg_dma_rx_des.data_ptr = *soc_rx_out_dst_des_ptr;
+		sg_dma_rx_des.byte_off = 0;
+	}
+	else
+	{
+		tmp_dst_data_ptr = fw_memcpy_ctxt->dst_data_ptr;
+					
+		//Adjust data_ptr based on previous fragment data length and byte_off
+		if(fw_memcpy_ctxt->byte_off)
+		{
+			tmp_dst_data_ptr = tmp_dst_data_ptr + fw_memcpy_ctxt->data_len + fw_memcpy_ctxt->byte_off;
+		}
+		else
+		{
+			tmp_dst_data_ptr = tmp_dst_data_ptr + fw_memcpy_ctxt->data_len;
+		}
+		
+		sg_dma_rx_des.data_ptr = (tmp_dst_data_ptr/4) * 4;
+		sg_dma_rx_des.byte_off = tmp_dst_data_ptr % 4;
+			
+	}
+	
+	//Update context for next descriptor process
+	fw_memcpy_ctxt->dst_data_ptr = sg_dma_rx_des.data_ptr;
+	fw_memcpy_ctxt->byte_off = sg_dma_rx_des.byte_off;
+	fw_memcpy_ctxt->data_len = soc_rx_out_src_des.dword2_3.data_len;
+	
+	//Save to metadata to reduce cpu load, avoid reading descriptor in post-processing
+	metadata_ptr->dst_data_ptr = sg_dma_rx_des.data_ptr;
+	metadata_ptr->byte_off = sg_dma_rx_des.byte_off;
+
+	#if DMA_2ND_WRITE
+	sg_dma_rx_des.own = !(sg_dma_rx_ctxt->des_in_own_val);
+	
+	#else
+	
+	sg_dma_rx_des.own = sg_dma_rx_ctxt->des_in_own_val;
+	
+	#endif
+	
+	//Write to SG_DMA_DES_LIST in DDR
+	*sg_dma_rx_des_ptr = sg_dma_rx_des;
+	//sg_dma_rx_des_ptr->own = sg_dma_rx_ctxt->des_in_own_val;
+	wmb();
+	
+	//Increment the counters
+	sg_dma_rx_ctxt->wr_frag_cnt++;
+	
+	sg_dma_rx_ctxt->wr_byte_cnt += soc_rx_out_src_des.dword2_3.data_len;
+	
+	if(soc_rx_out_src_des.dword2_3.eop == 1)
+	{
+		sg_dma_rx_ctxt->wr_pkt_cnt++;
+	}
+	
+	soc_rx_out_dst_ctxt->rd_frag_cnt++;
+	
+	soc_rx_out_dst_ctxt->rd_byte_cnt += soc_rx_out_src_des.dword2_3.data_len;
+	
+	if(soc_rx_out_src_des.dword2_3.eop == 1)
+	{
+		soc_rx_out_dst_ctxt->rd_pkt_cnt++;
+	}
+	
+	#if DMA_2ND_WRITE
+	sg_dma_rx_des_ptr->own = sg_dma_rx_ctxt->des_in_own_val;
+	wmb();
+	
+	#endif
+	
+	
+	
+}
+
+static  void mips_tc_memcpy_dma_tx(struct mips_tc_tx_des_soc soc_rx_out_src_des, struct mips_tc_tx_descriptor *sg_dma_tx_des_ptr)
+{	
+	struct mips_tc_tx_descriptor sg_dma_tx_des;
+	
+	fw_pp_ctxt->status = 3;
+	
+	sg_dma_tx_des = soc_rx_out_src_des.dword2_3;
+	
+	//Copy the sop and eop to the metadata sop_frag and eop_frag in the descriptor
+	sg_dma_tx_des.sop_frag = soc_rx_out_src_des.dword2_3.sop;
+	sg_dma_tx_des.eop_frag = soc_rx_out_src_des.dword2_3.eop;
+	
+	sg_dma_tx_des.sop = 1;
+	sg_dma_tx_des.eop = 1;
+	
+	#if DMA_2ND_WRITE
+	
+	sg_dma_tx_des.own = !(sg_dma_tx_ctxt->des_in_own_val);
+	
+	#else
+	
+	sg_dma_tx_des.own = sg_dma_tx_ctxt->des_in_own_val;
+		
+	#endif
+		
+	//Write to SG_DMA_DES_LIST in DDR
+	*sg_dma_tx_des_ptr = sg_dma_tx_des;
+	wmb();
+		
+	//Increment counters
+	sg_dma_tx_ctxt->wr_frag_cnt++;
+		
+	sg_dma_tx_ctxt->wr_byte_cnt += soc_rx_out_src_des.dword2_3.data_len;
+	
+	if(soc_rx_out_src_des.dword2_3.eop == 1)
+	{
+		sg_dma_tx_ctxt->wr_pkt_cnt++;
+	}
+	
+	soc_rx_out_src_ctxt->rd_frag_cnt++;
+	
+	soc_rx_out_src_ctxt->rd_byte_cnt += soc_rx_out_src_des.dword2_3.data_len;
+	
+	if(soc_rx_out_src_des.dword2_3.eop == 1)
+	{
+		soc_rx_out_src_ctxt->rd_pkt_cnt++;
+	}
+	
+	#if DMA_2ND_WRITE
+	sg_dma_tx_des_ptr->own = sg_dma_tx_ctxt->des_in_own_val;
+	wmb();
+	
+	#endif
+}
+
+static void __inline__ my_copy_2dw(volatile void * src_addr,void *dst_addr)
+{
+ asm("lw $8, 0(%0) ;  lw $9, 4(%0) ;   sw $8, 0(%1) ; sw $9, 4(%1)"
+ 	  :/*no output register*/
+ 	  :"r" (src_addr), "r" (dst_addr)
+	  :"$8","$9"
+ 	  );
+}
+
+static  void mips_tc_memcpy_dispatch()
+{
+	struct mips_tc_tx_des_soc *soc_rx_out_src_des_ptr;
+	//struct mips_tc_rx_des_soc *soc_rx_out_dst_des_ptr;
+	u32 *soc_rx_out_dst_des_ptr;
+	struct mips_tc_tx_descriptor *sg_dma_tx_des_ptr;
+	struct mips_tc_rx_descriptor *sg_dma_rx_des_ptr;
+	struct mips_tc_metadata *metadata_ptr;
+	
+	struct mips_tc_tx_des_soc soc_rx_out_src_des;
+	//struct mips_tc_rx_des_soc soc_rx_out_dst_des;
+	u32 proc_frag_cnt = 0;
+	
+	u32 soc_rx_out_src_des_base = 0;
+	//u32 soc_rx_out_dst_des_base = 0;
+	u32 sg_dma_tx_des_base = 0;
+	u32 sg_dma_rx_des_base = 0;
+
+	
+	fw_pp_ctxt->status = 1;
+	
+	soc_rx_out_src_des_base = soc_rx_out_src_ctxt->des_base_addr;
+	//soc_rx_out_dst_des_base = soc_rx_out_dst_ctxt->des_base_addr;
+	sg_dma_tx_des_base = sg_dma_tx_ctxt->des_base_addr;
+	sg_dma_rx_des_base = sg_dma_rx_ctxt->des_base_addr;
+	
+	while(proc_frag_cnt <= loop_cnt)
+	{
+		soc_rx_out_src_des_ptr = (struct mips_tc_tx_des_soc *)(soc_rx_out_src_des_base + soc_rx_out_src_ctxt->rd_idx);
+		//soc_rx_out_dst_des_ptr = (struct mips_tc_rx_des_soc *)(soc_rx_out_dst_des_base + soc_rx_out_dst_ctxt->rd_idx);
+		//soc_rx_out_dst_des_ptr = (u32 *)(soc_rx_out_dst_des_base + soc_rx_out_dst_ctxt->rd_idx);
+		soc_rx_out_dst_des_ptr 	= dma1_tx_data_ptr_base + (soc_rx_out_dst_ctxt->rd_idx/16);
+		sg_dma_tx_des_ptr = (struct mips_tc_tx_descriptor *)(sg_dma_tx_des_base + sg_dma_tx_ctxt->wr_idx);
+		sg_dma_rx_des_ptr = (struct mips_tc_rx_descriptor *)(sg_dma_rx_des_base + sg_dma_rx_ctxt->wr_idx);
+		metadata_ptr = (struct mips_tc_metadata *)(metadata + ((sg_dma_rx_ctxt->wr_idx)/8));
+#if 0	
+		//DMA TX descriptor handling
+		//Read SRC descriptor into local variable
+		soc_rx_out_src_des = *soc_rx_out_src_des_ptr;
+		//soc_rx_out_dst_des = *soc_rx_out_dst_des_ptr;
+#else		
+	    my_copy_2dw((volatile void *)soc_rx_out_src_des_ptr+8, (void *)(&soc_rx_out_src_des)+8);
+#endif	
+		if((soc_rx_out_src_des.dword2_3.own == soc_rx_out_src_ctxt->des_in_own_val) && ((soc_rx_out_src_ctxt->rd_frag_cnt - soc_rx_out_src_ctxt->wr_frag_cnt) < (soc_rx_out_src_ctxt->des_num)))
+		{	
+			if((soc_rx_out_dst_ctxt->rd_pkt_cnt - soc_rx_out_dst_ctxt->wr_pkt_cnt) < (soc_rx_out_dst_ctxt->des_num))
+			{	
+				//Assume number of descriptors in sg_dma_tx is same with sg_dma_rx
+				if((sg_dma_tx_ctxt->wr_frag_cnt - sg_dma_tx_ctxt->rd_frag_cnt) < ((sg_dma_tx_ctxt->des_num) - 3))
+				{
+					if((sg_dma_rx_ctxt->wr_frag_cnt - sg_dma_rx_ctxt->rd_frag_cnt) < ((sg_dma_rx_ctxt->des_num) - 3))
+					{ 
+						//Read one more time, it is possible that own bit is changed before other fields
+						//due to 64-bit bus access, read other fields after confirm the own bit
+						
+						#if DMA_2ND_READ
+						soc_rx_out_src_des = *soc_rx_out_src_des_ptr;
+						soc_rx_out_dst_des = *soc_rx_out_dst_des_ptr;
+						
+						#endif
+						
+						//Need to update DMA RX followed by DMA TX to avoid DMA blocking due to unavailable RX Des
+						mips_tc_memcpy_dma_rx(soc_rx_out_src_des, soc_rx_out_dst_des_ptr, sg_dma_rx_des_ptr, metadata_ptr);
+				
+						mips_tc_memcpy_dma_tx(soc_rx_out_src_des, sg_dma_tx_des_ptr);
+				
+						//Update process fragment count and index
+						proc_frag_cnt ++;
+				
+						soc_rx_out_src_ctxt->rd_idx += rx_out_des_size;
+						if((soc_rx_out_src_ctxt->rd_idx) == (soc_rx_out_src_ctxt->des_num * rx_out_des_size))
+						{
+							soc_rx_out_src_ctxt->rd_idx = 0;
+						}
+				
+						if(soc_rx_out_src_des.dword2_3.eop == 1)
+						{	
+							soc_rx_out_dst_ctxt->rd_idx += rx_out_des_size;
+			
+							if((soc_rx_out_dst_ctxt->rd_idx) == (soc_rx_out_dst_ctxt->des_num * rx_out_des_size))
+							{
+							soc_rx_out_dst_ctxt->rd_idx = 0;
+							}
+						}
+				
+						sg_dma_tx_ctxt->wr_idx += 8;
+						if((sg_dma_tx_ctxt->wr_idx) == (sg_dma_tx_ctxt->des_num * 8))
+						{
+							sg_dma_tx_ctxt->wr_idx = 0;
+						}
+				
+						sg_dma_rx_ctxt->wr_idx += 8;
+						if((sg_dma_rx_ctxt->wr_idx) == (sg_dma_rx_ctxt->des_num * 8))
+						{
+							sg_dma_rx_ctxt->wr_idx = 0;
+						}
+				
+						//mips_tc_test_results(sg_dma_tx_des_ptr,sg_dma_rx_des_ptr );
+				
+				
+					}
+					else
+					{
+						break;
+					}
+				}
+				else
+					break;
+			}
+			else
+			{
+				break;
+			}
+		}
+		else
+		{
+			break;
+		}
+	
+	}
+	
+	fw_pp_ctxt->status = 4;
+}
+
+static inline void mips_tc_soc_rx_out_src_rel(struct mips_tc_tx_des_soc *soc_rx_out_src_des_ptr)
+{	
+	struct mips_tc_tx_des_soc soc_rx_out_src_des;
+	
+	fw_pp_ctxt->status = 6;
+	
+	soc_rx_out_src_des.dword2_3.own = !(soc_rx_out_src_ctxt->des_in_own_val);
+	
+	//Write to DDR
+	*soc_rx_out_src_des_ptr = soc_rx_out_src_des;
+	
+	wmb();
+	
+	soc_rx_out_src_ctxt->wr_frag_cnt++;
+	
+	//soc_rx_out_src_ctxt->wr_byte_cnt += sg_dma_rx_des.data_len;
+	soc_rx_out_src_ctxt->wr_byte_cnt += fw_pp_ctxt->data_len;
+	
+	if(fw_pp_ctxt->eop == 1)
+	{
+		soc_rx_out_src_ctxt->wr_pkt_cnt++;
+	}
+	
+}
+
+static  void mips_tc_soc_rx_out_dst_enq(struct mips_tc_rx_des_soc *soc_rx_out_dst_des_ptr)
+{	
+	struct mips_tc_rx_des_soc soc_rx_out_dst_des;
+	u32 tmp_pkt_data_len = 0;
+	int delay = 0;
+	
+	fw_pp_ctxt->status = 7;
+		
+	if(fw_pp_ctxt->sop == 1)
+	{
+		//tmp_pkt_data_len = sg_dma_rx_des.data_len;
+		tmp_pkt_data_len = fw_pp_ctxt->data_len;
+	}
+	else
+	{
+		//tmp_pkt_data_len = fw_pp_ctxt->pkt_data_len + sg_dma_rx_des.data_len;
+		tmp_pkt_data_len = fw_pp_ctxt->pkt_data_len + fw_pp_ctxt->data_len;
+		
+	}
+	
+	fw_pp_ctxt->pkt_data_len = tmp_pkt_data_len;
+	
+	//Enqueue to soc_rx_out_dst descriptor list for DMATX1 (to switch)
+	if(fw_pp_ctxt->eop == 1)
+	{	soc_rx_out_dst_des.dword2_3.sop = 1;
+		soc_rx_out_dst_des.dword2_3.eop = 1;
+		soc_rx_out_dst_des.dword2_3.data_len = tmp_pkt_data_len;
+		
+		#if DMA_2ND_WRITE
+
+		soc_rx_out_dst_des.dword2_3.own = !(soc_rx_out_dst_ctxt->des_in_own_val);
+		
+		#else
+		
+		soc_rx_out_dst_des.dword2_3.own = soc_rx_out_dst_ctxt->des_in_own_val;
+		
+		#endif
+	
+		//Write to soc_rx_out_dst descriptor list in DDR
+		*soc_rx_out_dst_des_ptr = soc_rx_out_dst_des;
+		//soc_rx_out_dst_des_ptr->dword2_3.own = soc_rx_out_dst_ctxt->des_in_own_val;
+		wmb();
+		
+		for(delay = 0; delay <= 10; delay++)
+		{
+			//add delay for writing the own bit
+		}
+		
+		#if DMA_2ND_WRITE
+		soc_rx_out_dst_des_ptr->dword2_3.own = soc_rx_out_dst_ctxt->des_in_own_val;
+		wmb();
+		
+		#endif
+	}
+	
+	soc_rx_out_dst_ctxt->wr_frag_cnt++;
+	
+	//soc_rx_out_dst_ctxt->wr_byte_cnt += sg_dma_rx_des.data_len;
+	soc_rx_out_dst_ctxt->wr_byte_cnt += fw_pp_ctxt->data_len;
+	
+	if(fw_pp_ctxt->eop == 1)
+	{
+		soc_rx_out_dst_ctxt->wr_pkt_cnt++;
+	}
+	
+}
+
+static inline void mips_tc_dma_tx_rel()
+{
+	fw_pp_ctxt->status = 8;
+	
+	sg_dma_tx_ctxt->rd_frag_cnt++;
+		
+	//sg_dma_tx_ctxt->rd_byte_cnt += sg_dma_rx_des.data_len;
+	sg_dma_tx_ctxt->rd_byte_cnt += fw_pp_ctxt->data_len;
+	
+	if(fw_pp_ctxt->eop == 1)
+	{
+		sg_dma_tx_ctxt->rd_pkt_cnt++;
+	}
+}
+
+static inline void mips_tc_dma_rx_rel()
+{
+	fw_pp_ctxt->status = 9;
+	
+	sg_dma_rx_ctxt->rd_frag_cnt++;
+		
+	//sg_dma_rx_ctxt->rd_byte_cnt += sg_dma_rx_des.data_len;
+	sg_dma_rx_ctxt->rd_byte_cnt += fw_pp_ctxt->data_len;
+	
+	if(fw_pp_ctxt->eop == 1)
+	{
+		sg_dma_rx_ctxt->rd_pkt_cnt++;
+	}
+}
+
+static  void mips_tc_rx_in_umt()
+{
+	//need to define is_bonding; which pcie base address
+	u32 *rx_in_hd_acc_add_reg_ptr;
+	
+	fw_pp_ctxt->status = 10;
+	
+	if(tc_mode_bonding == 1)
+	{
+		rx_in_hd_acc_add_reg_ptr = *rxin_hd_acc_addr;
+		
+		#if 0
+		if(fw_pp_ctxt->rx_in_hd_accum > 16)
+		{
+			*rx_in_hd_acc_add_reg_ptr = 16;
+			*rx_in_hd_acc_add_reg_ptr = (fw_pp_ctxt->rx_in_hd_accum) - 16;
+			fw_pp_ctxt->rx_in_hd_accum = 0;
+		}
+		
+		#endif
+		
+		*rx_in_hd_acc_add_reg_ptr = fw_pp_ctxt->rx_in_hd_accum;
+		fw_pp_ctxt->rx_in_hd_accum = 0;
+		
+	}
+	
+}
+
+
+static  void mips_tc_memcpy_pp()
+{
+	struct mips_tc_tx_des_soc *soc_rx_out_src_des_ptr;
+	struct mips_tc_rx_des_soc *soc_rx_out_dst_des_ptr;
+	//struct mips_tc_tx_descriptor *sg_dma_tx_des_ptr;
+	//struct mips_tc_rx_descriptor *sg_dma_rx_des_ptr;
+	struct mips_tc_metadata *metadata_ptr;
+	struct mips_tc_metadata local_metadata;
+	
+	//struct mips_tc_tx_descriptor sg_dma_tx_des;
+	//struct mips_tc_rx_descriptor sg_dma_rx_des;
+	u32 soc_rx_out_src_des_base = 0;
+	u32 soc_rx_out_dst_des_base = 0;
+	
+	u32 rel_frag_cnt = 0;
+	
+	u32 *dma_cs_reg_ptr;
+	u32 *dma_cdptnrd_reg_ptr;
+	u32 dma_rx_curr_ptr;
+	u32 dma_rx_dist = 0;
+	
+	fw_pp_ctxt->status = 5;
+	
+	soc_rx_out_src_des_base = soc_rx_out_src_ctxt->des_base_addr;
+	soc_rx_out_dst_des_base = soc_rx_out_dst_ctxt->des_base_addr;
+	
+	dma_cs_reg_ptr 		= MIPS_TC_DMA0_CS_REG;
+	dma_cdptnrd_reg_ptr = MIPS_TC_DMA_CDPTNRD_REG;
+	
+	
+	while(rel_frag_cnt <= loop_cnt)
+	{
+		soc_rx_out_src_des_ptr = (struct mips_tc_tx_des_soc *)(soc_rx_out_src_des_base + soc_rx_out_src_ctxt->wr_idx);
+		soc_rx_out_dst_des_ptr = (struct mips_tc_rx_des_soc *)(soc_rx_out_dst_des_base + soc_rx_out_dst_ctxt->wr_idx);
+		//sg_dma_tx_des_ptr = (struct mips_tc_tx_descriptor *)(sg_dma_tx_ctxt->des_base_addr + sg_dma_tx_ctxt->rd_idx);
+		//sg_dma_rx_des_ptr = (struct mips_tc_rx_descriptor *)(sg_dma_rx_ctxt->des_base_addr + sg_dma_rx_ctxt->rd_idx);
+		metadata_ptr = (struct mips_tc_metadata *)(metadata + ((sg_dma_rx_ctxt->rd_idx)/8));
+		
+		local_metadata = *metadata_ptr;
+		
+		if(dma_rx_dist < 8)
+		{
+			*dma_cs_reg_ptr = 12;
+			dma_rx_curr_ptr = ((*dma_cdptnrd_reg_ptr) << 1) & 0x7FF;
+			dma_rx_dist = mips_tc_cal_des_dist(dma_rx_curr_ptr, sg_dma_rx_ctxt->rd_idx, sg_dma_rx_ctxt->des_num * 8);
+		}
+		
+	
+		if(sg_dma_tx_ctxt->rd_frag_cnt != sg_dma_tx_ctxt->wr_frag_cnt)
+		{
+			//if((dma_tx_curr_ptr != fw_pp_ctxt->dma_tx_ptr) && (dma_rx_curr_ptr != fw_pp_ctxt->dma_rx_ptr))
+			if((dma_rx_dist >= 8))
+			{
+				//Read one more time, it is possible that own bit is changed before other fields
+				//due to 64-bit bus access
+				
+				#if DMA_2ND_READ
+				sg_dma_tx_des = *sg_dma_tx_des_ptr;
+				sg_dma_rx_des = *sg_dma_rx_des_ptr;
+				
+				#endif
+				
+				//Update the dma pointers in the fw_pp_ctxt for next use
+				fw_pp_ctxt->prev_dma_rx_ptr = dma_rx_curr_ptr;
+				dma_rx_dist -= 8;
+				
+				fw_pp_ctxt->sop = local_metadata.sop_frag;
+				fw_pp_ctxt->eop = local_metadata.eop_frag;
+
+				
+				//Workaround memcpy channel RX data_len zero issue
+				//fw_pp_ctxt->data_len = sg_dma_rx_des.data_len;
+				fw_pp_ctxt->data_len = local_metadata.data_len;
+				
+				//Avoid accessing dma descriptor to reduce cpu load
+				fw_pp_ctxt->byte_off = local_metadata.byte_off;
+				fw_pp_ctxt->dst_data_ptr = local_metadata.dst_data_ptr;
+				fw_pp_ctxt->src_data_ptr = local_metadata.src_data_ptr;
+				
+				#if 0
+				//Debug code to detect DMA split error
+				if(sg_dma_rx_des.sop == 1 && sg_dma_rx_des.eop == 0)
+				{
+					(fw_pp_ctxt->split_error) ++;
+				}
+				
+				#endif
+	
+				mips_tc_soc_rx_out_src_rel(soc_rx_out_src_des_ptr);
+				mips_tc_dma_tx_rel();
+				mips_tc_dma_rx_rel();
+				mips_tc_soc_rx_out_dst_enq(soc_rx_out_dst_des_ptr);
+			
+				rel_frag_cnt ++;
+				
+				//Update pointers
+				
+				soc_rx_out_src_ctxt->wr_idx += rx_out_des_size;
+				
+				if((soc_rx_out_src_ctxt->wr_idx) == (soc_rx_out_src_ctxt->des_num * rx_out_des_size))
+				{
+					soc_rx_out_src_ctxt->wr_idx = 0;
+				}
+				
+				if(fw_pp_ctxt->eop == 1)
+				{	
+					soc_rx_out_dst_ctxt->wr_idx += rx_out_des_size;
+			
+					if((soc_rx_out_dst_ctxt->wr_idx) == (soc_rx_out_dst_ctxt->des_num * rx_out_des_size))
+					{
+						soc_rx_out_dst_ctxt->wr_idx = 0;
+					}
+				}
+				
+				sg_dma_tx_ctxt->rd_idx += 8;
+				
+				if((sg_dma_tx_ctxt->rd_idx) == (sg_dma_tx_ctxt->des_num * 8))
+				{
+					sg_dma_tx_ctxt->rd_idx = 0;
+				}
+				
+				sg_dma_rx_ctxt->rd_idx += 8;
+				if((sg_dma_rx_ctxt->rd_idx) == (sg_dma_rx_ctxt->des_num * 8))
+				{
+					sg_dma_rx_ctxt->rd_idx = 0;
+				}
+				
+			}
+			else
+			{
+				break;
+			}
+		}
+		else
+		{
+			break;
+		}
+	
+	
+	}
+	
+	if(rel_frag_cnt)
+	{	
+		fw_pp_ctxt->rx_in_hd_accum += rel_frag_cnt;
+		mips_tc_rx_in_umt();
+	}
+	
+	fw_pp_ctxt->status = 11;
+	
+}
+
+int __init tc_sg_init (void)
+{
+
+	/* create kernel thread to print debug message */
+	dbg_task = kthread_create(kernel_debug_thread, NULL, "debug_task");
+}
+early_initcall(tc_sg_init);
+
+#ifdef USE_CACHED_MECHANISM
+#define CACHED_LINE_SIZE 32
+#define CACHED_SIZE 1024
+#define mips_tc_round32(x) ((x+31) & ~31)
+extern void mips_dcache_flush(u32 size,u32 cache_size,u32 start_addr);
+extern void mips_dcache_invalidate(u32 size,u32 cache_size,u32 start_addr);
+extern void mips_l2cache_flush(u32 size, u32 cache_size,u32 start_addr);
+extern void mips_l2cache_invalidate(u32 size,u32 cache_size,u32 start_addr);
+static void align_addr_size(u32 *start_addr, u32 *size)
+{
+	u32 offset = 0;
+	offset = (*start_addr) % CACHED_LINE_SIZE;
+
+	if (offset) {
+		*start_addr -= offset;
+		*size += offset;
+	}
+
+	*size = mips_tc_round32(*size);
+
+	return;
+}
+
+int cpu_dcache_flush(u32 start_addr, u32 size)
+{
+	if (size > CACHED_SIZE)
+		return (0);
+
+	mips_dcache_flush(size, CACHED_LINE_SIZE, start_addr);
+
+	return (1);
+}
+
+int cpu_l2cache_flush(u32 start_addr, u32 size)
+{
+	if (size > CACHED_SIZE)
+		return (0);
+
+	mips_l2cache_flush(size, CACHED_LINE_SIZE, start_addr);
+
+	return (1);
+}
+
+int cpu_dcache_invalidate(u32 start_addr, u32 size)
+{
+	if (size > CACHED_SIZE)
+		return (0);
+
+	mips_dcache_invalidate(size, CACHED_LINE_SIZE, start_addr);
+
+	return (1);
+}
+
+int cpu_l2cache_invalidate(u32 start_addr, u32 size)
+{
+	if (size > CACHED_SIZE)
+		return (0);
+
+	mips_l2cache_invalidate(size, CACHED_LINE_SIZE, start_addr);
+
+	return (1);
+}
+
+
+int mips_tc_cache_flush(u32 start_addr, u32 size)
+{
+	u32 addr = start_addr;
+
+	align_addr_size(&addr, &size);
+
+	cpu_dcache_flush(addr, size);
+
+	cpu_l2cache_flush(addr, size);
+	return 0;
+}
+
+int mips_tc_cache_invalidate(u32 start_addr, u32 size)
+{
+	u32 addr = start_addr;
+
+	align_addr_size(&addr, &size);
+
+	cpu_dcache_invalidate(addr, size);
+
+	cpu_l2cache_invalidate(addr, size);
+	return 0;
+}
+#endif /* USE_CACHED_MECHANISM */
diff --git a/drivers/dma/mips_tc_sg.h b/drivers/dma/mips_tc_sg.h
new file mode 100644
--- /dev/null
+++ b/drivers/dma/mips_tc_sg.h
@@ -0,0 +1,40 @@
+/*******************************************************************************
+
+  Intel MIPS TC driver
+  Copyright(c) 2017 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify it
+  under the terms and conditions of the GNU General Public License,
+  version 2, as published by the Free Software Foundation.
+
+  This program is distributed in the hope it will be useful, but WITHOUT
+  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+  more details.
+
+  You should have received a copy of the GNU General Public License along with
+  this program; if not, write to the Free Software Foundation, Inc.,
+  51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
+
+  The full GNU General Public License is included in this distribution in
+  the file called "COPYING".
+
+*******************************************************************************/
+#ifndef __MIPS_TC_SG__
+#define __MIPS_TC_SG__
+
+#define SUPPORT_DSL_BONDING_FEATURE 1
+#ifdef SUPPORT_DSL_BONDING_FEATURE
+#define RUN_MIPS_TC 1
+/* #define USE_CACHED_MECHANISM 1 */
+#define rx_out_des_size		16
+extern void mips_tc_init(u32 *);
+extern int mips_tc_sg(struct mips_tc_sg_mem *shared_vir_mem);
+
+#if USE_CACHED_MECHANISM
+int mips_tc_cache_flush(u32 start_addr, u32 size);
+int mips_tc_cache_invalidate(u32 start_addr, u32 size);
+#endif /* USE_CACHED_MECHANISM */
+
+#endif
+#endif /* __MIPS_TC_SG__ */
diff --git a/drivers/dma/mips_tc_sg_s.S b/drivers/dma/mips_tc_sg_s.S
new file mode 100644
--- /dev/null
+++ b/drivers/dma/mips_tc_sg_s.S
@@ -0,0 +1,155 @@
+#include <asm/asm.h>
+#include <asm/asmmacro.h>
+#include <asm/regdef.h>
+#include <asm/mipsregs.h>
+#include <asm/stackframe.h>
+#include <asm/addrspace.h>
+#include <asm/cacheops.h>
+
+
+/* cache operations */
+#define CACHE_OPCODE( code, type )              ( ((code) << 2) | (type) )
+
+#define ICACHE_INDEX_INVALIDATE                 CACHE_OPCODE(0x0, 0)
+#define ICACHE_INDEX_LOAD_TAG                   CACHE_OPCODE(0x1, 0)
+#define ICACHE_INDEX_STORE_TAG                  CACHE_OPCODE(0x2, 0)
+#define DCACHE_INDEX_WRITEBACK_INVALIDATE       CACHE_OPCODE(0x0, 1)
+#define DCACHE_INDEX_LOAD_TAG                   CACHE_OPCODE(0x1, 1)
+#define DCACHE_INDEX_STORE_TAG                  CACHE_OPCODE(0x2, 1)
+#define SCACHE_INDEX_WRITEBACK_INVALIDATE       CACHE_OPCODE(0x0, 3)
+#define SCACHE_INDEX_STORE_TAG                  CACHE_OPCODE(0x2, 3)
+
+#define ICACHE_ADDR_HIT_INVALIDATE              CACHE_OPCODE(0x4, 0)
+#define ICACHE_ADDR_FILL                        CACHE_OPCODE(0x5, 0)
+#define ICACHE_ADDR_FETCH_LOCK                  CACHE_OPCODE(0x7, 0)
+#define DCACHE_ADDR_HIT_INVALIDATE              CACHE_OPCODE(0x4, 1)
+#define DCACHE_ADDR_HIT_WRITEBACK_INVALIDATE    CACHE_OPCODE(0x5, 1)
+#define DCACHE_ADDR_HIT_WRITEBACK               CACHE_OPCODE(0x6, 1)
+#define DCACHE_ADDR_FETCH_LOCK                  CACHE_OPCODE(0x7, 1)
+
+
+#define L2CACHE_ADDR_HIT_WRITEBACK_INVALIDATE   CACHE_OPCODE(0x5, 3)
+#define L2CACHE_ADDR_HIT_INVALIDATE             CACHE_OPCODE(0x4, 3)
+#define SCACHE_ADDR_HIT_WRITEBACK               CACHE_OPCODE(0x6, 3)
+
+
+.set noreorder   // no optimisation by re ordering assembly code
+.option pic0
+
+	/************************************************************************
+     *  Invalidate L$2 cache -> writeback & Invalidate base on address
+     ************************************************************************/
+    /* Routine for invalidating L2 cache */
+    /* Usage
+       mips_l2cache_invalidate(u32 lock_size, u32 cache_line_size,  u32 start_address)
+       a0 : lock size
+       a1 : cache line size
+       a2 : start address
+     */
+.globl   mips_l2cache_invalidate
+.ent mips_l2cache_invalidate
+.align 4
+    mips_l2cache_invalidate:
+    /* Calc an address that will correspond to the last cache line  */
+    addu    a3, a2, a0
+    subu    a3, a1
+
+    /* Loop through all lines, invalidating each of them */
+1:
+    cache   L2CACHE_ADDR_HIT_INVALIDATE, 0(a2)
+    bne     a2, a3, 1b
+    addu    a2, a1
+
+9:
+    jr  ra
+    nop
+.end   mips_l2cache_invalidate
+
+	/************************************************************************
+     *  flush l2 cache -> writeback & Invalidate base on index
+     ************************************************************************/
+    /* Routine for flushing L2 cache */
+    /* Usage
+       mips_l2cache_flush(u32 lock_size, u32 cache_line_size,  u32 start_address)
+       a0 : lock size
+       a1 : cache line size
+       a2 : start address
+     */
+.globl   mips_l2cache_flush
+.ent mips_l2cache_flush
+.align 4
+mips_l2cache_flush:
+    /* Calc an address that will correspond to the last cache line  */
+    addu    a3, a2, a0
+    subu    a3, a1
+
+    /* Loop through all lines, invalidating each of them */
+1:
+    cache   L2CACHE_ADDR_HIT_WRITEBACK_INVALIDATE, 0(a2)
+    bne     a2, a3, 1b
+    addu    a2, a1
+
+9:
+    jr  ra
+    nop
+.end   mips_l2cache_flush
+
+
+    /************************************************************************
+     *  flush dcache -> writeback & Invalidate base on index
+     ************************************************************************/
+    /* Routine for flushing Dcache */
+    /* Usage
+       mips_dcache_flush(u32 lock_size, u32 cache_line_size,  u32 start_address)
+       a0 : lock size
+       a1 : cache line size
+       a2 : start address
+     */
+.globl   mips_dcache_flush
+.ent mips_dcache_flush
+.align 4
+mips_dcache_flush:
+    /* Calc an address that will correspond to the last cache line  */
+    addu    a3, a2, a0
+    subu    a3, a1
+
+    /* Loop through all lines, invalidating each of them */
+1:
+    cache   DCACHE_ADDR_HIT_WRITEBACK_INVALIDATE, 0(a2)
+    bne     a2, a3, 1b
+    addu    a2, a1
+
+9:
+    jr  ra
+    nop
+.end   mips_dcache_flush
+
+    /************************************************************************
+     *  Invalidate dcache -> writeback & Invalidate base on address
+     ************************************************************************/
+    /* Routine for flushing Dcache */
+    /* Usage
+       mips_dcache_flush(u32 lock_size, u32 cache_line_size,  u32 start_address)
+       a0 : lock size
+       a1 : cache line size
+       a2 : start address
+     */
+.globl   mips_dcache_invalidate
+.ent mips_dcache_invalidate
+.align 4
+mips_dcache_invalidate:
+    /* Calc an address that will correspond to the last cache line  */
+    addu    a3, a2, a0
+    subu    a3, a1
+
+    /* Loop through all lines, invalidating each of them */
+1:
+    cache   DCACHE_ADDR_HIT_INVALIDATE, 0(a2)
+    bne     a2, a3, 1b
+    addu    a2, a1
+
+9:
+    jr  ra
+    nop
+.end   mips_dcache_invalidate
+
diff --git a/include/linux/ltq_hwmcpy.h b/include/linux/ltq_hwmcpy.h
new file mode 100755
--- /dev/null
+++ b/include/linux/ltq_hwmcpy.h
@@ -0,0 +1,392 @@
+/******************************************************************************
+ *
+ *                        Copyright (c) 2012, 2014, 2015
+ *                           Lantiq Deutschland GmbH
+ *
+ *  For licensing information, see the file 'LICENSE' in the root folder of
+ *  this software module.
+ *  
+ ******************************************************************************/
+
+#ifndef __LTQ_HWMCPY_H__
+#define __LTQ_HWMCPY_H__
+
+enum {
+	HWMCPY_F_PRIO_LOW	= 0x0,
+	HWMCPY_F_PRIO_HIGH	= 0x1,
+	HWMCPY_F_IPC		= 0x2,
+	HWMCPY_F_CHKSZ_1	= 0x4,
+	HWMCPY_F_CHKSZ_2	= 0x8,
+	HWMCPY_F_CHKSZ_3	= 0xC,
+	HWMCPY_F_CHKSZ_4	= 0x10,
+/*Trunk size support from 0 to 0x7(512B,1KB to 64KB),
+ start from bit 2, len 3 bits, max 0x1C */
+	HWMCPY_F_RESERVED	= 0x20,
+	HWMCPY_F_CHKSZ_SET	= 0x40,
+	HWMCPY_F_LAST		= 0x80,
+
+};
+
+enum mcpy_type {
+	MCPY_PHY_TO_PHY = 0,
+	MCPY_PHY_TO_IOCU,
+	MCPY_IOCU_TO_PHY,
+	MCPY_IOCU_TO_IOCU,
+	MCPY_SW_CPY,
+};
+
+struct mcpy_frag {
+	void *ptr;
+	u16 offset;
+	u16 size;
+};
+
+enum umt_mode {
+	UMT_SELFCNT_MODE = 0,
+	UMT_USER_MODE    = 1,
+	UMT_MODE_MAX,
+};
+enum umt_msg_mode {
+	UMT_NO_MSG    = 0,
+	UMT_MSG0_ONLY = 1,
+	UMT_MSG1_ONLY = 2,
+	UMT_MSG0_MSG1 = 3,
+	UMT_MSG_MAX,
+};
+
+enum umt_status {
+	UMT_DISABLE = 0,
+	UMT_ENABLE  = 1,
+	UMT_STATUS_MAX,
+	UMT_BROKEN,
+};
+
+extern void *ltq_hwmemcpy(void *dst, const void *src, u32 len,
+				u32 portid, enum mcpy_type mode, u32 flags);
+extern int ltq_hwmcpy_sg(void *dst, const struct mcpy_frag *src, u32 frag_num,
+				u32 portid, enum mcpy_type mode, u32 flags);
+extern int ltq_mcpy_reserve(void);
+extern void ltq_mcpy_release(u32);
+
+#ifdef CONFIG_LTQ_UMT_LEGACY_MODE
+extern int ltq_umt_enable(u32 enable);
+extern int ltq_umt_set_mode(u32 umt_mode, u32 msg_mode,
+				u32 phy_dst, u32 period, u32 enable);
+extern int ltq_umt_set_period(u32 period);
+#else
+int ltq_umt_set_period(u32 umt_id, u32 ep_id, u32 period);
+int ltq_umt_set_mode(u32 umt_id, u32 ep_id, u32 umt_mode, u32 msg_mode,
+			u32 phy_dst, u32 period, u32 enable);
+int ltq_umt_enable(u32 umt_id, u32 ep_id, u32 enable);
+int ltq_umt_request(u32 ep_id, u32 cbm_pid,
+		u32 *dma_ctrlid, u32 *dma_cid, u32 *umt_id);
+int ltq_umt_release(u32 umt_id, u32 ep_id);
+int ltq_umt_suspend(u32 umt_id, u32 ep_id, u32 enable);
+#ifdef CONFIG_LTQ_UMT_SW_MODE
+int umt_reset_port_dq_idx(u32 cbm_id);
+#endif
+
+#endif
+
+#ifdef CONFIG_LTQ_UMT_518_FW_SG
+#define MIPS_TC_DMA0_CS_REG 0xb6e00018
+#define MIPS_TC_DMA_CDPTNRD_REG 0xb6e00034
+#define MIPS_TC_OPTIMIZE 1
+
+#ifdef  MIPS_TC_OPTIMIZE
+struct mips_tc_q_cfg_ctxt {
+		u32 des_in_own_val;
+		u32 _res0;
+		u32 des_num;
+		
+		u32 des_base_addr;
+
+		u32 rd_idx;
+		u32 wr_idx;
+
+		u32 _dw_res0;
+		
+		u32 rd_frag_cnt;
+		
+		u32 _dw_res1;
+
+		u32 wr_frag_cnt;
+		
+		u32 _dw_res2;
+
+		u32 rd_pkt_cnt;
+
+		u32 rd_byte_cnt;
+
+		u32 wr_pkt_cnt;
+
+		u32 wr_byte_cnt;
+
+};
+
+struct mips_tc_fw_memcpy_ctxt_t {
+		u32 _res0;
+		u32 sop;
+		u32 eop;
+		u32 byte_off;
+		u32 src_data_ptr;
+		u32 dst_data_ptr;
+		u32 data_len;
+
+};
+
+struct mips_tc_fw_pp_ctxt_t {
+		u32 split_error;
+		u32 status;
+		u32 rx_in_hd_accum;
+		u32 _res0;
+		u32 sop;
+		u32 eop;
+		u32 byte_off;
+		u32 _res1;
+		u32 src_data_ptr;
+		u32 dst_data_ptr;
+		u32 data_len;
+		u32 pkt_data_len;
+		u32 prev_dma_tx_ptr;
+		u32 prev_dma_rx_ptr;
+		u32 _res2;
+		u32 _res3;
+
+};
+
+
+struct mips_tc_metadata {
+		u32 sop_frag;
+		u32 eop_frag;
+		u32 _res0;
+		u32 _res1;
+		u32 byte_off;
+		u32 data_len;
+		u32 src_data_ptr;
+		u32 dst_data_ptr;
+
+};
+
+struct mips_tc_rxout_dst_cache_ctxt {
+		u32 data_ptr;
+};
+
+
+
+#else /* MIPS_TC_OPTIMIZE */
+/* Define DMA descriptors */
+struct mips_tc_q_cfg_ctxt {
+		u32 des_in_own_val:1;
+		u32 _res0:15;
+		u32 des_num:16;
+		
+		u32 des_base_addr;
+
+		u32 rd_idx:16;
+		u32 wr_idx:16;
+
+		u32 _dw_res0;
+		
+		u32 rd_frag_cnt;
+		
+		u32 _dw_res1;
+
+		u32 wr_frag_cnt;
+		
+		u32 _dw_res2;
+
+		u32 rd_pkt_cnt;
+
+		u32 rd_byte_cnt;
+
+		u32 wr_pkt_cnt;
+
+		u32 wr_byte_cnt;
+
+};
+
+struct mips_tc_fw_memcpy_ctxt_t {
+		u32 _res0:30;
+		u32 sop:1;
+		u32 eop:1;
+		u32 byte_off;
+		u32 src_data_ptr;
+		u32 dst_data_ptr;
+		u32 data_len;
+
+};
+
+struct mips_tc_fw_pp_ctxt_t {
+		u32 split_error:12;
+		u32 status:4;
+		u32 rx_in_hd_accum:12;
+		u32 _res0:2;
+		u32 sop:1;
+		u32 eop:1;
+		u32 byte_off;
+		u32 src_data_ptr;
+		u32 dst_data_ptr;
+		u32 data_len;
+		u32 pkt_data_len;
+
+};
+
+struct mips_tc_metadata{
+	
+		u32 sop_frag:1;
+		u32 eop_frag:1;
+		u32 _res0:14;
+		u32 data_len:16;
+
+};
+
+struct mips_tc_rxout_dst_cache_ctxt {
+		u32 data_ptr;
+};
+
+
+
+#endif /* MIPS_TC_OPTIMIZE */
+
+
+struct mips_tc_tx_descriptor{
+
+		u32 own:1;
+		u32 c:1;
+		u32 sop:1;
+		u32 eop:1;
+		u32 dic:1;
+		u32 pdu_type:1;
+		u32 byte_off:3;
+		u32 qos:4;
+		u32 mpoa_pt:1;
+		//u32 mpoa_type:2;
+		//u32 sop_frag:1;
+		//u32 eop_frag:1;
+
+		u32 sop_frag:1;
+		u32 eop_frag:1;
+		u32 data_len:16;
+		u32 data_ptr:32;
+};
+
+struct mips_tc_rx_descriptor{
+
+		u32 own:1;
+		u32 c:1;
+		u32 sop:1;
+		u32 eop:1;
+		u32 dic:1;
+		u32 pdu_type:1;
+		u32 byte_off:3;
+		u32 qos:4;
+		u32 mpoa_pt:1;
+		//u32 mpoa_type:2;
+		u32 sop_frag:1;
+		u32 eop_frag:1;
+		u32 data_len:16;
+		
+		u32 data_ptr:32;
+
+};
+
+
+/* Section 1: Define MIPS TC control & debug structure */
+#define MIPS_TC_DEBUG_MSG_LEN 256
+#define MIPS_TC_DEBUG_MSG_ARRAY_LEN 4
+struct mips_tc_msg_param {
+	u32 flags;
+	char message[MIPS_TC_DEBUG_MSG_LEN];
+};
+enum {
+	MIPS_TC_C_STOP	= 0x0,
+	MIPS_TC_C_RUN	= 0x1,
+};
+
+struct mips_tc_ctrl_dbg {
+	u32 mips_tc_control;
+	u32 dbg_flags;
+	u32 ver_major:8;
+	u32 ver_mid:8;
+	u32 ver_minor:8;
+	u32 ver_reserved:8;
+	u32 priv_data_vir;
+	u32 priv_data_phy;
+	u32 priv_data_len;
+	u32 ctrl_max_process_pkt;
+	u32 reserved[16];
+	u32 msg_cur_idx;
+	struct mips_tc_msg_param msgs[MIPS_TC_DEBUG_MSG_ARRAY_LEN];
+};
+
+struct mips_tc_tx_des_soc{
+
+		u32 dword0;
+		
+		u32 dword1;
+		
+		struct mips_tc_tx_descriptor dword2_3;
+
+};
+
+struct mips_tc_rx_des_soc{
+
+		u32 dword0;
+		
+		u32 dword1;
+		
+		struct mips_tc_rx_descriptor dword2_3;
+
+}; 
+
+/* Section 2: Memory configuration */
+struct mips_tc_mem_conf {
+	/* Sec1: memory configure context */
+	struct mips_tc_q_cfg_ctxt soc_rx_out_src_ctxt;
+	struct mips_tc_q_cfg_ctxt soc_sg_dma_tx_ctxt;
+	struct mips_tc_q_cfg_ctxt soc_sg_dma_rx_ctxt;
+	struct mips_tc_q_cfg_ctxt soc_rx_out_dst_ctxt;
+	/* Sec2: Current context */
+	struct mips_tc_fw_memcpy_ctxt_t fw_memcpy_ctxt;
+	struct mips_tc_fw_pp_ctxt_t fw_pp_ctxt;
+	/* Sec3: control variable */
+	u32 aca_hw_rxin_hd_acc_addr;
+	u32 tc_mode_bonding;
+	u32 reserved[16];
+	struct mips_tc_rxout_dst_cache_ctxt *cache_rxout_ptr;
+};
+/* Section 3: MIPS TC Info */
+enum {
+	MIPS_TC_S_ERROR	= -1,
+	MIPS_TC_S_IDLE	= 0x0,
+	MIPS_TC_S_RUNNING = 0x1,
+	MIPS_TC_S_BUSY = 0x2,
+};
+struct mips_tc_mib {
+	u32 jobs;
+	u32 reserved[32];
+};
+struct mips_tc_info {
+	u32 state;
+	struct mips_tc_mib mib;
+};
+/* Define shared memory: note that new field
+* should be added to the tail of the structure.
+* The layout:
+*   - Control and debug structure ( 518 <-> SoC): mips_tc_ctrl_dbg
+*   - Memory configuration(518 -> SoC): mips_tc_mem_conf
+* 		+ Memory configure context.
+*	- MIPS TC information (SoC -> 518): mips_tc_info
+*		+ State machine, mib
+*	- Others
+*/
+struct mips_tc_sg_mem {
+	struct mips_tc_ctrl_dbg ctrl;
+	struct mips_tc_mem_conf conf;
+	struct mips_tc_info mib;
+};
+extern int mips_tc_get_shared_mem(struct mips_tc_sg_mem **shared_mem);
+#endif /* CONFIG_LTQ_UMT_518_FW_SG */
+
+#endif /* __LTQ_HWMCPY_H__ */
