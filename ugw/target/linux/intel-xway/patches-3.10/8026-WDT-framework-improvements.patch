# HG changeset patch
# Parent 45a04852fe3e38ed30ca0a0a364c688a57a9ec54

diff --git a/drivers/watchdog/Kconfig b/drivers/watchdog/Kconfig
--- a/drivers/watchdog/Kconfig
+++ b/drivers/watchdog/Kconfig
@@ -45,6 +45,12 @@ config WATCHDOG_NOWAYOUT
 	  get killed. If you say Y here, the watchdog cannot be stopped once
 	  it has been started.
 
+config WDT_SUPPORT_WORKQUEUE
+	bool "support watchdog for larger timeouts using work queue when HW timeout is less"
+	help
+		support watchdog for larger timeouts using work queue when HW timeout is less
+		say Y if you want to use WQ else say N. 
+
 #
 # General Watchdog drivers
 #
@@ -1117,6 +1123,7 @@ config GRX500_IAP_WDT
         tristate "GRX500 SoC watchdog for InterAptiv core"
         depends on SOC_GRX500
 	select WATCHDOG_CORE
+	select WDT_SUPPORT_WORKQUEUE
         help
           Hardware driver for the GRX500 SoC Watchdog Timer for
 	  InterAptiv core.
diff --git a/drivers/watchdog/grx500_wdt.c b/drivers/watchdog/grx500_wdt.c
--- a/drivers/watchdog/grx500_wdt.c
+++ b/drivers/watchdog/grx500_wdt.c
@@ -65,14 +65,43 @@ static inline void Enable_WDT_intr(void*
 	enable_percpu_irq((*(u32 *)wdtirq), 0);
 }
 
+static int grx500wdt_start_other(struct watchdog_device *wdt_dev)
+{
+	uint32_t config0;
+	GICWRITE(GIC_REG(VPE_LOCAL, GIC_VPE_OTHER_ADDR), wdt_dev->id);
+	wmb();	
+	GICREAD(GIC_REG(VPE_OTHER, GIC_VPE_WD_CONFIG0), config0);
+	rmb();
+	GICWRITE(GIC_REG(VPE_OTHER, GIC_VPE_WD_CONFIG0), (config0 | WD_START));
+  wmb();
+
+	return 0;
+}
+
+static int grx500wdt_stop_other(struct watchdog_device *wdt_dev)
+{
+	uint32_t config0;
+	GICWRITE(GIC_REG(VPE_LOCAL, GIC_VPE_OTHER_ADDR), wdt_dev->id);
+	wmb();
+	GICREAD(GIC_REG(VPE_OTHER, GIC_VPE_WD_CONFIG0), config0);
+	rmb();
+	GICWRITE(GIC_REG(VPE_OTHER, GIC_VPE_WD_CONFIG0), (config0 & ~WD_START));
+	wmb();
+
+	return 0;
+}
+
 static int grx500wdt_start(struct watchdog_device *wdt_dev)
 {
 	uint32_t config0;
-
+	if(smp_processor_id() != wdt_dev->id){
+		printk(KERN_ERR "cant start on cpu [%d] rather than cpu [%d]\n", smp_processor_id(), wdt_dev->id);
+		panic("Please set affinity of this process to cpu %d\n",wdt_dev->id);
+	}
 	GICREAD(GIC_REG(VPE_LOCAL, GIC_VPE_WD_CONFIG0), config0);
 	rmb();
 	GICWRITE(GIC_REG(VPE_LOCAL, GIC_VPE_WD_CONFIG0), (config0 | WD_START));
-        wmb();
+  wmb();
 
 	return 0;
 }
@@ -91,18 +120,22 @@ static int grx500wdt_stop(struct watchdo
 static int grx500wdt_set_timeout(struct watchdog_device *wdt_dev,
 			       unsigned int new_timeout)
 {
-	struct watchdog_device *grx500_wdt;
-	grx500_wdt = &per_cpu(grx500wdt, smp_processor_id());
+	int timeout_cal;
 
-	grx500_wdt->timeout = new_timeout;
-	printk("%s: timeout = %d, cpu = %d, id = %d PERCPUID = %d\n", __func__, new_timeout, smp_processor_id(), wdt_dev->id, grx500_wdt->id);
+	if(wdt_dev->id != smp_processor_id()){
+		printk(KERN_ERR "cant set timeout on cpu [%d] rather than cpu [%d]\n", smp_processor_id(), wdt_dev->id);
+		panic("Please set affinity of this process to cpu %d\n",wdt_dev->id);
+	}
+	wdt_dev->timeout = new_timeout;
+	timeout_cal = (new_timeout > wdt_dev->max_timeout)?wdt_dev->max_timeout:wdt_dev->timeout;
+	printk("%s: timeout = %d, cpu = %d, id = %d \n", __func__, new_timeout, smp_processor_id(), wdt_dev->id);
 
-	grx500wdt_stop(grx500_wdt);
+	grx500wdt_stop(wdt_dev);
 
-	GICWRITE(GIC_REG(VPE_LOCAL, GIC_VPE_WD_INITIAL0), (cpu_clk * grx500_wdt->timeout));
+	GICWRITE(GIC_REG(VPE_LOCAL, GIC_VPE_WD_INITIAL0), (cpu_clk * timeout_cal));
 	wmb();
 
-	grx500wdt_start(grx500_wdt);
+	grx500wdt_start(wdt_dev);
 
 	return 0;
 }
@@ -122,25 +155,39 @@ static uint32_t grx500wdt_get_timeleft(s
 
 static int grx500wdt_ping(struct watchdog_device *wdt_dev)
 {
-        struct watchdog_device *grx500_wdt;
-        grx500_wdt = &per_cpu(grx500wdt, smp_processor_id());
-
-        grx500wdt_stop(grx500_wdt);
-        grx500wdt_start(grx500_wdt);
-
-        /* grx500wdt_get_timeleft(grx500_wdt); */
-
-        return 0;
+	/*ping from anothe cpu is allowed only in case if its coming from wq,
+		for user space call detect it and throw panic
+		For wq it can get scheduled on any CPU and its not a good idea to set affinity
+		for WQ as it looses the granularity of increasing the timeout when the 
+		CPU is really stuck*/
+	if(smp_processor_id() != wdt_dev->id){
+		if(wdt_dev->wd_data->user_call){
+			printk(KERN_ERR "User space ping received from cpu[%d] but  expected from cpu [%d] \n",
+							smp_processor_id(),wdt_dev->id);
+			panic("Please set affinity of this process to cpu %d\n",wdt_dev->id);
+		}else{/*has to work upon another cpu in case of wq from another cpu*/
+   		grx500wdt_stop_other(wdt_dev);
+  		grx500wdt_start_other(wdt_dev);
+		}
+	}else{/*same cpu*/
+   	grx500wdt_stop(wdt_dev);
+  	grx500wdt_start(wdt_dev);
+	}
+	/*reset the user space call flag irrespective of WQ/user call*/
+	wdt_dev->wd_data->user_call = 0;
+	return 0;
 }
 
 static irqreturn_t grx500wdt_irq(int irqno, void *param)
 {
 	struct watchdog_device *grx500_wdt;
 	grx500_wdt = &per_cpu(grx500wdt, smp_processor_id());
+	/*stop and start the timer to have longer duration after pre warning ISR*/	
+	grx500wdt_stop(grx500_wdt);
+	WARN_ONCE(1, " IRQ %d triggered as WDT%d Timer Overflow on CPU %d !!!.. \n", irqno, grx500_wdt->id, smp_processor_id());
+	grx500wdt_start(grx500_wdt);
 
-	WARN_ONCE(1, " IRQ %d triggered as WDT%d Timer Overflow on CPU %d !!!.. \n", irqno, grx500_wdt->id, smp_processor_id());
-
-        return IRQ_HANDLED;
+  return IRQ_HANDLED;
 }
 
 struct irqaction grx500wdt_irqaction = {
@@ -207,6 +254,15 @@ static int grx500wdt_probe(struct platfo
 		grx500_wdt->id = cpu;
 		grx500_wdt->info = &grx500wdt_info;
 		grx500_wdt->ops = &grx500wdt_ops; 
+		/*we dont want kernel to override this based on no of cpus present
+			as we want to detect the cpu from the id rather than other means
+			setting this bit will ensure ID is not changed
+			w/o this the behaviour is 
+			core0 Linux - WDT0 
+			core1 legacy- ?
+			core2 Linux -WDT1 instead of WDT2, as kernel is allocating in sequence
+			this check will avoid that */
+		set_bit(WDOG_ID_NOT_REQ, &grx500_wdt->status);
 	}
 	
 	/* Enable WDT reset to RCU for VPEx */  
@@ -224,6 +280,9 @@ static int grx500wdt_probe(struct platfo
 
         	grx500_wdt->min_timeout = 1;
 	        grx500_wdt->max_timeout = (0xffffffff / cpu_clk);
+					/*filling in heartbeat for WQ to kick in in case if its enabled*/
+					grx500_wdt->max_hw_heartbeat_ms = grx500_wdt->max_timeout *1000;
+					grx500_wdt->min_hw_heartbeat_ms = grx500_wdt->min_timeout *1000;
 
 		watchdog_init_timeout(grx500_wdt, timeout , &pdev->dev);
 	        watchdog_set_nowayout(grx500_wdt, nowayout);
diff --git a/drivers/watchdog/watchdog_core.c b/drivers/watchdog/watchdog_core.c
--- a/drivers/watchdog/watchdog_core.c
+++ b/drivers/watchdog/watchdog_core.c
@@ -128,11 +128,18 @@ int watchdog_register_device(struct watc
 	 */
 
 	mutex_init(&wdd->lock);
-	id = ida_simple_get(&watchdog_ida, 0, MAX_DOGS, GFP_KERNEL);
-	if (id < 0)
+	/*In case we dont want to over ride, just mark it in ida
+		else the procedure is as usual*/
+	if(test_bit(WDOG_ID_NOT_REQ, &wdd->status))
+	{
+		id = ida_simple_get(&watchdog_ida,wdd->id,wdd->id + 1, GFP_KERNEL);
+	}else{
+		id = ida_simple_get(&watchdog_ida, 0, MAX_DOGS, GFP_KERNEL);
+	}
+
+	if (id < 0 )
 		return id;
 	wdd->id = id;
-
 	ret = watchdog_dev_register(wdd);
 	if (ret) {
 		ida_simple_remove(&watchdog_ida, id);
diff --git a/drivers/watchdog/watchdog_dev.c b/drivers/watchdog/watchdog_dev.c
--- a/drivers/watchdog/watchdog_dev.c
+++ b/drivers/watchdog/watchdog_dev.c
@@ -44,11 +44,714 @@
 
 #include "watchdog_core.h"
 
+#ifdef CONFIG_WDT_SUPPORT_WORKQUEUE
+#include <linux/workqueue.h>	/* For workqueue */
+#include <linux/jiffies.h>	/* For timeout functions */
+#include <linux/slab.h>
+
+#endif
+
 /* the dev_t structure to store the dynamically allocated watchdog devices */
 static dev_t watchdog_devt;
 /* the watchdog device behind /dev/watchdog */
 static struct watchdog_device *old_wdd;
 
+#ifdef CONFIG_WDT_SUPPORT_WORKQUEUE
+static struct watchdog_core_data *old_wd_data;
+static struct workqueue_struct *watchdog_wq;
+
+
+static inline bool watchdog_need_worker(struct watchdog_device *wdd)
+{
+	/* All variables in milli-seconds */
+	unsigned int hm = wdd->max_hw_heartbeat_ms;
+	unsigned int t = wdd->timeout * 1000;
+
+	/*
+	 * A worker to generate heartbeat requests is needed if all of the
+	 * following conditions are true.
+	 * - Userspace activated the watchdog.
+	 * - The driver provided a value for the maximum hardware timeout, and
+	 *   thus is aware that the framework supports generating heartbeat
+	 *   requests.
+	 * - Userspace requests a longer timeout than the hardware can handle.
+	 *
+	 * Alternatively, if userspace has not opened the watchdog
+	 * device, we take care of feeding the watchdog if it is
+	 * running.
+	 */
+	return (hm && watchdog_active(wdd) && t > hm) ||
+		(t && !watchdog_active(wdd) && watchdog_hw_running(wdd));
+}
+
+static long watchdog_next_keepalive(struct watchdog_device *wdd)
+{
+	struct watchdog_core_data *wd_data = wdd->wd_data;
+	unsigned int timeout_ms = wdd->timeout * 1000;
+	unsigned long keepalive_interval;
+	unsigned long last_heartbeat;
+	unsigned long virt_timeout;
+	unsigned int hw_heartbeat_ms;
+
+	virt_timeout = wd_data->last_keepalive + msecs_to_jiffies(timeout_ms);
+	hw_heartbeat_ms = min_not_zero(timeout_ms, wdd->max_hw_heartbeat_ms);
+	keepalive_interval = msecs_to_jiffies(hw_heartbeat_ms / 2);
+
+	if (!watchdog_active(wdd))
+		return keepalive_interval;
+
+	/*
+	 * To ensure that the watchdog times out wdd->timeout seconds
+	 * after the most recent ping from userspace, the last
+	 * worker ping has to come in hw_heartbeat_ms before this timeout.
+	 */
+	last_heartbeat = virt_timeout - msecs_to_jiffies(hw_heartbeat_ms);
+	return min_t(long, last_heartbeat - jiffies, keepalive_interval);
+}
+
+static inline void watchdog_update_worker(struct watchdog_device *wdd)
+{
+	struct watchdog_core_data *wd_data = wdd->wd_data;
+
+	if (watchdog_need_worker(wdd)) {
+		long t = watchdog_next_keepalive(wdd);
+
+		if (t > 0)
+			mod_delayed_work(watchdog_wq, &wd_data->work, t);
+	} else {
+		cancel_delayed_work(&wd_data->work);
+	}
+}
+
+static int __watchdog_ping(struct watchdog_device *wdd)
+{
+	struct watchdog_core_data *wd_data = wdd->wd_data;
+	unsigned long earliest_keepalive = wd_data->last_hw_keepalive +
+				msecs_to_jiffies(wdd->min_hw_heartbeat_ms);
+	int err;
+
+	if (time_is_after_jiffies(earliest_keepalive)) {
+		mod_delayed_work(watchdog_wq, &wd_data->work,
+				 earliest_keepalive - jiffies);
+		return 0;
+	}
+
+	wd_data->last_hw_keepalive = jiffies;
+
+	if (wdd->ops->ping)
+		err = wdd->ops->ping(wdd);  /* ping the watchdog */
+	else
+		err = wdd->ops->start(wdd); /* restart watchdog */
+
+	watchdog_update_worker(wdd);
+
+	return err;
+}
+/*
+ *	watchdog_ping: ping the watchdog.
+ *	@wddev: the watchdog device to ping
+ *
+ *	If the watchdog has no own ping operation then it needs to be
+ *	restarted via the start operation. This wrapper function does
+ *	exactly that.
+ *	We only ping when the watchdog device is running.
+ */
+
+ 
+static int watchdog_ping(struct watchdog_device *wdd)
+{
+	struct watchdog_core_data *wd_data = wdd->wd_data;
+
+	if (!watchdog_active(wdd) && !watchdog_hw_running(wdd))
+		return 0;
+
+	set_bit(_WDOG_KEEPALIVE, &wd_data->status);
+
+	wd_data->last_keepalive = jiffies;
+	return __watchdog_ping(wdd);
+
+}
+
+static void watchdog_ping_work(struct work_struct *work)
+{
+	struct watchdog_core_data *wd_data;
+	struct watchdog_device *wdd;
+
+	wd_data = container_of(to_delayed_work(work), struct watchdog_core_data,
+			       work);
+
+	mutex_lock(&wd_data->lock);
+	wdd = wd_data->wdd;
+	if (wdd && (watchdog_active(wdd) || watchdog_hw_running(wdd)))
+		__watchdog_ping(wdd);
+	mutex_unlock(&wd_data->lock);
+}
+
+static int watchdog_start(struct watchdog_device *wdd)
+{
+	struct watchdog_core_data *wd_data = wdd->wd_data;
+	unsigned long started_at;
+	int err;
+
+	if (watchdog_active(wdd))
+		return 0;
+
+	set_bit(_WDOG_KEEPALIVE, &wd_data->status);
+
+	started_at = jiffies;
+	if (watchdog_hw_running(wdd) && wdd->ops->ping)
+		err = wdd->ops->ping(wdd);
+	else
+		err = wdd->ops->start(wdd);
+	if (err == 0) {
+		set_bit(WDOG_ACTIVE, &wdd->status);
+		wd_data->last_keepalive = started_at;
+		watchdog_update_worker(wdd);
+	}
+
+	return err;
+}
+
+static int watchdog_stop(struct watchdog_device *wdd)
+{
+	int err = 0;
+
+	if (!watchdog_active(wdd))
+		return 0;
+
+	if (test_bit(WDOG_NO_WAY_OUT, &wdd->status)) {
+		pr_info("watchdog%d: nowayout prevents watchdog being stopped!\n",
+			wdd->id);
+		return -EBUSY;
+	}
+
+	if (wdd->ops->stop) {
+		clear_bit(WDOG_HW_RUNNING, &wdd->status);
+		err = wdd->ops->stop(wdd);
+	} else {
+		set_bit(WDOG_HW_RUNNING, &wdd->status);
+	}
+
+	if (err == 0) {
+		clear_bit(WDOG_ACTIVE, &wdd->status);
+		watchdog_update_worker(wdd);
+	}
+
+	return err;
+}
+
+static unsigned int watchdog_get_status(struct watchdog_device *wdd)
+{
+	struct watchdog_core_data *wd_data = wdd->wd_data;
+	unsigned int status;
+
+	if (wdd->ops->status)
+		status = wdd->ops->status(wdd);
+	else
+		status = wdd->bootstatus & (WDIOF_CARDRESET |
+					    WDIOF_OVERHEAT |
+					    WDIOF_FANFAULT |
+					    WDIOF_EXTERN1 |
+					    WDIOF_EXTERN2 |
+					    WDIOF_POWERUNDER |
+					    WDIOF_POWEROVER);
+
+	if (test_bit(_WDOG_ALLOW_RELEASE, &wd_data->status))
+		status |= WDIOF_MAGICCLOSE;
+
+	if (test_and_clear_bit(_WDOG_KEEPALIVE, &wd_data->status))
+		status |= WDIOF_KEEPALIVEPING;
+
+	return status;
+}
+
+static int watchdog_set_timeout(struct watchdog_device *wdd,
+							unsigned int timeout)
+{
+	int err = 0;
+
+	if (!(wdd->info->options & WDIOF_SETTIMEOUT))
+		return -EOPNOTSUPP;
+
+	if (watchdog_timeout_invalid(wdd, timeout))
+		return -EINVAL;
+	if (wdd->ops->set_timeout) {
+		err = wdd->ops->set_timeout(wdd, timeout);
+	} else {
+		wdd->timeout = timeout;
+	}
+
+	watchdog_update_worker(wdd);
+
+	return err;
+}
+
+static int watchdog_get_timeleft(struct watchdog_device *wdd,
+							unsigned int *timeleft)
+{
+	*timeleft = 0;
+
+	if (!wdd->ops->get_timeleft)
+		return -EOPNOTSUPP;
+
+	*timeleft = wdd->ops->get_timeleft(wdd);
+
+	return 0;
+}
+
+static int watchdog_ioctl_op(struct watchdog_device *wdd, unsigned int cmd,
+							unsigned long arg)
+{
+	if (!wdd->ops->ioctl)
+		return -ENOIOCTLCMD;
+
+	return wdd->ops->ioctl(wdd, cmd, arg);
+}
+
+static ssize_t watchdog_write(struct file *file, const char __user *data,
+						size_t len, loff_t *ppos)
+{
+	struct watchdog_core_data *wd_data = file->private_data;
+	struct watchdog_device *wdd;
+	int err;
+	size_t i;
+	char c;
+
+	if (len == 0)
+		return 0;
+
+	/*
+	 * Note: just in case someone wrote the magic character
+	 * five months ago...
+	 */
+	clear_bit(_WDOG_ALLOW_RELEASE, &wd_data->status);
+
+	/* scan to see whether or not we got the magic character */
+	for (i = 0; i != len; i++) {
+		if (get_user(c, data + i))
+			return -EFAULT;
+		if (c == 'V')
+			set_bit(_WDOG_ALLOW_RELEASE, &wd_data->status);
+	}
+
+	/* someone wrote to us, so we send the watchdog a keepalive ping */
+
+	err = -ENODEV;
+	mutex_lock(&wd_data->lock);
+	wd_data->user_call = 1;
+	wdd = wd_data->wdd;
+	if (wdd)
+		err = watchdog_ping(wdd);
+	mutex_unlock(&wd_data->lock);
+
+	if (err < 0)
+		return err;
+
+	return len;
+}
+
+static long watchdog_ioctl(struct file *file, unsigned int cmd,
+							unsigned long arg)
+{
+	struct watchdog_core_data *wd_data = file->private_data;
+	void __user *argp = (void __user *)arg;
+	struct watchdog_device *wdd;
+	int __user *p = argp;
+	unsigned int val;
+	int err;
+
+	mutex_lock(&wd_data->lock);
+
+	wdd = wd_data->wdd;
+	if (!wdd) {
+		err = -ENODEV;
+		goto out_ioctl;
+	}
+
+	err = watchdog_ioctl_op(wdd, cmd, arg);
+	if (err != -ENOIOCTLCMD)
+		goto out_ioctl;
+
+	switch (cmd) {
+	case WDIOC_GETSUPPORT:
+		err = copy_to_user(argp, wdd->info,
+			sizeof(struct watchdog_info)) ? -EFAULT : 0;
+		break;
+	case WDIOC_GETSTATUS:
+		val = watchdog_get_status(wdd);
+		err = put_user(val, p);
+		break;
+	case WDIOC_GETBOOTSTATUS:
+		err = put_user(wdd->bootstatus, p);
+		break;
+	case WDIOC_SETOPTIONS:
+		if (get_user(val, p)) {
+			err = -EFAULT;
+			break;
+		}
+		if (val & WDIOS_DISABLECARD) {
+			err = watchdog_stop(wdd);
+			if (err < 0)
+				break;
+		}
+		if (val & WDIOS_ENABLECARD)
+			err = watchdog_start(wdd);
+		break;
+	case WDIOC_KEEPALIVE:
+		if (!(wdd->info->options & WDIOF_KEEPALIVEPING)) {
+			err = -EOPNOTSUPP;
+			break;
+		}
+		wdd->wd_data->user_call = 1;
+		err = watchdog_ping(wdd);
+		break;
+	case WDIOC_SETTIMEOUT:
+		if (get_user(val, p)) {
+			err = -EFAULT;
+			break;
+		}
+		err = watchdog_set_timeout(wdd, val);
+		if (err < 0)
+			break;
+		/* If the watchdog is active then we send a keepalive ping
+		 * to make sure that the watchdog keep's running (and if
+		 * possible that it takes the new timeout) */
+		err = watchdog_ping(wdd);
+		if (err < 0)
+			break;
+		/* Fall */
+	case WDIOC_GETTIMEOUT:
+		/* timeout == 0 means that we don't know the timeout */
+		if (wdd->timeout == 0) {
+			err = -EOPNOTSUPP;
+			break;
+		}
+		err = put_user(wdd->timeout, p);
+		break;
+	case WDIOC_GETTIMELEFT:
+		err = watchdog_get_timeleft(wdd, &val);
+		if (err < 0)
+			break;
+		err = put_user(val, p);
+		break;
+	default:
+		err = -ENOTTY;
+		break;
+	}
+
+out_ioctl:
+	mutex_unlock(&wd_data->lock);
+	return err;
+}
+
+static int watchdog_open(struct inode *inode, struct file *file)
+{
+	struct watchdog_core_data *wd_data;
+	struct watchdog_device *wdd;
+	int err;
+
+	/* Get the corresponding watchdog device */
+	if (imajor(inode) == MISC_MAJOR)
+		wd_data = old_wd_data;
+	else
+		wd_data = container_of(inode->i_cdev, struct watchdog_core_data,
+				       cdev);
+
+	/* the watchdog is single open! */
+	if (test_and_set_bit(_WDOG_DEV_OPEN, &wd_data->status))
+		return -EBUSY;
+
+	wdd = wd_data->wdd;
+
+	/*
+	 * If the /dev/watchdog device is open, we don't want the module
+	 * to be unloaded.
+	 */
+	if (!watchdog_hw_running(wdd) && !try_module_get(wdd->ops->owner)) {
+		err = -EBUSY;
+		goto out_clear;
+	}
+
+	err = watchdog_start(wdd);
+	if (err < 0)
+		goto out_mod;
+
+	file->private_data = wd_data;
+
+	if (!watchdog_hw_running(wdd))
+		kref_get(&wd_data->kref);
+
+	/* dev/watchdog is a virtual (and thus non-seekable) filesystem */
+	return nonseekable_open(inode, file);
+
+out_mod:
+	module_put(wd_data->wdd->ops->owner);
+out_clear:
+	clear_bit(_WDOG_DEV_OPEN, &wd_data->status);
+	return err;
+}
+
+static void watchdog_core_data_release(struct kref *kref)
+{
+	struct watchdog_core_data *wd_data;
+
+	wd_data = container_of(kref, struct watchdog_core_data, kref);
+
+	kfree(wd_data);
+}
+
+static int watchdog_release(struct inode *inode, struct file *file)
+{
+	struct watchdog_core_data *wd_data = file->private_data;
+	struct watchdog_device *wdd;
+	int err = -EBUSY;
+	bool running;
+
+	mutex_lock(&wd_data->lock);
+
+	wdd = wd_data->wdd;
+	if (!wdd)
+		goto done;
+
+	/*
+	 * We only stop the watchdog if we received the magic character
+	 * or if WDIOF_MAGICCLOSE is not set. If nowayout was set then
+	 * watchdog_stop will fail.
+	 */
+	if (!test_bit(WDOG_ACTIVE, &wdd->status))
+		err = 0;
+	else if (test_and_clear_bit(_WDOG_ALLOW_RELEASE, &wd_data->status) ||
+		 !(wdd->info->options & WDIOF_MAGICCLOSE))
+		err = watchdog_stop(wdd);
+
+	/* If the watchdog was not stopped, send a keepalive ping */
+	if (err < 0) {
+		pr_crit("watchdog%d: watchdog did not stop!\n", wdd->id);
+		watchdog_ping(wdd);
+	}
+
+	watchdog_update_worker(wdd);
+
+	/* make sure that /dev/watchdog can be re-opened */
+	clear_bit(_WDOG_DEV_OPEN, &wd_data->status);
+
+done:
+	running = wdd && watchdog_hw_running(wdd);
+	mutex_unlock(&wd_data->lock);
+	/*
+	 * Allow the owner module to be unloaded again unless the watchdog
+	 * is still running. If the watchdog is still running, it can not
+	 * be stopped, and its driver must not be unloaded.
+	 */
+	if (!running) {
+		module_put(wd_data->cdev.owner);
+		kref_put(&wd_data->kref, watchdog_core_data_release);
+	}
+	return 0;
+}
+static const struct file_operations watchdog_fops = {
+	.owner		= THIS_MODULE,
+	.write		= watchdog_write,
+	.unlocked_ioctl	= watchdog_ioctl,
+	.open		= watchdog_open,
+	.release	= watchdog_release,
+};
+
+static struct miscdevice watchdog_miscdev = {
+	.minor		= WATCHDOG_MINOR,
+	.name		= "watchdog",
+	.fops		= &watchdog_fops,
+};
+
+
+/*
+ *	watchdog_cdev_register: register watchdog character device
+ *	@wdd: watchdog device
+ *	@devno: character device number
+ *
+ *	Register a watchdog character device including handling the legacy
+ *	/dev/watchdog node. /dev/watchdog is actually a miscdevice and
+ *	thus we set it up like that.
+ */
+
+static int watchdog_cdev_register(struct watchdog_device *wdd, dev_t devno)
+{
+	struct watchdog_core_data *wd_data;
+	int err;
+
+	wd_data = kzalloc(sizeof(struct watchdog_core_data), GFP_KERNEL);
+	if (!wd_data)
+		return -ENOMEM;
+	kref_init(&wd_data->kref);
+	mutex_init(&wd_data->lock);
+
+	wd_data->wdd = wdd;
+	wdd->wd_data = wd_data;
+
+	if (!watchdog_wq)
+		return -ENODEV;
+
+	INIT_DELAYED_WORK(&wd_data->work, watchdog_ping_work);
+
+	if (wdd->id == 0) {
+		old_wd_data = wd_data;
+		watchdog_miscdev.parent = wdd->parent;
+		err = misc_register(&watchdog_miscdev);
+		if (err != 0) {
+			pr_err("%s: cannot register miscdev on minor=%d (err=%d).\n",
+				wdd->info->identity, WATCHDOG_MINOR, err);
+			if (err == -EBUSY)
+				pr_err("%s: a legacy watchdog module is probably present.\n",
+					wdd->info->identity);
+			old_wd_data = NULL;
+			kfree(wd_data);
+			return err;
+		}
+	}
+
+	/* Fill in the data structures */
+	cdev_init(&wd_data->cdev, &watchdog_fops);
+	wd_data->cdev.owner = wdd->ops->owner;
+
+	/* Add the device */
+	err = cdev_add(&wd_data->cdev, devno, 1);
+	if (err) {
+		pr_err("watchdog%d unable to add device %d:%d\n",
+			wdd->id,  MAJOR(watchdog_devt), wdd->id);
+		if (wdd->id == 0) {
+			misc_deregister(&watchdog_miscdev);
+			old_wd_data = NULL;
+			kref_put(&wd_data->kref, watchdog_core_data_release);
+		}
+		return err;
+	}
+	/*copy core data cdev to main dev*/
+	memcpy(&wdd->cdev,&wd_data->cdev,sizeof(wd_data->cdev));
+
+	/* Record time of most recent heartbeat as 'just before now'. */
+	wd_data->last_hw_keepalive = jiffies - 1;
+
+	/*
+	 * If the watchdog is running, prevent its driver from being unloaded,
+	 * and schedule an immediate ping.
+	 */
+	if (watchdog_hw_running(wdd)) {
+		__module_get(wdd->ops->owner);
+		kref_get(&wd_data->kref);
+		queue_delayed_work(watchdog_wq, &wd_data->work, 0);
+	}
+
+	return 0;
+}
+
+/*
+ *	watchdog_cdev_unregister: unregister watchdog character device
+ *	@watchdog: watchdog device
+ *
+ *	Unregister watchdog character device and if needed the legacy
+ *	/dev/watchdog device.
+ */
+
+static void watchdog_cdev_unregister(struct watchdog_device *wdd)
+{
+	struct watchdog_core_data *wd_data = wdd->wd_data;
+
+	cdev_del(&wd_data->cdev);
+	if (wdd->id == 0) {
+		misc_deregister(&watchdog_miscdev);
+		old_wd_data = NULL;
+	}
+
+	mutex_lock(&wd_data->lock);
+	wd_data->wdd = NULL;
+	wdd->wd_data = NULL;
+	mutex_unlock(&wd_data->lock);
+
+	cancel_delayed_work_sync(&wd_data->work);
+
+	kref_put(&wd_data->kref, watchdog_core_data_release);
+}
+
+
+/*
+ *	watchdog_dev_register: register a watchdog device
+ *	@wdd: watchdog device
+ *
+ *	Register a watchdog device including handling the legacy
+ *	/dev/watchdog node. /dev/watchdog is actually a miscdevice and
+ *	thus we set it up like that.
+ */
+
+int watchdog_dev_register(struct watchdog_device *wdd)
+{
+//	struct device *dev;
+	dev_t devno;
+	int ret;
+
+	devno = MKDEV(MAJOR(watchdog_devt), wdd->id);
+	ret = watchdog_cdev_register(wdd, devno);
+
+	return ret;
+}
+
+/*
+ *	watchdog_dev_unregister: unregister a watchdog device
+ *	@watchdog: watchdog device
+ *
+ *	Unregister watchdog device and if needed the legacy
+ *	/dev/watchdog device.
+ */
+
+int watchdog_dev_unregister(struct watchdog_device *wdd)
+{
+	watchdog_cdev_unregister(wdd);
+	return 0;
+}
+
+/*
+ *	watchdog_dev_init: init dev part of watchdog core
+ *
+ *	Allocate a range of chardev nodes to use for watchdog devices
+ */
+
+int __init watchdog_dev_init(void)
+{
+	int err;
+
+	watchdog_wq = alloc_workqueue("watchdogd",
+				      WQ_HIGHPRI | WQ_MEM_RECLAIM, 0);
+	if (!watchdog_wq) {
+		pr_err("Failed to create watchdog workqueue\n");
+		return -ENOMEM;
+	}
+
+
+	err = alloc_chrdev_region(&watchdog_devt, 0, MAX_DOGS, "watchdog");
+	if (err < 0) {
+		pr_err("watchdog: unable to allocate char dev region\n");
+		goto err_alloc;
+	}
+
+	return 0;
+
+err_alloc:
+	destroy_workqueue(watchdog_wq);
+	return err;
+}
+
+/*
+ *	watchdog_dev_exit: exit dev part of watchdog core
+ *
+ *	Release the range of chardev nodes used for watchdog devices
+ */
+
+void __exit watchdog_dev_exit(void)
+{
+	unregister_chrdev_region(watchdog_devt, MAX_DOGS);
+	destroy_workqueue(watchdog_wq);
+}
+
+#else
 /*
  *	watchdog_ping: ping the watchdog.
  *	@wddev: the watchdog device to ping
@@ -91,7 +794,6 @@ out_ping:
  *	This function returns zero on success or a negative errno code for
  *	failure.
  */
-
 static int watchdog_start(struct watchdog_device *wddev)
 {
 	int err = 0;
@@ -124,7 +826,6 @@ out_start:
  *	failure.
  *	If the 'nowayout' feature was set, the watchdog cannot be stopped.
  */
-
 static int watchdog_stop(struct watchdog_device *wddev)
 {
 	int err = 0;
@@ -600,3 +1301,4 @@ void __exit watchdog_dev_exit(void)
 {
 	unregister_chrdev_region(watchdog_devt, MAX_DOGS);
 }
+#endif
diff --git a/include/linux/watchdog.h b/include/linux/watchdog.h
--- a/include/linux/watchdog.h
+++ b/include/linux/watchdog.h
@@ -49,6 +49,28 @@ struct watchdog_ops {
 	void (*unref)(struct watchdog_device *);
 	long (*ioctl)(struct watchdog_device *, unsigned int, unsigned long);
 };
+/*
+ * struct watchdog_core_data - watchdog core internal data
+ * @kref:	Reference count.
+ * @cdev:	The watchdog's Character device.
+ * @wdd:	Pointer to watchdog device.
+ * @lock:	Lock for watchdog core.
+ * @status:	Watchdog core internal status bits.
+ */
+struct watchdog_core_data {
+	struct kref kref;
+	struct cdev cdev;
+	struct watchdog_device *wdd;
+	struct mutex lock;
+	unsigned long last_keepalive;
+	unsigned long last_hw_keepalive;
+	struct delayed_work work;
+	unsigned long status;		/* Internal status bits */
+	bool user_call; /* user_space call or wq call for ping */
+#define _WDOG_DEV_OPEN		0	/* Opened ? */
+#define _WDOG_ALLOW_RELEASE	1	/* Did we receive the magic char ? */
+#define _WDOG_KEEPALIVE		2	/* Did we receive a keepalive ? */
+};
 
 /** struct watchdog_device - The structure that defines a watchdog device
  *
@@ -86,8 +108,11 @@ struct watchdog_device {
 	unsigned int timeout;
 	unsigned int min_timeout;
 	unsigned int max_timeout;
+	unsigned int min_hw_heartbeat_ms;
+	unsigned int max_hw_heartbeat_ms;
 	void *driver_data;
 	struct mutex lock;
+	struct watchdog_core_data *wd_data;
 	unsigned long status;
 /* Bit numbers for status flags */
 #define WDOG_ACTIVE		0	/* Is the watchdog running/active */
@@ -95,6 +120,8 @@ struct watchdog_device {
 #define WDOG_ALLOW_RELEASE	2	/* Did we receive the magic char ? */
 #define WDOG_NO_WAY_OUT		3	/* Is 'nowayout' feature set ? */
 #define WDOG_UNREGISTERED	4	/* Has the device been unregistered */
+#define WDOG_HW_RUNNING		5	/* True if HW watchdog running */
+#define WDOG_ID_NOT_REQ		6	/* watchdog id creation from ida not req */
 };
 
 #ifdef CONFIG_WATCHDOG_NOWAYOUT
@@ -111,6 +138,16 @@ static inline bool watchdog_active(struc
 	return test_bit(WDOG_ACTIVE, &wdd->status);
 }
 
+/*
+ * Use the following function to check whether or not the hardware watchdog
+ * is running
+ */
+static inline bool watchdog_hw_running(struct watchdog_device *wdd)
+{
+	return test_bit(WDOG_HW_RUNNING, &wdd->status);
+}
+
+
 /* Use the following function to set the nowayout feature */
 static inline void watchdog_set_nowayout(struct watchdog_device *wdd, bool nowayout)
 {
@@ -121,8 +158,20 @@ static inline void watchdog_set_nowayout
 /* Use the following function to check if a timeout value is invalid */
 static inline bool watchdog_timeout_invalid(struct watchdog_device *wdd, unsigned int t)
 {
-	return ((wdd->max_timeout != 0) &&
-		(t < wdd->min_timeout || t > wdd->max_timeout));
+	/*
+	 * The timeout is invalid if
+	 * - the requested value is larger than UINT_MAX / 1000
+	 *   (since internal calculations are done in milli-seconds),
+	 * or
+	 * - the requested value is smaller than the configured minimum timeout,
+	 * or
+	 * - a maximum hardware timeout is not configured, a maximum timeout
+	 *   is configured, and the requested value is larger than the
+	 *   configured maximum timeout.
+	 */
+	return t > UINT_MAX / 1000 || t < wdd->min_timeout ||
+		(!wdd->max_hw_heartbeat_ms && wdd->max_timeout &&
+		 t > wdd->max_timeout);
 }
 
 /* Use the following functions to manipulate watchdog driver specific data */
