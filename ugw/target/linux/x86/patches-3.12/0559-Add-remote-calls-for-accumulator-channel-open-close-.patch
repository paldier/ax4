From 37d00b2ee13803f923dd8edb158d4c40d3829812 Mon Sep 17 00:00:00 2001
From: ofirbitt <ofir1.bitton@intel.com>
Date: Wed, 21 Dec 2016 19:29:55 +0200
Subject: [PATCH 559/639] Add remote calls for accumulator channel open/close
 to pal_cppi41.c

---
 arch/x86/pal_cppi41/pal_cppi41.c                   | 190 ++++++++++++++++++++-
 .../linux/avalanche/generic/cppi41_hw_mbox_if.h    |  13 --
 include/linux/avalanche/generic/pal_cppi41.h       |  11 ++
 3 files changed, 200 insertions(+), 14 deletions(-)

diff --git a/arch/x86/pal_cppi41/pal_cppi41.c b/arch/x86/pal_cppi41/pal_cppi41.c
index 7e45596..9cda5c3 100644
--- a/arch/x86/pal_cppi41/pal_cppi41.c
+++ b/arch/x86/pal_cppi41/pal_cppi41.c
@@ -407,7 +407,7 @@ static AVALANCHE_PP_RET_e __cppi4AccChEnRateLimit(AVALANCHE_PP_ACC_CH_INFO_t *pt
 
     return (PP_RC_SUCCESS);
 }
-
+#ifdef CONFIG_INTEL_KERNEL_PP_DRIVER_LOCAL
 PAL_Cppi4AccChHnd PAL_cppi4AccChOpen(PAL_Handle hnd, Cppi4AccumulatorCfg* accCfg)
 {
     Uint32 cookie;
@@ -497,6 +497,107 @@ PAL_Cppi4AccChHnd PAL_cppi4AccChOpen(PAL_Handle hnd, Cppi4AccumulatorCfg* accCfg
 
     return (PAL_Cppi4AccChHnd) accChObj;
 }
+#else
+/* Following API will use HW mailbox provide Accumulator fuctionalities */ 
+PAL_Cppi4AccChHnd PAL_cppi4AccChOpen(PAL_Handle hnd, Cppi4AccumulatorCfg* accCfg)
+{
+    
+    /*Return pointer to the caller */
+    PAL_Cppi4AccChObj *accChObj = NULL;
+    /* transport message over HW_MBOX */
+    Cppi41HwMboxAccChOpenMsg_t  openAccChObj = {0};
+    /* local temporary varaibles */
+    Cppi41HwMboxAccChOpenReplyMsg_t* tmp = NULL;
+    unsigned long tmpPtr = 0;
+    /* transport message over HW_MBOX */
+    /* Return length of HW mailbox Op-Code channle */
+    Uint32 dataLen = sizeof(Cppi41HwMboxAccChOpenMsg_t);
+    if(!accCfg)
+    {
+        EPRINTK("NULL pointer reference.");
+        return NULL;
+    }
+
+    /* kmalloc returns cache line aligned memory unless you are debugging the slab allocator (2.6.18) */
+    accChObj = (PAL_Cppi4AccChObj *)kzalloc(sizeof(PAL_Cppi4AccChObj) ,GFP_KERNEL);
+    if(!accChObj)
+    {
+        EPRINTK("could not allocate memeory for local accumulator ojbect");
+        return NULL;
+    }
+
+    /*copy accCfg data to accumulator channel onject */
+    if(!accCfg->list.listBase)
+    {
+
+        EPRINTK("NULL pointer reference. for accCfg.list.base");
+        return NULL;
+    }
+
+    /* Copy datapipe accumulator init paramters into the message container */
+    memcpy(&openAccChObj.initCfg, accCfg, sizeof(Cppi4AccumulatorCfg));  
+    DPRINTK(" Virtual  list.listBase=%p, address received=%p before sending to HWMbox.\n",  openAccChObj.initCfg.list.listBase,  accCfg->list.listBase );
+
+    /* APPCPU virtual address need to converted to Physical address before sending to HW mailbox */
+    tmpPtr = (unsigned long)PAL_CPPI4_VIRT_2_PHYS(openAccChObj.initCfg.list.listBase);
+    openAccChObj.initCfg.list.listBase = (void*)tmpPtr; 
+    DPRINTK(" Physical  list.listBase=%p, Original address received=%p before sending to HWMbox.\n",  openAccChObj.initCfg.list.listBase,  accCfg->list.listBase );
+
+    /* hardware mailbox implementation to open accumulator channel goes here */
+    if(hwMbox_isReady())
+    { 
+        EPRINTK("HW mailbox isn't ready yet.");
+        kfree(accChObj);
+        return NULL;
+    }
+     ACCUM_CH_PARAM_DEBUG(openAccChObj.initCfg);
+    /* need to convert data from cpu_to_be(); */
+     
+    if(!Cppi41HwMboxAccChangeEndianness(&openAccChObj, endiannessBig))
+    {
+        EPRINTK("data conversion fo endianness failed");
+        kfree(accChObj);
+        return NULL;
+    }
+
+    /* need to send accumulator handler as well  though we are not using it right now but incase needed in future */
+    /*  will receive back Object address in SendReplyOp() at npcpuAddress variable */
+     openAccChObj.cmd = cpu_to_be32(CPPI41_HWMBOX_CMD_ACC_CH_OPEN);
+     ACCUM_CH_PARAM_DEBUG(openAccChObj.initCfg);
+    /* send a message to NP-CPU and expect a 64 byte reply back using SendReplyOp()*/
+    DPRINTK(" size of data length=%d.", sizeof(Cppi41HwMboxAccChOpenMsg_t));
+    if(hwMbox_sendOpcode(HW_MBOX_MASTER_NP_CPU, NPCPU_APPCPU_HW_MBOX_TAG_CPPI41_MBX , (uint8_t *)&openAccChObj, sizeof(Cppi41HwMboxAccChOpenMsg_t) , sizeof(Cppi41HwMboxAccChOpenMsg_t) , &dataLen))
+    {
+        EPRINTK("HW mailbox hwMbox_sendOpcode failed.");
+        kfree(accChObj);
+        return NULL;
+    }
+    if(dataLen != sizeof(Cppi41HwMboxAccChOpenReplyMsg_t))
+    {
+        EPRINTK("HW mailbox hwMbox_sendOpcode reply wasnt of desire length Cppi41HwMboxAccChOpenReplyMsg=%d dataLen=%d ",sizeof(Cppi41HwMboxAccChOpenReplyMsg_t), dataLen);
+        kfree(accChObj);
+        return NULL;
+    }
+
+    DPRINTK("HW mailbox adpHwMboxmessageObj.msgData.initCfg.list.listBase before Endian change=%p.", openAccChObj.initCfg.list.listBase);
+    /* need to conver data from be_to_cpu(); */
+
+    DPRINTK("HW mailbox Received adpHwMboxmessageObj.msgData.initCfg.list.listBase after Endian change=%p.", openAccChObj.initCfg.list.listBase);
+    DPRINTK("HW mailbox called to  accumulator open successful.");
+    /* copy HW_Mbox message to kmalloced object for return */
+    DPRINTK("data length=%d.",dataLen );
+    memcpy(&(accChObj->initCfg), accCfg, sizeof(Cppi4AccumulatorCfg)); 
+    tmp = (Cppi41HwMboxAccChOpenReplyMsg_t *) &openAccChObj;
+    accChObj->curPage = be32_to_cpu( tmp->curPage);
+    DPRINTK("curPage=%d.", accChObj->curPage );
+    accChObj->palCppi4Obj = (void *)tmp->accChHnd;
+    DPRINTK("npcpuAddress=%d.", tmp->accChHnd );
+
+    DPRINTK("HW mailbox Received accChObj->initCfg.list.listBase after phys_to_virt=%p.", accChObj->initCfg.list.listBase);
+
+    return (PAL_Cppi4AccChHnd)accChObj;  
+}
+#endif
 EXPORT_SYMBOL(PAL_cppi4AccChOpen);
 
 PAL_Cppi4AccChHnd PAL_cppi4AccChOpenAppCpu(PAL_Handle hnd, Cppi4AccumulatorCfg* accCfg)
@@ -574,6 +675,7 @@ void PAL_cppi4AccChEnRateLimit(Cppi4AccumulatorRateLimitCfg* accCfg)
 }
 EXPORT_SYMBOL(PAL_cppi4AccChEnRateLimit);
 
+#ifdef CONFIG_INTEL_KERNEL_PP_DRIVER_LOCAL
 int PAL_cppi4AccChClose(PAL_Cppi4AccChHnd hnd, void *closeArgs)
 {
     PAL_Cppi4AccChObj *accChObj = (PAL_Cppi4AccChObj *) hnd;
@@ -617,8 +719,94 @@ int PAL_cppi4AccChClose(PAL_Cppi4AccChHnd hnd, void *closeArgs)
 
     return PAL_SOK;
 }
+#else
+int PAL_cppi4AccChClose(PAL_Cppi4AccChHnd hnd, void *closeArgs)
+{
+    /* local pointer to free */
+    PAL_Cppi4AccChObj *accChObj;
+    /* transport message over HW_MBOX */
+    Cppi41HwMboxAccChCloseMsg_t adpHwMboxmessageObj;
+    Uint32 dataLen = sizeof(Cppi41HwMboxAccChCloseMsg_t);
+
+    if(!hnd)
+    {
+        EPRINTK("NULL pointer reference.");
+        return false;
+    }
+
+    accChObj = (PAL_Cppi4AccChObj *)hnd;
+    /*copy PAL_Cppi4AccChObj data to accumulator channel onject */
+    /* convert data since CPPI need ch_num for accumulator close  */
+    adpHwMboxmessageObj.accChHnd =  (void *)accChObj->palCppi4Obj;  
+    DPRINTK("npcpuAddress=%d.", accChObj->palCppi4Obj );
+
+    adpHwMboxmessageObj.cmd = cpu_to_be32(CPPI41_HWMBOX_CMD_ACC_CH_CLOSE);
+    /* send a message to NP-CPU and expect to pointer get free in NPCPUaddress space make sure correct poiter by reply*/
+    if(hwMbox_sendOpcode(HW_MBOX_MASTER_NP_CPU,NPCPU_APPCPU_HW_MBOX_TAG_CPPI41_MBX, (uint8_t *)&adpHwMboxmessageObj, sizeof(Cppi41HwMboxAccChCloseMsg_t), sizeof(Cppi41HwMboxAccChCloseMsg_t), &dataLen))
+    {
+        EPRINTK("HW mailbox hwMbox_sendOpcode failed.");
+        return false;
+    }
+    /* free local onject which was created in Open call */
+    kfree(accChObj);
+    /* hardware mailbox implementation to close accumulator channel goes here */
+    DPRINTK("HW mailbox called to free accumulator channel successful.");
+    return true;
+}
+#endif
 EXPORT_SYMBOL(PAL_cppi4AccChClose);
 
+bool Cppi41AccChangeEndianness(Cppi41HwMboxAccChOpenMsg_t *destCfgData, endianness_e endianity)
+{
+    if( !destCfgData )
+    {
+        EPRINTK(" null pointer reference ");
+        return false;
+    }
+    if( (endianity != endiannessBig) && (endianity != endiannessLittle) )
+    {
+        EPRINTK(" Endianness value pass in datapipe is not correct ");
+        return false; 
+    }
+
+    if(endianity == endiannessBig)
+    {
+        (*destCfgData).initCfg.accChanNum               =   cpu_to_be32((*destCfgData).initCfg.accChanNum);
+        (*destCfgData).initCfg.mode                     =   cpu_to_be32((*destCfgData).initCfg.mode);
+        (*destCfgData).initCfg.queue.qMgr               =   cpu_to_be32((*destCfgData).initCfg.queue.qMgr);
+        (*destCfgData).initCfg.queue.qNum               =   cpu_to_be32((*destCfgData).initCfg.queue.qNum);
+        (*destCfgData).initCfg.pacingTickCnt            =   cpu_to_be32((*destCfgData).initCfg.pacingTickCnt);
+        (*destCfgData).initCfg.list.listBase            =   (void *)cpu_to_be32((unsigned int)(*destCfgData).initCfg.list.listBase);
+        (*destCfgData).initCfg.list.maxPageEntry        =   cpu_to_be32((*destCfgData).initCfg.list.maxPageEntry);
+        (*destCfgData).initCfg.list.pacingMode          =   cpu_to_be32((*destCfgData).initCfg.list.pacingMode);
+        (*destCfgData).initCfg.list.stallAvoidance      =   cpu_to_be32((*destCfgData).initCfg.list.stallAvoidance);
+        (*destCfgData).initCfg.list.listCountMode       =   cpu_to_be32((*destCfgData).initCfg.list.listCountMode);
+        (*destCfgData).initCfg.list.listEntrySize       =   cpu_to_be32((*destCfgData).initCfg.list.listEntrySize);
+        (*destCfgData).initCfg.list.maxPageCnt          =   cpu_to_be32((*destCfgData).initCfg.list.maxPageCnt);
+        (*destCfgData).initCfg.monitor.pktCountThresh   =   cpu_to_be32((*destCfgData).initCfg.monitor.pktCountThresh);
+        (*destCfgData).initCfg.monitor.pacingMode       =   cpu_to_be32((*destCfgData).initCfg.monitor.pacingMode);
+    }
+    if(endianity == endiannessLittle)
+    {
+        (*destCfgData).initCfg.accChanNum               =   be32_to_cpu((*destCfgData).initCfg.accChanNum);
+        (*destCfgData).initCfg.mode                     =   be32_to_cpu((*destCfgData).initCfg.mode);
+        (*destCfgData).initCfg.queue.qMgr               =   be32_to_cpu((*destCfgData).initCfg.queue.qMgr);
+        (*destCfgData).initCfg.queue.qNum               =   be32_to_cpu((*destCfgData).initCfg.queue.qNum);
+        (*destCfgData).initCfg.pacingTickCnt            =   be32_to_cpu((*destCfgData).initCfg.pacingTickCnt);
+        (*destCfgData).initCfg.list.listBase            =   (void *)be32_to_cpu((*destCfgData).initCfg.list.listBase);
+        (*destCfgData).initCfg.list.maxPageEntry        =   be32_to_cpu((*destCfgData).initCfg.list.maxPageEntry);
+        (*destCfgData).initCfg.list.pacingMode          =   be32_to_cpu((*destCfgData).initCfg.list.pacingMode);
+        (*destCfgData).initCfg.list.stallAvoidance      =   be32_to_cpu((*destCfgData).initCfg.list.stallAvoidance);
+        (*destCfgData).initCfg.list.listCountMode       =   be32_to_cpu((*destCfgData).initCfg.list.listCountMode);
+        (*destCfgData).initCfg.list.listEntrySize       =   be32_to_cpu((*destCfgData).initCfg.list.listEntrySize);
+        (*destCfgData).initCfg.list.maxPageCnt          =   be32_to_cpu((*destCfgData).initCfg.list.maxPageCnt);
+        (*destCfgData).initCfg.monitor.pktCountThresh   =   be32_to_cpu((*destCfgData).initCfg.monitor.pktCountThresh);
+        (*destCfgData).initCfg.monitor.pacingMode       =   be32_to_cpu((*destCfgData).initCfg.monitor.pacingMode);
+    }
+    return true;
+}
+EXPORT_SYMBOL(Cppi41AccChangeEndianness);
+
 void* PAL_cppi4AccChGetNextList(PAL_Cppi4AccChHnd hnd)
 {
     PAL_Cppi4AccChObj *accChObj = (PAL_Cppi4AccChObj *) hnd;
diff --git a/include/linux/avalanche/generic/cppi41_hw_mbox_if.h b/include/linux/avalanche/generic/cppi41_hw_mbox_if.h
index 1b8d4c4..5d81a82 100644
--- a/include/linux/avalanche/generic/cppi41_hw_mbox_if.h
+++ b/include/linux/avalanche/generic/cppi41_hw_mbox_if.h
@@ -116,17 +116,4 @@ typedef struct
 } Cppi41HwMboxOffChipMemInfoReplyMsg_t;
 #endif /* __CPPI41_HW_MBOX_IF_H__ */
 
-/***************************************/
-/*  enum for endianness conversion    **/
-/**************************************/
-typedef enum
-{
-    endiannessBig,
-    endiannessLittle
-}endianness_e;
-
-/*********************************/
-/**    Functions declaration    **/
-/*********************************/
 bool  Cppi41HwMboxAccChangeEndianness(Cppi41HwMboxAccChOpenMsg_t *destCfgData, endianness_e endianity);
-
diff --git a/include/linux/avalanche/generic/pal_cppi41.h b/include/linux/avalanche/generic/pal_cppi41.h
index 1a96a00d..1a14ff3 100644
--- a/include/linux/avalanche/generic/pal_cppi41.h
+++ b/include/linux/avalanche/generic/pal_cppi41.h
@@ -507,6 +507,16 @@ typedef enum desc_type
     DESC_TEARDOWN = 19
 } DESC_TYPE;
 
+
+/***************************************/
+/*  enum for endianness conversion    **/
+/**************************************/
+typedef enum
+{
+    endiannessBig,
+    endiannessLittle
+}endianness_e;
+
 #ifdef __KERNEL__
 
 
@@ -720,6 +730,7 @@ PAL_Cppi4AccChHnd PAL_cppi4AccChOpenAppCpu(PAL_Handle hnd, Cppi4AccumulatorCfg*
  */
 int PAL_cppi4AccChClose(PAL_Cppi4AccChHnd hnd, void *closeArgs);
 
+
 /**
  *  \brief PAL CPPI 4.1 get transmit channel accumulator page.
  *
-- 
2.10.1

