# HG changeset patch
# Parent 57cb605593872aa284f4e28981c549390d36ffb1
# Improve conntrack handling algorithm when system gets full

Signed-off-by: Kumar, Punith <punith.kumar@intel.com>

diff --git a/net/netfilter/nf_conntrack_core.c b/net/netfilter/nf_conntrack_core.c
--- a/net/netfilter/nf_conntrack_core.c
+++ b/net/netfilter/nf_conntrack_core.c
@@ -90,6 +90,7 @@ EXPORT_SYMBOL_GPL(nf_conntrack_max);
 DEFINE_PER_CPU(struct nf_conn, nf_conntrack_untracked);
 EXPORT_PER_CPU_SYMBOL(nf_conntrack_untracked);
 
+#define LTQ_IP_CONNTRACK_REPLACEMENT
 unsigned int nf_conntrack_hash_rnd __read_mostly;
 EXPORT_SYMBOL_GPL(nf_conntrack_hash_rnd);
 
@@ -772,23 +773,55 @@ static noinline int early_drop(struct ne
 	struct hlist_nulls_node *n;
 	unsigned int i, cnt = 0;
 	int dropped = 0;
+#ifdef LTQ_IP_CONNTRACK_REPLACEMENT
+	int recheck  = 1;
+redo:
+#endif
 
 	rcu_read_lock();
 	for (i = 0; i < net->ct.htable_size; i++) {
 		hlist_nulls_for_each_entry_rcu(h, n, &net->ct.hash[hash],
 					 hnnode) {
 			tmp = nf_ct_tuplehash_to_ctrack(h);
+#ifdef LTQ_IP_CONNTRACK_REPLACEMENT
+			if (recheck) {
+				if (!test_bit(IPS_CONFIRMED_BIT, &tmp->status))
+					ct = tmp;
+			} else {
+				ct = tmp;
+			}
+#else
 			if (!test_bit(IPS_ASSURED_BIT, &tmp->status))
 				ct = tmp;
+#endif
 			cnt++;
 		}
-
 		if (ct != NULL) {
 			if (likely(!nf_ct_is_dying(ct) &&
-				   atomic_inc_not_zero(&ct->ct_general.use)))
+						atomic_inc_not_zero(&ct->ct_general.use))
+#ifdef CONFIG_LTQ_HANDLE_CONNTRACK_SESSIONS
+#if defined(CONFIG_LTQ_PPA_API) || defined(CONFIG_LTQ_PPA_API_MODULE)
+				&& (ppa_hook_session_add_fn == NULL || ppa_hook_session_prio_fn(ct, PPA_F_SESSION_ORG_DIR | PPA_F_SESSION_REPLY_DIR) <= nf_conntrack_default_prio_thresh)
+#endif
+#endif
+			){
+#ifdef CONFIG_TI_PACKET_PROCESSOR
+				/* Check if the ORIGINAL Tuple is being accelerated? */
+				if ((!IS_TI_PP_SESSION_CT_INVALID(ct->tuplehash[IP_CT_DIR_ORIGINAL].ti_pp_session_handle)) &&
+						(1 == ct->tuplehash[IP_CT_DIR_ORIGINAL].ti_pp_sessions_count)) {
+					ti_hil_pp_event(TI_CT_NETFILTER_DELETE_SESSION,
+							(void *)ct->tuplehash[IP_CT_DIR_ORIGINAL].ti_pp_session_handle);
+				}
+				if ((!IS_TI_PP_SESSION_CT_INVALID(ct->tuplehash[IP_CT_DIR_REPLY].ti_pp_session_handle)) &&
+						(1 == ct->tuplehash[IP_CT_DIR_REPLY].ti_pp_sessions_count)) {
+					ti_hil_pp_event(TI_CT_NETFILTER_DELETE_SESSION,
+							(void *)ct->tuplehash[IP_CT_DIR_REPLY].ti_pp_session_handle);
+				}
+#endif
 				break;
-			else
+			} else {
 				ct = NULL;
+			}
 		}
 
 		if (cnt >= NF_CT_EVICTION_RANGE)
@@ -798,8 +831,18 @@ static noinline int early_drop(struct ne
 	}
 	rcu_read_unlock();
 
+#ifdef LTQ_IP_CONNTRACK_REPLACEMENT
+	if (!ct) {
+		if (recheck) {
+			recheck = 0;
+			goto redo;
+		}
+		return dropped;
+	}
+#else
 	if (!ct)
 		return dropped;
+#endif
 
 	if (del_timer(&ct->timeout)) {
 		if (nf_ct_delete(ct, 0, 0)) {
@@ -847,7 +890,6 @@ static struct nf_conn *
 	    unlikely(atomic_read(&net->ct.count) > nf_conntrack_max)) {
 		if (!early_drop(net, hash_bucket(hash, net))) {
 			atomic_dec(&net->ct.count);
-			net_warn_ratelimited("nf_conntrack: table full, dropping packet\n");
 			return ERR_PTR(-ENOMEM);
 		}
 	}
